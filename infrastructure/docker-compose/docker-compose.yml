# =============================================================================
# Docker Compose - Local Airflow 3 Development Environment
# =============================================================================
#
# ⚠️  DEVELOPMENT ONLY - NOT FOR PRODUCTION USE ⚠️
#
# This configuration is designed for local development and learning.
# For production deployments, use:
#   - Kubernetes with Helm charts (see ../helm/)
#   - Managed Airflow services (MWAA, Cloud Composer, Astronomer)
#   - External managed database (RDS, Cloud SQL, Azure Database)
#   - Proper secrets management (Vault, AWS Secrets Manager, etc.)
#
# Quick Start:
#   1. Copy .env.example to .env: cp .env.example .env
#   2. (Optional) Generate secure keys - see .env.example for commands
#   3. Start services: docker-compose up -d
#   4. Access UI: http://localhost:8080
#   5. Default login: admin / admin (change immediately)
#
# =============================================================================

services:
  postgres:
    image: postgres:15
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-airflow}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-airflow}
      POSTGRES_DB: ${POSTGRES_DB:-airflow}
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "${POSTGRES_USER:-airflow}"]
      interval: 10s
      retries: 5
      start_period: 5s
    restart: always
    # ⚠️ Security: Database is only accessible within Docker network
    # DO NOT expose port 5432 to host unless absolutely necessary

  airflow-init:
    image: apache/airflow:3.1.5
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: ${AIRFLOW_DB_CONNECTION:-postgresql+psycopg2://airflow:airflow@postgres/airflow}
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW_FERNET_KEY:-}
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      _AIRFLOW_DB_MIGRATE: 'true'
      _AIRFLOW_WWW_USER_CREATE: 'true'
      _AIRFLOW_WWW_USER_USERNAME: ${AIRFLOW_ADMIN_USERNAME:-admin}
      _AIRFLOW_WWW_USER_PASSWORD: ${AIRFLOW_ADMIN_PASSWORD:-admin}
    entrypoint: /bin/bash
    command:
      - -c
      - |
        airflow db migrate
        airflow users create \
          --username ${AIRFLOW_ADMIN_USERNAME:-admin} \
          --password ${AIRFLOW_ADMIN_PASSWORD:-admin} \
          --firstname ${AIRFLOW_ADMIN_FIRSTNAME:-Airflow} \
          --lastname ${AIRFLOW_ADMIN_LASTNAME:-Admin} \
          --role Admin \
          --email ${AIRFLOW_ADMIN_EMAIL:-admin@example.com} || true

  airflow-api-server:
    image: apache/airflow:3.1.5
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    environment: &airflow-common-env
      # Database connection
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: ${AIRFLOW_DB_CONNECTION:-postgresql+psycopg2://airflow:airflow@postgres/airflow}
      # Executor configuration
      AIRFLOW__CORE__EXECUTOR: ${AIRFLOW_EXECUTOR:-LocalExecutor}
      # Security keys - MUST be set for production
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW_FERNET_KEY:-}
      AIRFLOW__WEBSERVER__SECRET_KEY: ${AIRFLOW_WEBSERVER_SECRET_KEY:-development-secret-key-change-in-prod}
      # DAG configuration
      AIRFLOW__CORE__LOAD_EXAMPLES: ${AIRFLOW_LOAD_EXAMPLES:-false}
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: ${AIRFLOW_DAGS_PAUSED_AT_CREATION:-true}
      # Timezone
      AIRFLOW__CORE__DEFAULT_TIMEZONE: ${AIRFLOW_TIMEZONE:-UTC}
    volumes:
      - ../../dags:/opt/airflow/dags
      - airflow-logs:/opt/airflow/logs
    ports:
      # ⚠️ Security: Only bind to localhost in production
      # Change to "127.0.0.1:8080:8080" to restrict to local access only
      - "8080:8080"
    command: api-server
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always

  airflow-scheduler:
    image: apache/airflow:3.1.5
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    environment:
      <<: *airflow-common-env
    volumes:
      - ../../dags:/opt/airflow/dags
      - airflow-logs:/opt/airflow/logs
    command: scheduler
    healthcheck:
      test: ["CMD", "airflow", "jobs", "check", "--job-type", "SchedulerJob", "--hostname", "$${HOSTNAME}"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always

  airflow-dag-processor:
    image: apache/airflow:3.1.5
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    environment:
      <<: *airflow-common-env
    volumes:
      - ../../dags:/opt/airflow/dags
      - airflow-logs:/opt/airflow/logs
    command: dag-processor
    restart: always

volumes:
  postgres-db-volume:
  airflow-logs:

# =============================================================================
# SECURITY CHECKLIST FOR PRODUCTION
# =============================================================================
#
# Before using in any shared or production environment:
#
# [ ] Generate Fernet key: python3 -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())"
# [ ] Generate webserver secret: python3 -c "import secrets; print(secrets.token_hex(32))"
# [ ] Set strong database password (32+ chars, mixed case, numbers, symbols)
# [ ] Set strong admin password
# [ ] Change port binding to 127.0.0.1:8080:8080
# [ ] Use external managed database
# [ ] Enable HTTPS via reverse proxy (nginx, Traefik)
# [ ] Configure authentication (LDAP, OAuth, SAML)
# [ ] Set up monitoring and alerting
# [ ] Enable audit logging
# [ ] Review network exposure
#
# =============================================================================
