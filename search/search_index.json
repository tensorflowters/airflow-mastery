{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#airflow-mastery","title":"Airflow Mastery","text":"<p>The Complete Apache Airflow 3.x Curriculum</p> <p>From beginner to production-ready \u2014 master modern data orchestration with hands-on exercises, real-world case studies, and best practices.</p> <p> Get Started  View Curriculum  Case Studies</p>"},{"location":"#what-youll-learn","title":"What You'll Learn","text":""},{"location":"#modern-tooling","title":"Modern Tooling","text":"<p>Learn professional Python development with uv, ruff, pre-commit, and Docker \u2014 the same tools used at top tech companies.</p>"},{"location":"#taskflow-api","title":"TaskFlow API","text":"<p>Master Airflow 3.x's modern @dag and @task decorators for clean, Pythonic workflow definitions.</p>"},{"location":"#assets-dependencies","title":"Assets &amp; Dependencies","text":"<p>Understand Asset-based scheduling \u2014 the future of data-aware orchestration replacing sensor polling.</p>"},{"location":"#production-patterns","title":"Production Patterns","text":"<p>Deploy confidently with Kubernetes Executor, Helm charts, monitoring, and security best practices.</p>"},{"location":"#curriculum-overview","title":"Curriculum Overview","text":"Module Topic Duration 00 Environment Setup &amp; Modern Python Tooling 2-3 hours 01 Airflow Foundations &amp; TaskFlow API 3-4 hours 02 Operators, Sensors &amp; Hooks 3-4 hours 03 XCom &amp; Data Passing 2-3 hours 04 Scheduling &amp; Assets 2-3 hours 05 Testing &amp; Quality 2-3 hours 06 Connections &amp; Security 2-3 hours 07 Custom Operators &amp; Plugins 3-4 hours 08 Kubernetes Executor 3-4 hours 09 Production Patterns 3-4 hours"},{"location":"#quick-start","title":"Quick Start","text":"<p>Get up and running in under 5 minutes:</p> macOS / LinuxWindows (PowerShell) <pre><code>curl -fsSL https://raw.githubusercontent.com/YOUR_ORG/airflow-mastery/main/scripts/quickstart.sh | bash\n</code></pre> <pre><code>irm https://raw.githubusercontent.com/YOUR_ORG/airflow-mastery/main/scripts/quickstart.ps1 | iex\n</code></pre> <p>This will:</p> <ol> <li> Clone the repository</li> <li> Install dependencies with uv</li> <li> Start Airflow with Docker Compose</li> <li> Open the Airflow UI and documentation</li> </ol> <p> Manual Setup Instructions</p>"},{"location":"#real-world-case-studies","title":"Real-World Case Studies","text":"<p>Learn from production implementations at leading companies:</p> <ul> <li> Spotify \u2014 ML recommendation pipelines</li> <li> Stripe \u2014 Real-time fraud detection</li> <li> Airbnb \u2014 A/B testing infrastructure</li> <li> RAG Architecture \u2014 Modern AI workflows</li> </ul>"},{"location":"#contributing","title":"Contributing","text":"<p>We welcome contributions! See our Contributing Guide to get started.</p>   Built with :heart: for the data engineering community   <p>_[DAG]: Directed Acyclic Graph - Airflow's core workflow abstraction _[XCom]: Cross-Communication - Airflow's mechanism for task-to-task data passing _[Executor]: The component that runs tasks (Local, Celery, Kubernetes, etc.) _[Scheduler]: The component that triggers DAGs and determines task execution order _[Webserver]: The component that serves the Airflow UI _[Provider]: An Airflow package that integrates with external systems _[TaskFlow]: The modern Python-native API for defining Airflow tasks using decorators _[Asset]: A logical representation of data that DAGs can produce or consume _[SDK]: Software Development Kit - the airflow.sdk module in Airflow 3.x _[K8s]: Kubernetes - container orchestration platform _[uv]: A fast Python package manager by Astral _[ruff]: A fast Python linter and formatter by Astral _[CI]: Continuous Integration _[CD]: Continuous Deployment *[PR]: Pull Request</p>"},{"location":"CONTRIBUTING/","title":"Contributing to Airflow Mastery","text":"<p>Thank you for your interest in contributing to the Airflow Mastery curriculum! This guide will help you get started.</p>"},{"location":"CONTRIBUTING/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Code of Conduct</li> <li>Getting Started</li> <li>Development Setup</li> <li>Contribution Types</li> <li>Style Guide</li> <li>Testing</li> <li>Submitting Changes</li> </ul>"},{"location":"CONTRIBUTING/#code-of-conduct","title":"Code of Conduct","text":"<p>This project follows a simple code of conduct: - Be respectful and inclusive - Focus on constructive feedback - Help others learn and grow</p>"},{"location":"CONTRIBUTING/#getting-started","title":"Getting Started","text":""},{"location":"CONTRIBUTING/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.9+</li> <li>Apache Airflow 3.x</li> <li>Docker and Docker Compose (for local development)</li> <li>Git</li> </ul>"},{"location":"CONTRIBUTING/#fork-and-clone","title":"Fork and Clone","text":"<ol> <li>Fork the repository on GitHub</li> <li> <p>Clone your fork:    <pre><code>git clone https://github.com/YOUR_USERNAME/airflow-mastery.git\ncd airflow-mastery\n</code></pre></p> </li> <li> <p>Add upstream remote:    <pre><code>git remote add upstream https://github.com/ORIGINAL_OWNER/airflow-mastery.git\n</code></pre></p> </li> </ol>"},{"location":"CONTRIBUTING/#development-setup","title":"Development Setup","text":""},{"location":"CONTRIBUTING/#using-docker-recommended","title":"Using Docker (Recommended)","text":"<pre><code>cd infrastructure/docker-compose\ncp .env.example .env\ndocker-compose up -d\n</code></pre>"},{"location":"CONTRIBUTING/#local-python-environment","title":"Local Python Environment","text":"<pre><code># Create virtual environment\npython -m venv venv\nsource venv/bin/activate  # or `venv\\Scripts\\activate` on Windows\n\n# Install dependencies\npip install apache-airflow==3.1.5\npip install pytest pytest-cov\n\n# Run tests\npytest tests/ -v\n</code></pre>"},{"location":"CONTRIBUTING/#contribution-types","title":"Contribution Types","text":""},{"location":"CONTRIBUTING/#bug-fixes","title":"Bug Fixes","text":"<ol> <li>Check existing issues for duplicates</li> <li>Create a clear bug report with:</li> <li>Steps to reproduce</li> <li>Expected vs actual behavior</li> <li>Airflow version and environment</li> </ol>"},{"location":"CONTRIBUTING/#new-content","title":"New Content","text":""},{"location":"CONTRIBUTING/#adding-a-new-module","title":"Adding a New Module","text":"<ol> <li> <p>Create directory structure:    <pre><code>modules/XX-module-name/\n\u251c\u2500\u2500 README.md           # Module overview and learning objectives\n\u251c\u2500\u2500 exercises/          # Exercise starter files\n\u2502   \u251c\u2500\u2500 exercise_X_1_name_starter.py\n\u2502   \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 solutions/          # Complete solutions\n    \u251c\u2500\u2500 solution_X_1_name.py\n    \u2514\u2500\u2500 ...\n</code></pre></p> </li> <li> <p>Follow the module template in existing modules</p> </li> </ol>"},{"location":"CONTRIBUTING/#adding-exercises","title":"Adding Exercises","text":"<ol> <li> <p>Create exercise file following naming convention:    <pre><code>exercise_X_Y_descriptive_name_starter.py\n</code></pre></p> </li> <li> <p>Include:</p> </li> <li>Clear docstring with objectives</li> <li>TODO sections for learners</li> <li>Success criteria checklist</li> <li> <p>Hints where helpful</p> </li> <li> <p>Create matching solution file:    <pre><code>solution_X_Y_descriptive_name.py\n</code></pre></p> </li> </ol>"},{"location":"CONTRIBUTING/#adding-example-dags","title":"Adding Example DAGs","text":"<ol> <li>Add to <code>dags/examples/</code> directory</li> <li>Follow naming convention: <code>XX_descriptive_name.py</code></li> <li>Include comprehensive comments explaining concepts</li> </ol>"},{"location":"CONTRIBUTING/#documentation-improvements","title":"Documentation Improvements","text":"<ul> <li>Fix typos and clarify explanations</li> <li>Add missing documentation</li> <li>Improve code comments</li> <li>Update outdated information</li> </ul>"},{"location":"CONTRIBUTING/#style-guide","title":"Style Guide","text":""},{"location":"CONTRIBUTING/#python-code","title":"Python Code","text":"<p>Follow Airflow 3.x patterns:</p> <pre><code>\"\"\"\nModule docstring explaining purpose.\n\"\"\"\n\nfrom datetime import datetime\nfrom airflow.sdk import dag, task\n\n\n@dag(\n    dag_id=\"descriptive_dag_name\",\n    start_date=datetime(2024, 1, 1),\n    schedule=None,\n    catchup=False,\n    tags=[\"category\", \"module-XX\"],\n    description=\"Clear description of what this DAG does\",\n)\ndef my_dag():\n    \"\"\"DAG-level docstring.\"\"\"\n\n    @task\n    def my_task() -&gt; dict:\n        \"\"\"Task docstring explaining purpose.\"\"\"\n        return {\"status\": \"success\"}\n\n    my_task()\n\n\n# Instantiate the DAG\nmy_dag()\n</code></pre>"},{"location":"CONTRIBUTING/#key-conventions","title":"Key Conventions","text":"<ol> <li>DAG Decorator Pattern: Use <code>@dag</code> decorator for TaskFlow DAGs</li> <li>Type Hints: Include return type hints on <code>@task</code> functions</li> <li>Tags: Always include <code>module-XX</code> tag and descriptive category tags</li> <li>Catchup: Default to <code>catchup=False</code> for learning exercises</li> <li>Docstrings: Every function and module should have docstrings</li> <li>Comments: Explain the \"why\", not the \"what\"</li> </ol>"},{"location":"CONTRIBUTING/#file-naming","title":"File Naming","text":"Type Pattern Example Exercise <code>exercise_X_Y_name_starter.py</code> <code>exercise_2_1_etl_pipeline_starter.py</code> Solution <code>solution_X_Y_name.py</code> <code>solution_2_1_etl_pipeline.py</code> Example DAG <code>XX_descriptive_name.py</code> <code>02_taskflow_etl.py</code>"},{"location":"CONTRIBUTING/#commit-messages","title":"Commit Messages","text":"<p>Follow conventional commits format:</p> <pre><code>type(scope): description\n\nfeat(module-05): add multi-asset trigger exercise\nfix(infrastructure): update pod template version\ndocs(readme): clarify installation steps\ntest(solutions): add parametrized tests for module 3\n</code></pre> <p>Types: <code>feat</code>, <code>fix</code>, <code>docs</code>, <code>test</code>, <code>refactor</code>, <code>style</code>, <code>chore</code></p>"},{"location":"CONTRIBUTING/#testing","title":"Testing","text":""},{"location":"CONTRIBUTING/#running-tests","title":"Running Tests","text":"<pre><code># All tests\npytest tests/ -v\n\n# Specific test file\npytest tests/test_solutions.py -v\n\n# With coverage\npytest tests/ --cov=dags --cov-report=html\n\n# Skip slow tests\npytest tests/ -v -m \"not slow\"\n\n# Include integration tests\npytest tests/ -v --run-integration\n</code></pre>"},{"location":"CONTRIBUTING/#writing-tests","title":"Writing Tests","text":"<p>Add tests for: - New solutions (DAG parsing, structure validation) - TaskFlow patterns (XCom, mapping, context) - Exercise structure (TODO sections, hints)</p> <p>Example test:</p> <pre><code>def test_solution_has_correct_tags(self, solution_file):\n    \"\"\"Verify solution includes required tags.\"\"\"\n    content = solution_file.read_text()\n\n    assert '\"solution\"' in content\n    assert f'\"module-{module_num:02d}\"' in content\n</code></pre>"},{"location":"CONTRIBUTING/#submitting-changes","title":"Submitting Changes","text":""},{"location":"CONTRIBUTING/#pull-request-process","title":"Pull Request Process","text":"<ol> <li> <p>Create feature branch:    <pre><code>git checkout -b feat/add-module-15\n</code></pre></p> </li> <li> <p>Make changes and commit:    <pre><code>git add .\ngit commit -m \"feat(module-15): add advanced orchestration module\"\n</code></pre></p> </li> <li> <p>Push to your fork:    <pre><code>git push origin feat/add-module-15\n</code></pre></p> </li> <li> <p>Create Pull Request with:</p> </li> <li>Clear title and description</li> <li>Link to related issues</li> <li>Screenshots/examples if applicable</li> </ol>"},{"location":"CONTRIBUTING/#pr-checklist","title":"PR Checklist","text":"<ul> <li> Tests pass locally (<code>pytest tests/ -v</code>)</li> <li> Code follows style guide</li> <li> Docstrings and comments included</li> <li> README updated if adding new module</li> <li> No credentials or secrets in code</li> </ul>"},{"location":"CONTRIBUTING/#review-process","title":"Review Process","text":"<ol> <li>Maintainers will review within 1-2 weeks</li> <li>Address feedback and push updates</li> <li>Once approved, changes will be merged</li> </ol>"},{"location":"CONTRIBUTING/#questions","title":"Questions?","text":"<ul> <li>Open an issue for general questions</li> <li>Tag with <code>question</code> label</li> <li>Check existing issues first</li> </ul> <p>Thank you for contributing to Airflow Mastery!</p> <p>_[DAG]: Directed Acyclic Graph - Airflow's core workflow abstraction _[XCom]: Cross-Communication - Airflow's mechanism for task-to-task data passing _[Executor]: The component that runs tasks (Local, Celery, Kubernetes, etc.) _[Scheduler]: The component that triggers DAGs and determines task execution order _[Webserver]: The component that serves the Airflow UI _[Provider]: An Airflow package that integrates with external systems _[TaskFlow]: The modern Python-native API for defining Airflow tasks using decorators _[Asset]: A logical representation of data that DAGs can produce or consume _[SDK]: Software Development Kit - the airflow.sdk module in Airflow 3.x _[K8s]: Kubernetes - container orchestration platform _[uv]: A fast Python package manager by Astral _[ruff]: A fast Python linter and formatter by Astral _[CI]: Continuous Integration _[CD]: Continuous Deployment *[PR]: Pull Request</p>"},{"location":"airflow3-k8s-guide/","title":"Apache Airflow 3 self-hosted Kubernetes deployment guide","text":"<p>Airflow 3.0 represents the most significant architectural shift in the project's history, introducing a client-server model where workers communicate via a new Task Execution API rather than directly accessing the metadata database. Released on April 22, 2025, Airflow 3 (current stable: 3.1.5) brings DAG versioning\u2014the most requested community feature\u2014alongside a completely rewritten React-based UI, event-driven scheduling, and enhanced security isolation. For Kubernetes deployments, the official Helm chart (version 1.18.0) fully supports Airflow 3 with the KubernetesExecutor remaining the recommended choice for self-hosted production environments.</p>"},{"location":"airflow3-k8s-guide/#architectural-changes-redefine-how-airflow-components-communicate","title":"Architectural changes redefine how Airflow components communicate","text":"<p>The fundamental change in Airflow 3 is the Task Execution Interface (AIP-72), which introduces a true client-server architecture. In Airflow 2.x, all components\u2014scheduler, workers, webserver\u2014communicated directly with the metadata database, creating security risks and scaling challenges. Airflow 3 eliminates this by routing all worker communication through a dedicated REST API.</p> <p>New core components you'll encounter include the API Server (replacing the webserver via <code>airflow api-server</code>), which serves both the REST API and static UI files, and the DAG Processor, which must now be started independently with <code>airflow dag-processor</code>. The scheduler continues as before but no longer expects workers to have database access.</p> <p>The Task SDK provides a lightweight runtime for task execution, enabling containerized execution with zero-trust principles and scoped token authentication. This SDK is versioned separately (currently Task SDK 1.1.5) and opens the door for multi-language support\u2014a Golang SDK is planned. Tasks must now use the Task SDK or the Python Client (<code>apache-airflow-client</code>) for metadata interactions; direct database access via sessions is prohibited.</p> <p>DAG Versioning (AIP-65, AIP-66) ensures DAGs run to completion based on the version at trigger time, even if the DAG file changes mid-execution. DAG Bundles replace the deprecated <code>--subdir</code> CLI flag, with <code>LocalDagBundle</code> and <code>GitDagBundle</code> as primary options. The UI now shows version-aware task structure, code, and logs.</p> <p>Assets (renamed from Datasets) gain significant enhancements including the <code>@asset</code> decorator for asset-centric DAG definitions, Asset Watchers for monitoring external systems, and AWS SQS integration for event-driven triggers. The Edge Executor (AIP-69), available via <code>apache-airflow-providers-edge3</code>, enables task execution on edge devices, remote data centers, and hybrid multi-cloud environments.</p>"},{"location":"airflow3-k8s-guide/#breaking-changes-require-careful-dag-migration","title":"Breaking changes require careful DAG migration","text":"<p>Several features have been removed entirely in Airflow 3, requiring updates before migration:</p> Removed feature Replacement SubDAGs TaskGroups, Assets, Data-Aware Scheduling SequentialExecutor LocalExecutor (works with SQLite) CeleryKubernetesExecutor Multiple Executor Configuration LocalKubernetesExecutor Multiple Executor Configuration SLAs Deadline Alerts REST API <code>/api/v1</code> FastAPI-based <code>/api/v2</code> DAG/XCom Pickling Removed entirely <code>--subdir</code> CLI flag DAG Bundles <p>Context variables including <code>execution_date</code>, <code>tomorrow_ds</code>, <code>yesterday_ds</code>, <code>prev_ds</code>, <code>next_ds</code>, and their derivatives are no longer available\u2014use <code>logical_date</code> instead. The <code>catchup_by_default</code> configuration now defaults to <code>False</code>, meaning new DAGs won't backfill automatically.</p> <p>Import paths have shifted to the new <code>airflow.sdk</code> namespace. Update imports from <code>airflow.decorators.dag</code> to <code>airflow.sdk.dag</code>, <code>airflow.models.DAG</code> to <code>airflow.sdk.DAG</code>, and <code>airflow.datasets.Dataset</code> to <code>airflow.sdk.Asset</code>. Common operators have moved to <code>apache-airflow-providers-standard</code>, requiring explicit installation\u2014<code>BashOperator</code> is now at <code>airflow.providers.standard.operators.bash</code>.</p> <p>Run Ruff with AIR rules (version 0.13.1+) to identify and auto-fix most issues: <pre><code>ruff check dags/ --select AIR301 --show-fixes\nruff check dags/ --select AIR301 --fix --unsafe-fixes\n</code></pre></p>"},{"location":"airflow3-k8s-guide/#kubernetes-deployment-uses-kubernetesexecutor-for-optimal-isolation","title":"Kubernetes deployment uses KubernetesExecutor for optimal isolation","text":"<p>For self-hosted Kubernetes deployments, the KubernetesExecutor provides complete task isolation by creating a dedicated pod for each task instance. This approach scales to zero when idle, requires no Redis or message broker, and allows per-task resource specification\u2014ideal for variable workloads with heterogeneous resource requirements.</p> <p>The official Helm chart installation is straightforward: <pre><code>helm repo add apache-airflow https://airflow.apache.org\nhelm repo update\nhelm upgrade --install airflow apache-airflow/airflow \\\n  --namespace airflow --create-namespace \\\n  -f values.yaml\n</code></pre></p> <p>Essential values.yaml configuration for production: <pre><code>executor: KubernetesExecutor\nuseStandardNaming: true\ndefaultAirflowTag: \"3.0.2\"\n\n# External PostgreSQL (mandatory for production)\npostgresql:\n  enabled: false\ndata:\n  metadataSecretName: airflow-database-secret\n\n# Connection pooling\npgbouncer:\n  enabled: true\n  maxClientConn: 100\n  metadataPoolSize: 10\n\n# High availability\nscheduler:\n  replicas: 2\n  resources:\n    requests: { cpu: \"500m\", memory: \"1Gi\" }\n    limits: { cpu: \"2\", memory: \"4Gi\" }\n\nwebserver:\n  replicas: 2\n  resources:\n    requests: { cpu: \"250m\", memory: \"512Mi\" }\n\n# DAG sync via git-sync\ndags:\n  gitSync:\n    enabled: true\n    repo: \"git@github.com:your-org/airflow-dags.git\"\n    branch: main\n    sshKeySecret: airflow-ssh-secret\n    wait: 60\n\n# Security\nwebserverSecretKeySecretName: airflow-webserver-secret\n</code></pre></p> <p>The CeleryExecutor remains viable when you need lower task startup latency (workers are always running) or high-throughput predictable workloads, but requires Redis/RabbitMQ and incurs higher baseline costs. The hybrid executors (<code>CeleryKubernetesExecutor</code>, <code>LocalKubernetesExecutor</code>) are deprecated\u2014use the new Multiple Executor Configuration feature instead.</p>"},{"location":"airflow3-k8s-guide/#production-storage-and-database-configuration","title":"Production storage and database configuration","text":"<p>DAG deployment should use git-sync (recommended), which continuously pulls DAGs from a Git repository. Create an SSH key secret and configure known_hosts to prevent MITM attacks. Alternatively, bake DAGs directly into custom Docker images for immutable deployments with explicit versioning.</p> <p>Log persistence requires remote logging for production\u2014configure S3, GCS, or Azure Blob Storage: <pre><code>config:\n  logging:\n    remote_logging: 'True'\n    remote_base_log_folder: 's3://your-bucket/airflow-logs'\n    remote_log_conn_id: 'aws_default'\n</code></pre></p> <p>Database setup mandates PostgreSQL or MySQL\u2014never use the embedded PostgreSQL or SQLite for production. Enable PgBouncer to handle connection pooling; Airflow opens many database connections, especially with high DAG counts. Configure the connection via Kubernetes Secret rather than plaintext values: <pre><code>kubectl create secret generic airflow-database-secret \\\n  --from-literal=connection=postgresql://user:pass@host:5432/airflow\n</code></pre></p> <p>Sizing recommendations scale with workload: 50 DAGs typically requires ~5 CPUs and 5GB memory total; 200 DAGs needs 10 CPUs and 20GB; 500+ DAGs requires horizontal scaling with dedicated node pools.</p>"},{"location":"airflow3-k8s-guide/#learning-path-for-experienced-airflow-2x-users","title":"Learning path for experienced Airflow 2.x users","text":"<p>For users with Airflow 2.x experience, prioritize learning these concepts in order:</p> <ol> <li>New import paths (<code>airflow.sdk</code>) and installing <code>apache-airflow-providers-standard</code></li> <li>Task SDK and execution model\u2014understanding the API Server architecture</li> <li>DAG Versioning and Bundles\u2014major workflow changes</li> <li>Assets (formerly Datasets)\u2014terminology and API updates</li> <li>New React UI navigation\u2014asset-centric and task-centric views</li> </ol> <p>Core documentation starting points: - Upgrading to Airflow 3\u2014essential migration guide - Helm Chart Production Guide\u2014Kubernetes-specific - TaskFlow Tutorial\u2014updated patterns - Dynamic Task Mapping</p> <p>Sample TaskFlow pattern for Airflow 3: <pre><code>from airflow.sdk import DAG, task\nfrom datetime import datetime\n\n@task\ndef extract():\n    return {\"data\": [1, 2, 3]}\n\n@task\ndef transform(data):\n    return {\"sum\": sum(data[\"data\"])}\n\n@task\ndef load(values: dict):\n    print(f\"Loaded: {values}\")\n\nwith DAG(dag_id=\"etl_example\", start_date=datetime(2024, 1, 1), schedule=None):\n    raw = extract()\n    transformed = transform(raw)\n    load(transformed)\n</code></pre></p>"},{"location":"airflow3-k8s-guide/#common-kubernetes-deployment-pitfalls-to-avoid","title":"Common Kubernetes deployment pitfalls to avoid","text":"<p>Configuration mistakes frequently include forgetting to install <code>apache-airflow-providers-standard</code> (BashOperator won't work), not setting <code>webserverSecretKeySecretName</code> (causes session issues across pod restarts), and misconfiguring git-sync repository URLs or subPath settings.</p> <p>Database connection errors manifest as <code>psycopg2.OperationalError: FATAL: sorry, too many clients already</code>\u2014enable PgBouncer to resolve. Never use SQLite in production; it cannot handle concurrent writes from multiple scheduler replicas.</p> <p>Airflow 3-specific issues include tasks failing when they attempt direct database access (must use REST API or Task SDK), forgetting to start <code>airflow dag-processor</code> separately, and using the old <code>airflow webserver</code> command instead of <code>airflow api-server</code>.</p> <p>Resource contention appears as pod evictions or scheduling failures. Set appropriate resource requests and limits, use <code>safeToEvict: false</code> for workers in KubernetesExecutor deployments, and consider KEDA autoscaling for CeleryExecutor workloads.</p> <p>Debug with these commands: <pre><code># Check scheduler logs\nkubectl logs -n airflow deployment/airflow-scheduler -c scheduler\n\n# Validate KubernetesExecutor pod specs\nairflow kubernetes generate-dag-yaml\n\n# Access UI via port-forward\nkubectl port-forward svc/airflow-api-server 8080:8080 -n airflow\n</code></pre></p>"},{"location":"airflow3-k8s-guide/#step-by-step-deployment-checklist","title":"Step-by-step deployment checklist","text":"<p>Prerequisites: Kubernetes 1.30+, Helm 3.10+, external PostgreSQL database, and a Git repository for DAGs.</p> <p>Phase 1\u2014Secrets and namespace: <pre><code>kubectl create namespace airflow\n\n# Generate secrets\npython3 -c 'import secrets; print(secrets.token_hex(16))'  # webserver key\npython3 -c 'from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())'  # fernet\n\n# Create database secret\nkubectl create secret generic airflow-database-secret \\\n  --from-literal=connection=postgresql://user:pass@host:5432/airflow \\\n  -n airflow\n</code></pre></p> <p>Phase 2\u2014Create values.yaml with configurations for executor, database, git-sync, resources, and security contexts as shown in the production configuration section above.</p> <p>Phase 3\u2014Install and verify: <pre><code>helm install airflow apache-airflow/airflow -n airflow -f values.yaml --debug --timeout 10m\nkubectl get pods -n airflow\nkubectl port-forward svc/airflow-api-server 8080:8080 -n airflow\n</code></pre></p> <p>Phase 4\u2014Post-installation: Create admin users via <code>kubectl exec</code>, configure connections and variables, enable remote logging, and set up monitoring with StatsD/Prometheus.</p>"},{"location":"airflow3-k8s-guide/#conclusion","title":"Conclusion","text":"<p>Airflow 3's architectural shift to a client-server model fundamentally improves security and scalability, but requires thoughtful migration planning. The key insight for Kubernetes deployments is that the KubernetesExecutor pairs naturally with Airflow 3's isolation model\u2014each task runs in its own pod with scoped API access, eliminating direct database exposure. Prioritize updating imports to <code>airflow.sdk</code>, removing deprecated features like SubDAGs, and using the official Helm chart with external PostgreSQL and PgBouncer. The migration tooling\u2014particularly Ruff's AIR rules\u2014automates most code changes, making the upgrade tractable for experienced teams willing to test thoroughly in staging environments.</p> <p>_[DAG]: Directed Acyclic Graph - Airflow's core workflow abstraction _[XCom]: Cross-Communication - Airflow's mechanism for task-to-task data passing _[Executor]: The component that runs tasks (Local, Celery, Kubernetes, etc.) _[Scheduler]: The component that triggers DAGs and determines task execution order _[Webserver]: The component that serves the Airflow UI _[Provider]: An Airflow package that integrates with external systems _[TaskFlow]: The modern Python-native API for defining Airflow tasks using decorators _[Asset]: A logical representation of data that DAGs can produce or consume _[SDK]: Software Development Kit - the airflow.sdk module in Airflow 3.x _[K8s]: Kubernetes - container orchestration platform _[uv]: A fast Python package manager by Astral _[ruff]: A fast Python linter and formatter by Astral _[CI]: Continuous Integration _[CD]: Continuous Deployment *[PR]: Pull Request</p>"},{"location":"references/","title":"References &amp; Learning Resources","text":"<p>This document contains all references, official documentation, and community resources used to create this learning curriculum. Organized by topic for easy navigation.</p>"},{"location":"references/#modern-python-tooling-module-00","title":"Modern Python Tooling (Module 00)","text":""},{"location":"references/#uv-fast-python-package-manager","title":"uv - Fast Python Package Manager","text":"<ul> <li>Official Documentation: https://docs.astral.sh/uv/</li> <li>Installation Guide: https://docs.astral.sh/uv/getting-started/installation/</li> <li>Project Management: https://docs.astral.sh/uv/concepts/projects/</li> <li>Docker Integration: https://docs.astral.sh/uv/guides/integration/docker/</li> <li>GitHub Repository: https://github.com/astral-sh/uv</li> <li>Docker Examples: https://github.com/astral-sh/uv-docker-example</li> </ul> <p>Key concepts:</p> <ul> <li>10-100x faster than pip for package installation</li> <li>Replaces pip, pip-tools, virtualenv, and pyenv</li> <li>Built in Rust for maximum performance</li> <li>Drop-in replacement for existing workflows</li> </ul>"},{"location":"references/#ruff-python-linter-formatter","title":"Ruff - Python Linter &amp; Formatter","text":"<ul> <li>Official Documentation: https://docs.astral.sh/ruff/</li> <li>Configuration Guide: https://docs.astral.sh/ruff/configuration/</li> <li>Rules Reference: https://docs.astral.sh/ruff/rules/</li> <li>Airflow Rules (AIR): https://docs.astral.sh/ruff/rules/#airflow-air</li> <li>GitHub Repository: https://github.com/astral-sh/ruff</li> </ul> <p>Airflow-specific rules:</p> <ul> <li><code>AIR001</code>: Task variable name mismatch</li> <li><code>AIR301</code>: Airflow 3.0 removal (import changes)</li> <li><code>AIR302</code>: Moved-to-provider deprecation fixes</li> </ul>"},{"location":"references/#pre-commit-hooks","title":"Pre-commit Hooks","text":"<ul> <li>Official Documentation: https://pre-commit.com/</li> <li>Hook Repository: https://github.com/pre-commit/pre-commit-hooks</li> <li>Ruff Pre-commit: https://github.com/astral-sh/ruff-pre-commit</li> <li>Configuration Guide: https://pre-commit.com/#adding-pre-commit-plugins-to-your-project</li> </ul>"},{"location":"references/#pyprojecttoml-standards","title":"pyproject.toml Standards","text":"<ul> <li>PEP 518: https://peps.python.org/pep-0518/ - Build system specification</li> <li>PEP 621: https://peps.python.org/pep-0621/ - Project metadata</li> <li>PEP 735: https://peps.python.org/pep-0735/ - Dependency groups</li> <li>Packaging Guide: https://packaging.python.org/en/latest/guides/writing-pyproject-toml/</li> </ul>"},{"location":"references/#docker-best-practices","title":"Docker Best Practices","text":"<ul> <li>Multi-stage Builds: https://docs.docker.com/build/building/multi-stage/</li> <li>Python Docker Images: https://hub.docker.com/_/python</li> <li>uv Docker Integration: https://docs.astral.sh/uv/guides/integration/docker/</li> <li>BuildKit Cache Mounts: https://docs.docker.com/build/cache/</li> </ul>"},{"location":"references/#official-apache-airflow-documentation","title":"Official Apache Airflow Documentation","text":""},{"location":"references/#core-documentation","title":"Core Documentation","text":"<ul> <li>Airflow 3.0 Release Notes: https://airflow.apache.org/docs/apache-airflow/3.0.0/release_notes.html</li> <li>Airflow 3.1.5 Release Notes (Stable): https://airflow.apache.org/docs/apache-airflow/stable/release_notes.html</li> <li>Upgrading to Airflow 3: https://airflow.apache.org/docs/apache-airflow/stable/installation/upgrading_to_airflow3.html</li> <li>TaskFlow Tutorial: https://airflow.apache.org/docs/apache-airflow/stable/tutorial/taskflow.html</li> <li>Dynamic Task Mapping: https://airflow.apache.org/docs/apache-airflow/stable/authoring-and-scheduling/dynamic-task-mapping.html</li> <li>Concepts Overview: https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/index.html</li> <li>Best Practices: https://airflow.apache.org/docs/apache-airflow/stable/best-practices.html</li> </ul>"},{"location":"references/#helm-chart-documentation","title":"Helm Chart Documentation","text":"<ul> <li>Helm Chart for Apache Airflow: https://airflow.apache.org/docs/helm-chart/stable/index.html</li> <li>Production Guide: https://airflow.apache.org/docs/helm-chart/stable/production-guide.html</li> <li>Manage DAG Files: https://airflow.apache.org/docs/helm-chart/stable/manage-dag-files.html</li> <li>Adding Connections and Variables: https://airflow.apache.org/docs/helm-chart/stable/adding-connections-and-variables.html</li> <li>Helm Chart Parameters Reference: https://airflow.apache.org/docs/helm-chart/stable/parameters-ref.html</li> </ul>"},{"location":"references/#api-sdk-documentation","title":"API &amp; SDK Documentation","text":"<ul> <li>REST API Reference: https://airflow.apache.org/docs/apache-airflow/stable/stable-rest-api-ref.html</li> <li>Task SDK Documentation: https://airflow.apache.org/docs/apache-airflow/stable/authoring-and-scheduling/task-sdk.html</li> <li>Python Client: https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/python-client.html</li> </ul>"},{"location":"references/#sensors-operators","title":"Sensors &amp; Operators","text":"<ul> <li>Sensors: https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/sensors.html</li> <li>Deferrable Operators: https://airflow.apache.org/docs/apache-airflow/stable/authoring-and-scheduling/deferring.html</li> <li>Custom Triggers: https://airflow.apache.org/docs/apache-airflow/stable/authoring-and-scheduling/triggers.html</li> </ul>"},{"location":"references/#connections-secrets","title":"Connections &amp; Secrets","text":"<ul> <li>Managing Connections: https://airflow.apache.org/docs/apache-airflow/stable/howto/connection.html</li> <li>Secrets Backend: https://airflow.apache.org/docs/apache-airflow/stable/security/secrets/secrets-backend/index.html</li> <li>Variables: https://airflow.apache.org/docs/apache-airflow/stable/howto/variable.html</li> <li>Environment Variables Secrets Backend: https://airflow.apache.org/docs/apache-airflow/stable/security/secrets/secrets-backend/local-filesystem-secrets-backend.html</li> </ul>"},{"location":"references/#resource-management","title":"Resource Management","text":"<ul> <li>Pools: https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/pools.html</li> <li>Priority Weights: https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/priority-weight.html</li> <li>Concurrency: https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/concurrency.html</li> </ul>"},{"location":"references/#official-blog-posts-announcements","title":"Official Blog Posts &amp; Announcements","text":"<ul> <li>Apache Airflow 3 is Generally Available!: https://airflow.apache.org/blog/airflow-three-point-oh-is-here/<ul> <li>Primary source for Airflow 3 feature announcements</li> <li>Task Execution Interface (AIP-72) details</li> <li>DAG Versioning explanation</li> </ul> </li> </ul>"},{"location":"references/#airflow-improvement-proposals-aips","title":"Airflow Improvement Proposals (AIPs)","text":"<p>Understanding the design decisions behind Airflow 3:</p> <ul> <li>AIP-65: DAG Versioning - Core versioning architecture</li> <li>AIP-66: DAG Bundles - DAG deployment and bundling</li> <li>AIP-69: Edge Executor - Hybrid and edge deployments</li> <li>AIP-72: Task Execution Interface - Client-server architecture</li> </ul> <p>Full AIP list: https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals</p>"},{"location":"references/#migration-tools-automation","title":"Migration Tools &amp; Automation","text":"<ul> <li> <p>Airflow 2 to 3 Auto Migration Rules (GitHub Issue #41641): https://github.com/apache/airflow/issues/41641</p> <ul> <li>Ruff linter rules for automated migration</li> <li>AIR301+ rule set for code fixes</li> </ul> </li> <li> <p>Ruff Linter: https://docs.astral.sh/ruff/</p> <ul> <li>AIR rule set for Airflow-specific linting</li> <li>Automated code fixes for deprecations</li> </ul> </li> </ul>"},{"location":"references/#community-articles-tutorials","title":"Community Articles &amp; Tutorials","text":""},{"location":"references/#kubernetes-deployment-guides","title":"Kubernetes Deployment Guides","text":"<ul> <li> <p>NextLytics: Apache Airflow 3.0 - Everything You Need to Know: https://www.nextlytics.com/blog/apache-airflow-3.0-everything-you-need-to-know-about-the-new-release</p> <ul> <li>Comprehensive feature overview</li> <li>Migration considerations</li> </ul> </li> <li> <p>Microsoft Learn: Deploy Apache Airflow on AKS with Helm: https://learn.microsoft.com/en-us/azure/aks/airflow-overview</p> <ul> <li>Azure-specific but architecture patterns applicable to any K8s</li> </ul> </li> <li> <p>Medium: Deploying Apache Airflow on Kubernetes with Helm: https://medium.com/@jdegbun/deploying-apache-airflow-on-kubernetes-with-helm-and-minikube-syncing-dags-from-github-bce4730d7881</p> <ul> <li>Practical walkthrough with git-sync</li> </ul> </li> </ul>"},{"location":"references/#technical-deep-dives","title":"Technical Deep Dives","text":"<ul> <li>An Outing with Airflow on Kubernetes: https://varunbpatil.github.io/2020/10/01/airflow-on-kubernetes.html<ul> <li>KubernetesExecutor internals</li> <li>Pod template configuration</li> </ul> </li> </ul>"},{"location":"references/#github-repositories","title":"GitHub Repositories","text":""},{"location":"references/#official","title":"Official","text":"<ul> <li>Apache Airflow: https://github.com/apache/airflow</li> <li>Airflow Helm Chart: https://github.com/apache/airflow/tree/main/chart</li> <li>Airflow Provider Packages: https://github.com/apache/airflow/tree/main/providers</li> </ul>"},{"location":"references/#community","title":"Community","text":"<ul> <li>Helm Charts (Legacy): https://github.com/helm/charts/tree/master/stable/airflow<ul> <li>Historical reference (now deprecated in favor of official chart)</li> </ul> </li> </ul>"},{"location":"references/#video-resources","title":"Video Resources","text":""},{"location":"references/#official-airflow-youtube","title":"Official Airflow YouTube","text":"<ul> <li>Apache Airflow YouTube Channel: https://www.youtube.com/@ApacheAirflow</li> <li>Airflow Summit recordings: Various deep-dive sessions</li> </ul>"},{"location":"references/#recommended-playlists","title":"Recommended Playlists","text":"<ul> <li>Astronomer Academy (free courses): https://academy.astronomer.io/</li> <li>Data Engineering Podcast episodes on Airflow</li> </ul>"},{"location":"references/#books","title":"Books","text":"<ul> <li> <p>Data Pipelines with Apache Airflow by Bas Harenslak &amp; Julian de Ruiter (Manning)</p> <ul> <li>Note: Written for Airflow 2.x, concepts still applicable but imports/APIs differ</li> </ul> </li> <li> <p>Apache Airflow Best Practices (O'Reilly)</p> <ul> <li>Production patterns and anti-patterns</li> </ul> </li> </ul>"},{"location":"references/#tools-integrations","title":"Tools &amp; Integrations","text":""},{"location":"references/#development-tools","title":"Development Tools","text":"<ul> <li>Ruff: https://docs.astral.sh/ruff/ - Fast Python linter with Airflow rules</li> <li>pytest-airflow: DAG testing utilities</li> <li>pre-commit hooks: https://pre-commit.com/ - Code quality automation</li> </ul>"},{"location":"references/#monitoring-observability","title":"Monitoring &amp; Observability","text":"<ul> <li>StatsD: https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/metrics.html</li> <li>Prometheus/Grafana: Community dashboards available</li> <li>OpenTelemetry: Tracing support in Airflow 3</li> </ul>"},{"location":"references/#infrastructure","title":"Infrastructure","text":"<ul> <li>PgBouncer: https://www.pgbouncer.org/ - Connection pooling (critical for production)</li> <li>KEDA: https://keda.sh/ - Event-driven autoscaling for CeleryExecutor</li> </ul>"},{"location":"references/#aiml-orchestration-resources","title":"AI/ML Orchestration Resources","text":""},{"location":"references/#llm-rag-pipelines","title":"LLM &amp; RAG Pipelines","text":"<ul> <li>LangChain Airflow Integration: https://python.langchain.com/docs/integrations/</li> <li>LlamaIndex with Airflow: https://docs.llamaindex.ai/en/stable/examples/</li> <li>OpenAI API Best Practices: https://platform.openai.com/docs/guides/production-best-practices</li> <li>Anthropic API Rate Limits: https://docs.anthropic.com/en/api/rate-limits</li> </ul>"},{"location":"references/#vector-databases","title":"Vector Databases","text":"<ul> <li>Pinecone Airflow Integration: https://docs.pinecone.io/</li> <li>Weaviate Python Client: https://weaviate.io/developers/weaviate/client-libraries/python</li> <li>Chroma Documentation: https://docs.trychroma.com/</li> <li>Qdrant Airflow Operator: https://qdrant.tech/documentation/</li> </ul>"},{"location":"references/#embedding-text-processing","title":"Embedding &amp; Text Processing","text":"<ul> <li>OpenAI Embeddings Guide: https://platform.openai.com/docs/guides/embeddings</li> <li>Sentence Transformers: https://www.sbert.net/</li> <li>tiktoken for Token Counting: https://github.com/openai/tiktoken</li> </ul>"},{"location":"references/#ml-pipeline-patterns","title":"ML Pipeline Patterns","text":"<ul> <li>MLflow Integration: https://mlflow.org/docs/latest/</li> <li>Weights &amp; Biases Airflow: https://docs.wandb.ai/guides/integrations/</li> <li>Feature Store Patterns: https://feast.dev/</li> </ul>"},{"location":"references/#case-studies-internal","title":"Case Studies (Internal)","text":"<ul> <li>Spotify Recommendations - Dynamic mapping for ML pipelines</li> <li>Stripe Fraud Detection - Real-time ML scoring with retry patterns</li> <li>Airbnb Experimentation - A/B testing at scale</li> <li>Modern RAG Architecture - Production RAG pipeline patterns</li> </ul>"},{"location":"references/#community-support","title":"Community &amp; Support","text":""},{"location":"references/#official-channels","title":"Official Channels","text":"<ul> <li> <p>Slack: https://apache-airflow-slack.herokuapp.com/</p> <ul> <li> </li> <li> </li> <li> </li> </ul> </li> <li> <p>GitHub Discussions: https://github.com/apache/airflow/discussions</p> </li> <li>Stack Overflow: Tag <code>apache-airflow</code></li> </ul>"},{"location":"references/#troubleshooting-for-help","title":"troubleshooting for help","text":""},{"location":"references/#announcements-for-updates","title":"announcements for updates","text":""},{"location":"references/#kubernetes-for-k8s-specific-discussions","title":"kubernetes for K8s-specific discussions","text":""},{"location":"references/#mailing-lists","title":"Mailing Lists","text":"<ul> <li>dev@airflow.apache.org - Development discussions</li> <li>users@airflow.apache.org - User questions</li> </ul>"},{"location":"references/#cheat-sheets-quick-references","title":"Cheat Sheets &amp; Quick References","text":""},{"location":"references/#cli-commands-airflow-3","title":"CLI Commands (Airflow 3)","text":"<pre><code># Core commands\nairflow dags list                    # List all DAGs\nairflow dags test &lt;dag_id&gt; &lt;date&gt;    # Test a DAG\nairflow tasks test &lt;dag_id&gt; &lt;task&gt;   # Test a task\nairflow dags trigger &lt;dag_id&gt;        # Trigger a DAG run\n\n# New in Airflow 3\nairflow api-server                   # Start API server (replaces webserver)\nairflow dag-processor                # Start DAG processor (now separate)\nairflow kubernetes generate-dag-yaml # Generate K8s pod specs\n\n# Database\nairflow db migrate                   # Apply migrations\nairflow db check                     # Check DB connection\n\n# Debug\nairflow config list                  # Show configuration\nairflow info                         # Show system info\n</code></pre>"},{"location":"references/#key-configuration-variables","title":"Key Configuration Variables","text":"<pre><code>[core]\nexecutor = KubernetesExecutor\ndags_folder = /opt/airflow/dags\nload_examples = False\n\n[kubernetes]\nnamespace = airflow\nworker_container_repository = apache/airflow\nworker_container_tag = 3.0.2\ndelete_worker_pods = True\ndelete_worker_pods_on_failure = False\n\n[logging]\nremote_logging = True\nremote_base_log_folder = s3://bucket/logs\n\n[api]\nauth_backends = airflow.providers.fab.auth_manager.api.backend.basic_auth\n</code></pre>"},{"location":"references/#version-compatibility-matrix","title":"Version Compatibility Matrix","text":"Component Minimum Version Recommended Python 3.9 3.11+ Kubernetes 1.26 1.30+ Helm 3.10 3.14+ PostgreSQL 12 15+ Airflow Helm Chart 1.18.0 Latest"},{"location":"references/#glossary","title":"Glossary","text":"Term Definition Asset Formerly Dataset; represents a data dependency for scheduling DAG Bundle Packaging mechanism for DAG files (replaces --subdir) Task SDK Lightweight runtime for task execution in Airflow 3 API Server New component replacing webserver; serves UI and REST API DAG Processor Component that parses DAG files; now runs separately Task Execution Interface REST API for worker-to-scheduler communication Edge Executor Execute tasks on remote/edge devices Logical Date Replaces execution_date; represents the scheduled time Data Interval Time range a DAG run covers (start, end) <p>Last updated: January 2025 Airflow version: 3.1.5 Helm chart version: 1.18.0</p> <p>_[DAG]: Directed Acyclic Graph - Airflow's core workflow abstraction _[XCom]: Cross-Communication - Airflow's mechanism for task-to-task data passing _[Executor]: The component that runs tasks (Local, Celery, Kubernetes, etc.) _[Scheduler]: The component that triggers DAGs and determines task execution order _[Webserver]: The component that serves the Airflow UI _[Provider]: An Airflow package that integrates with external systems _[TaskFlow]: The modern Python-native API for defining Airflow tasks using decorators _[Asset]: A logical representation of data that DAGs can produce or consume _[SDK]: Software Development Kit - the airflow.sdk module in Airflow 3.x _[K8s]: Kubernetes - container orchestration platform _[uv]: A fast Python package manager by Astral _[ruff]: A fast Python linter and formatter by Astral _[CI]: Continuous Integration _[CD]: Continuous Deployment *[PR]: Pull Request</p>"},{"location":"case-studies/","title":"Case Studies: Real-World Airflow at Scale","text":"<p>This directory contains case studies examining how leading technology companies use Apache Airflow in production. Each case study analyzes architectural patterns, lessons learned, and code examples you can apply to your own workflows.</p>"},{"location":"case-studies/#available-case-studies","title":"Available Case Studies","text":"Case Study Company Focus Area Key Patterns Spotify Recommendations Spotify ML Pipelines Dynamic tasks, ML model training, A/B testing Stripe Fraud Detection Stripe Real-Time ML Low-latency pipelines, feature engineering Airbnb Experimentation Airbnb A/B Testing Experiment orchestration, metrics pipelines Modern RAG Architecture Industry AI/ML RAG pipelines, vector stores, LLM orchestration"},{"location":"case-studies/#how-to-use-these-case-studies","title":"How to Use These Case Studies","text":""},{"location":"case-studies/#for-learning","title":"For Learning","text":"<ol> <li>Read the context - Understand the company's scale and challenges</li> <li>Study the architecture - See how patterns fit together</li> <li>Apply to exercises - Connect patterns to curriculum exercises</li> <li>Reference during projects - Use as templates for real work</li> </ol>"},{"location":"case-studies/#for-production-reference","title":"For Production Reference","text":"<p>Each case study includes:</p> <ul> <li>Architecture diagrams - Visual representation of data flows</li> <li>Pattern breakdown - Which Airflow features solve which problems</li> <li>Code examples - Simplified implementations you can adapt</li> <li>Lessons learned - What worked and what didn't</li> <li>Related exercises - Curriculum exercises that teach these patterns</li> </ul>"},{"location":"case-studies/#pattern-cross-reference","title":"Pattern Cross-Reference","text":"Pattern Case Studies Curriculum Modules Dynamic Task Mapping Spotify, Airbnb Module 06 Deferrable Sensors Stripe, Modern RAG Module 11 KubernetesExecutor All Module 08 Asset-Driven Scheduling Spotify, Modern RAG Module 05 Production Monitoring Stripe, Spotify Module 09 LLM Integration Modern RAG Module 15"},{"location":"case-studies/#contributing","title":"Contributing","text":"<p>These case studies are synthesized from:</p> <ul> <li>Public engineering blogs and conference talks</li> <li>Open-source Airflow provider packages</li> <li>Community discussions and best practices</li> <li>Industry-standard patterns and architectures</li> </ul> <p>The examples are simplified for educational purposes while maintaining authentic patterns.</p> <p>\u2190 Back to Documentation | Module 15: AI/ML \u2192</p> <p>_[DAG]: Directed Acyclic Graph - Airflow's core workflow abstraction _[XCom]: Cross-Communication - Airflow's mechanism for task-to-task data passing _[Executor]: The component that runs tasks (Local, Celery, Kubernetes, etc.) _[Scheduler]: The component that triggers DAGs and determines task execution order _[Webserver]: The component that serves the Airflow UI _[Provider]: An Airflow package that integrates with external systems _[TaskFlow]: The modern Python-native API for defining Airflow tasks using decorators _[Asset]: A logical representation of data that DAGs can produce or consume _[SDK]: Software Development Kit - the airflow.sdk module in Airflow 3.x _[K8s]: Kubernetes - container orchestration platform _[uv]: A fast Python package manager by Astral _[ruff]: A fast Python linter and formatter by Astral _[CI]: Continuous Integration _[CD]: Continuous Deployment *[PR]: Pull Request</p>"},{"location":"case-studies/airbnb-experimentation/","title":"Case Study: Airbnb's Experimentation Platform","text":""},{"location":"case-studies/airbnb-experimentation/#company-context","title":"Company Context","text":"<p>Scale: 7+ million listings, 150+ million users, thousands of concurrent experiments Challenge: Orchestrate A/B tests across search, pricing, and booking flows Requirements: Statistically rigorous analysis, rapid experiment iteration, cross-platform consistency</p>"},{"location":"case-studies/airbnb-experimentation/#architecture-overview","title":"Architecture Overview","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                       AIRBNB EXPERIMENTATION                                 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                             \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                  \u2502\n\u2502  \u2502   Experiment \u2502\u2500\u2500\u2500\u25b6\u2502   User       \u2502\u2500\u2500\u2500\u25b6\u2502   Event      \u2502                  \u2502\n\u2502  \u2502   Config     \u2502    \u2502   Assignment \u2502    \u2502   Collection \u2502                  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                  \u2502\n\u2502        \u2502                                        \u2502                           \u2502\n\u2502        \u25bc                                        \u25bc                           \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                  \u2502\n\u2502  \u2502   Feature    \u2502                         \u2502   Data       \u2502                  \u2502\n\u2502  \u2502   Flags      \u2502                         \u2502   Warehouse  \u2502                  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                  \u2502\n\u2502                                                 \u2502                           \u2502\n\u2502                                                 \u25bc                           \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u2502\n\u2502  \u2502                    AIRFLOW ORCHESTRATION                      \u2502          \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510              \u2502          \u2502\n\u2502  \u2502  \u2502  Metrics   \u2502  \u2502 Analysis   \u2502  \u2502 Decision   \u2502              \u2502          \u2502\n\u2502  \u2502  \u2502  Pipeline  \u2502\u2500\u2500\u2502  Pipeline  \u2502\u2500\u2500\u2502  Pipeline  \u2502              \u2502          \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518              \u2502          \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2502\n\u2502                                                 \u2502                           \u2502\n\u2502                                                 \u25bc                           \u2502\n\u2502                                          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                  \u2502\n\u2502                                          \u2502   Results    \u2502                  \u2502\n\u2502                                          \u2502   Dashboard  \u2502                  \u2502\n\u2502                                          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                  \u2502\n\u2502                                                                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"case-studies/airbnb-experimentation/#key-patterns-used","title":"Key Patterns Used","text":""},{"location":"case-studies/airbnb-experimentation/#1-dynamic-task-mapping-for-experiment-analysis","title":"1. Dynamic Task Mapping for Experiment Analysis","text":"<p>Each experiment requires independent statistical analysis. Dynamic mapping scales automatically:</p> <pre><code>from datetime import datetime\n\nfrom airflow.sdk import dag, task\n\n\n@dag(schedule=\"@daily\", start_date=datetime(2024, 1, 1))\ndef experiment_analysis_pipeline():\n    @task\n    def get_active_experiments() -&gt; list[dict]:\n        \"\"\"Fetch all active experiments requiring analysis.\"\"\"\n        return [\n            {\n                \"id\": \"search_ranking_v2\",\n                \"variants\": [\"control\", \"treatment_a\", \"treatment_b\"],\n                \"metrics\": [\"bookings\", \"revenue\", \"search_clicks\"],\n            },\n            {\n                \"id\": \"pricing_algorithm\",\n                \"variants\": [\"control\", \"ml_pricing\"],\n                \"metrics\": [\"bookings\", \"revenue\", \"host_earnings\"],\n            },\n            {\n                \"id\": \"checkout_flow\",\n                \"variants\": [\"control\", \"simplified\"],\n                \"metrics\": [\"conversion\", \"time_to_book\", \"abandonment\"],\n            },\n        ]\n\n    @task\n    def analyze_experiment(experiment: dict) -&gt; dict:\n        \"\"\"Run statistical analysis for a single experiment.\"\"\"\n        experiment_id = experiment[\"id\"]\n        variants = experiment[\"variants\"]\n        metrics = experiment[\"metrics\"]\n\n        results = {\n            \"experiment_id\": experiment_id,\n            \"analysis_date\": datetime.now().isoformat(),\n            \"metrics\": {},\n        }\n\n        for metric in metrics:\n            # Calculate per-metric statistics\n            stats = calculate_metric_statistics(\n                experiment_id=experiment_id,\n                variants=variants,\n                metric=metric,\n            )\n\n            results[\"metrics\"][metric] = {\n                \"control_mean\": stats[\"control_mean\"],\n                \"treatment_mean\": stats[\"treatment_mean\"],\n                \"lift\": stats[\"lift\"],\n                \"p_value\": stats[\"p_value\"],\n                \"significant\": stats[\"p_value\"] &lt; 0.05,\n                \"confidence_interval\": stats[\"ci\"],\n            }\n\n        return results\n\n    @task\n    def aggregate_results(analyses: list[dict]) -&gt; dict:\n        \"\"\"Combine all experiment analyses into summary.\"\"\"\n        significant_count = sum(1 for a in analyses if any(m[\"significant\"] for m in a[\"metrics\"].values()))\n\n        return {\n            \"total_experiments\": len(analyses),\n            \"significant_results\": significant_count,\n            \"experiments\": analyses,\n        }\n\n    @task\n    def publish_results(summary: dict):\n        \"\"\"Publish to dashboard and notify stakeholders.\"\"\"\n        # Update experimentation dashboard\n        update_dashboard(summary)\n\n        # Notify teams with significant results\n        for exp in summary[\"experiments\"]:\n            if any(m[\"significant\"] for m in exp[\"metrics\"].values()):\n                notify_team(exp[\"experiment_id\"], exp)\n\n    # Dynamic mapping: analyze each experiment in parallel\n    experiments = get_active_experiments()\n    analyses = analyze_experiment.expand(experiment=experiments)\n    summary = aggregate_results(analyses)\n    publish_results(summary)\n</code></pre>"},{"location":"case-studies/airbnb-experimentation/#2-metrics-pipeline-with-quality-gates","title":"2. Metrics Pipeline with Quality Gates","text":"<p>Experiment metrics require validation before analysis:</p> <pre><code>from airflow.sdk import dag, task\n\n\n@dag(schedule=\"@daily\")\ndef experiment_metrics_pipeline():\n    @task\n    def extract_raw_events(experiment_id: str, date: str) -&gt; dict:\n        \"\"\"Extract raw experiment events from data warehouse.\"\"\"\n        events = query_warehouse(\n            f\"\"\"\n            SELECT user_id, variant, event_type, timestamp, properties\n            FROM events\n            WHERE experiment_id = '{experiment_id}'\n              AND date = '{date}'\n            \"\"\"\n        )\n        return {\"experiment_id\": experiment_id, \"event_count\": len(events)}\n\n    @task\n    def validate_data_quality(extraction_result: dict) -&gt; dict:\n        \"\"\"Validate data meets quality thresholds.\"\"\"\n        checks = {\n            \"sufficient_sample_size\": extraction_result[\"event_count\"] &gt;= 1000,\n            \"balanced_variants\": check_variant_balance(extraction_result[\"experiment_id\"]),\n            \"no_assignment_bias\": check_assignment_randomness(extraction_result[\"experiment_id\"]),\n        }\n\n        passed = all(checks.values())\n\n        return {\n            \"experiment_id\": extraction_result[\"experiment_id\"],\n            \"quality_passed\": passed,\n            \"checks\": checks,\n        }\n\n    def decide_analysis_path(validation_result: dict) -&gt; str:\n        \"\"\"Branch based on data quality validation.\"\"\"\n        if validation_result[\"quality_passed\"]:\n            return \"run_full_analysis\"\n        return \"flag_for_review\"\n\n    @task\n    def run_full_analysis(validation_result: dict) -&gt; dict:\n        \"\"\"Execute statistical analysis on validated data.\"\"\"\n        return calculate_experiment_statistics(validation_result[\"experiment_id\"])\n\n    @task\n    def flag_for_review(validation_result: dict):\n        \"\"\"Flag experiment for manual data quality review.\"\"\"\n        create_review_ticket(\n            experiment_id=validation_result[\"experiment_id\"],\n            failed_checks=[k for k, v in validation_result[\"checks\"].items() if not v],\n        )\n</code></pre>"},{"location":"case-studies/airbnb-experimentation/#3-cross-experiment-interaction-detection","title":"3. Cross-Experiment Interaction Detection","text":"<p>Running multiple experiments requires detecting interactions:</p> <pre><code>@dag(schedule=\"@weekly\")\ndef experiment_interaction_detection():\n    @task\n    def get_experiment_pairs() -&gt; list[tuple[str, str]]:\n        \"\"\"Generate pairs of experiments to check for interactions.\"\"\"\n        active_experiments = get_active_experiments()\n        pairs = []\n\n        for i, exp1 in enumerate(active_experiments):\n            for exp2 in active_experiments[i + 1 :]:\n                # Only check overlapping experiments\n                if experiments_overlap(exp1, exp2):\n                    pairs.append((exp1[\"id\"], exp2[\"id\"]))\n\n        return pairs\n\n    @task\n    def detect_interaction(pair: tuple[str, str]) -&gt; dict:\n        \"\"\"Detect statistical interaction between experiment pair.\"\"\"\n        exp1_id, exp2_id = pair\n\n        # Get users in both experiments\n        overlap_users = get_overlapping_users(exp1_id, exp2_id)\n\n        if len(overlap_users) &lt; 100:\n            return {\n                \"pair\": pair,\n                \"interaction_detected\": False,\n                \"reason\": \"insufficient_overlap\",\n            }\n\n        # Run interaction analysis\n        interaction_stats = calculate_interaction_effect(exp1_id, exp2_id, overlap_users)\n\n        return {\n            \"pair\": pair,\n            \"interaction_detected\": interaction_stats[\"significant\"],\n            \"interaction_effect\": interaction_stats[\"effect_size\"],\n            \"p_value\": interaction_stats[\"p_value\"],\n        }\n\n    @task\n    def report_interactions(results: list[dict]) -&gt; dict:\n        \"\"\"Report any detected interactions to experiment owners.\"\"\"\n        interactions = [r for r in results if r[\"interaction_detected\"]]\n\n        for interaction in interactions:\n            alert_experiment_owners(\n                experiments=interaction[\"pair\"],\n                effect=interaction[\"interaction_effect\"],\n            )\n\n        return {\n            \"pairs_analyzed\": len(results),\n            \"interactions_found\": len(interactions),\n        }\n\n    pairs = get_experiment_pairs()\n    interaction_results = detect_interaction.expand(pair=pairs)\n    report_interactions(interaction_results)\n</code></pre>"},{"location":"case-studies/airbnb-experimentation/#4-experiment-lifecycle-management","title":"4. Experiment Lifecycle Management","text":"<p>Experiments have defined lifecycles managed through Airflow:</p> <pre><code>from datetime import datetime\n\nfrom airflow.sdk import dag, task\n\n\n@dag(schedule=\"@hourly\")\ndef experiment_lifecycle_manager():\n    @task\n    def check_experiment_status() -&gt; list[dict]:\n        \"\"\"Check status of all experiments.\"\"\"\n        experiments = get_all_experiments()\n        status_updates = []\n\n        for exp in experiments:\n            if exp[\"state\"] == \"ramping\":\n                # Check if ramp-up complete\n                if datetime.now() &gt; exp[\"ramp_end_date\"]:\n                    status_updates.append(\n                        {\n                            \"id\": exp[\"id\"],\n                            \"action\": \"complete_ramp\",\n                            \"new_allocation\": 1.0,\n                        }\n                    )\n\n            elif exp[\"state\"] == \"running\":\n                # Check if experiment has reached statistical power\n                power = calculate_current_power(exp[\"id\"])\n                if power &gt;= 0.8:\n                    status_updates.append(\n                        {\n                            \"id\": exp[\"id\"],\n                            \"action\": \"mark_powered\",\n                            \"power\": power,\n                        }\n                    )\n\n                # Check for early stopping (harm detection)\n                if detect_significant_harm(exp[\"id\"]):\n                    status_updates.append(\n                        {\n                            \"id\": exp[\"id\"],\n                            \"action\": \"emergency_stop\",\n                            \"reason\": \"significant_harm_detected\",\n                        }\n                    )\n\n            elif exp[\"state\"] == \"analyzing\":\n                # Check if analysis window complete\n                if datetime.now() &gt; exp[\"analysis_end_date\"]:\n                    status_updates.append(\n                        {\n                            \"id\": exp[\"id\"],\n                            \"action\": \"complete_analysis\",\n                        }\n                    )\n\n        return status_updates\n\n    @task\n    def apply_status_updates(updates: list[dict]):\n        \"\"\"Apply lifecycle updates to experiments.\"\"\"\n        for update in updates:\n            exp_id = update[\"id\"]\n            action = update[\"action\"]\n\n            if action == \"complete_ramp\":\n                update_experiment_allocation(exp_id, 1.0)\n                transition_experiment_state(exp_id, \"running\")\n\n            elif action == \"mark_powered\":\n                log_experiment_powered(exp_id, update[\"power\"])\n\n            elif action == \"emergency_stop\":\n                stop_experiment(exp_id, reason=update[\"reason\"])\n                notify_experiment_owners(exp_id, \"EMERGENCY_STOP\", update)\n\n            elif action == \"complete_analysis\":\n                transition_experiment_state(exp_id, \"complete\")\n                generate_final_report(exp_id)\n\n    updates = check_experiment_status()\n    apply_status_updates(updates)\n</code></pre>"},{"location":"case-studies/airbnb-experimentation/#lessons-learned","title":"Lessons Learned","text":""},{"location":"case-studies/airbnb-experimentation/#what-worked","title":"What Worked","text":"<ol> <li>Dynamic mapping for experiment parallelism - Scaled from 10 to 1000+ concurrent experiments</li> <li>Quality gates before analysis - Prevented bad data from polluting results</li> <li>Automated interaction detection - Caught cross-experiment effects early</li> <li>Lifecycle automation - Reduced manual experiment management overhead</li> </ol>"},{"location":"case-studies/airbnb-experimentation/#challenges-encountered","title":"Challenges Encountered","text":"<ol> <li>Backfill complexity - Historical analysis required careful date handling</li> <li>Metric definition consistency - Required centralized metric catalog</li> <li>Real-time vs. batch tradeoffs - Some decisions needed faster signals</li> </ol>"},{"location":"case-studies/airbnb-experimentation/#key-metrics","title":"Key Metrics","text":"Metric Before Airflow After Airflow Experiments per quarter 50 500+ Analysis latency 48 hours 4 hours Data quality issues 15% of experiments 2% of experiments Interaction detection Manual Automated"},{"location":"case-studies/airbnb-experimentation/#code-patterns","title":"Code Patterns","text":""},{"location":"case-studies/airbnb-experimentation/#pattern-parallel-experiment-analysis","title":"Pattern: Parallel Experiment Analysis","text":"<pre><code># From Module 06: Dynamic Task Mapping\n@task\ndef get_experiments() -&gt; list[dict]:\n    return [{\"id\": \"exp1\"}, {\"id\": \"exp2\"}, {\"id\": \"exp3\"}]\n\n\n@task\ndef analyze(exp: dict) -&gt; dict:\n    \"\"\"Parallel analysis for each experiment.\"\"\"\n    return {\n        \"id\": exp[\"id\"],\n        \"result\": run_analysis(exp[\"id\"]),\n    }\n\n\n# Automatic parallelization\nexperiments = get_experiments()\nresults = analyze.expand(exp=experiments)\n</code></pre>"},{"location":"case-studies/airbnb-experimentation/#pattern-statistical-quality-gates","title":"Pattern: Statistical Quality Gates","text":"<pre><code>@task\ndef validate_before_analysis(experiment_id: str) -&gt; bool:\n    \"\"\"Gate analysis on data quality.\"\"\"\n    checks = [\n        check_sample_size(experiment_id) &gt;= 1000,\n        check_variant_balance(experiment_id) &gt; 0.95,\n        check_srm_test(experiment_id)[\"passed\"],\n    ]\n    return all(checks)\n</code></pre>"},{"location":"case-studies/airbnb-experimentation/#related-exercises","title":"Related Exercises","text":"Exercise Concepts Applied Exercise 6.4: Parallel Embeddings Dynamic mapping for parallel processing Exercise 9.4: LLM Retry Patterns Quality gates and validation patterns Exercise 15.3: Data Prep Data quality pipelines"},{"location":"case-studies/airbnb-experimentation/#further-reading","title":"Further Reading","text":"<ul> <li>Airbnb Engineering: Experimentation Platform</li> <li>Building a Scalable Experimentation Platform</li> <li>Statistical Rigor in A/B Testing</li> </ul> <p>\u2190 Stripe Fraud Detection | Modern RAG Architecture \u2192</p> <p>_[DAG]: Directed Acyclic Graph - Airflow's core workflow abstraction _[XCom]: Cross-Communication - Airflow's mechanism for task-to-task data passing _[Executor]: The component that runs tasks (Local, Celery, Kubernetes, etc.) _[Scheduler]: The component that triggers DAGs and determines task execution order _[Webserver]: The component that serves the Airflow UI _[Provider]: An Airflow package that integrates with external systems _[TaskFlow]: The modern Python-native API for defining Airflow tasks using decorators _[Asset]: A logical representation of data that DAGs can produce or consume _[SDK]: Software Development Kit - the airflow.sdk module in Airflow 3.x _[K8s]: Kubernetes - container orchestration platform _[uv]: A fast Python package manager by Astral _[ruff]: A fast Python linter and formatter by Astral _[CI]: Continuous Integration _[CD]: Continuous Deployment *[PR]: Pull Request</p>"},{"location":"case-studies/modern-rag-architecture/","title":"Case Study: Modern RAG Pipeline Architecture","text":""},{"location":"case-studies/modern-rag-architecture/#industry-context","title":"Industry Context","text":"<p>Trend: Retrieval-Augmented Generation (RAG) has become the standard pattern for production LLM applications Challenge: Orchestrate complex document processing, embedding generation, and retrieval pipelines Requirements: Maintain freshness, ensure quality, control costs, scale horizontally</p>"},{"location":"case-studies/modern-rag-architecture/#architecture-overview","title":"Architecture Overview","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                          MODERN RAG ARCHITECTURE                             \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                             \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                  \u2502\n\u2502  \u2502   Document   \u2502\u2500\u2500\u2500\u25b6\u2502   Content    \u2502\u2500\u2500\u2500\u25b6\u2502   Chunking   \u2502                  \u2502\n\u2502  \u2502   Sources    \u2502    \u2502   Extraction \u2502    \u2502   Strategy   \u2502                  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                  \u2502\n\u2502                                                \u2502                           \u2502\n\u2502                                                \u25bc                           \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510            \u2502\n\u2502  \u2502                    AIRFLOW ORCHESTRATION                    \u2502            \u2502\n\u2502  \u2502                                                              \u2502            \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510            \u2502            \u2502\n\u2502  \u2502  \u2502  Embedding \u2502  \u2502   Vector   \u2502  \u2502   Index    \u2502            \u2502            \u2502\n\u2502  \u2502  \u2502  Pipeline  \u2502\u2500\u2500\u2502   Store    \u2502\u2500\u2500\u2502   Refresh  \u2502            \u2502            \u2502\n\u2502  \u2502  \u2502  (Batch)   \u2502  \u2502   Upsert   \u2502  \u2502   Sensor   \u2502            \u2502            \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518            \u2502            \u2502\n\u2502  \u2502                                                              \u2502            \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518            \u2502\n\u2502                                                \u2502                           \u2502\n\u2502                                                \u25bc                           \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                  \u2502\n\u2502  \u2502   Query      \u2502\u2500\u2500\u2500\u25b6\u2502   Retrieval  \u2502\u2500\u2500\u2500\u25b6\u2502   LLM        \u2502                  \u2502\n\u2502  \u2502   Router     \u2502    \u2502   Engine     \u2502    \u2502   Generation \u2502                  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                  \u2502\n\u2502                                                                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"case-studies/modern-rag-architecture/#key-patterns-used","title":"Key Patterns Used","text":""},{"location":"case-studies/modern-rag-architecture/#1-document-ingestion-with-dynamic-mapping","title":"1. Document Ingestion with Dynamic Mapping","text":"<p>Process documents in parallel with intelligent chunking:</p> <pre><code>from datetime import datetime\n\nfrom airflow.sdk import Asset, dag, task\n\n# Assets for triggering downstream pipelines\nraw_documents = Asset(\"rag/raw_documents\")\ndocument_chunks = Asset(\"rag/chunks\")\nvector_embeddings = Asset(\"rag/embeddings\")\n\n\n@dag(\n    schedule=\"@daily\",\n    start_date=datetime(2024, 1, 1),\n    tags=[\"rag\", \"ingestion\"],\n)\ndef rag_document_ingestion():\n    @task\n    def discover_documents(source_path: str) -&gt; list[dict]:\n        \"\"\"Discover new/modified documents for processing.\"\"\"\n        documents = []\n\n        for doc in scan_directory(source_path):\n            if is_new_or_modified(doc):\n                documents.append(\n                    {\n                        \"path\": doc.path,\n                        \"type\": doc.extension,\n                        \"size\": doc.size,\n                        \"modified\": doc.modified_at.isoformat(),\n                    }\n                )\n\n        return documents\n\n    @task\n    def extract_content(doc_info: dict) -&gt; dict:\n        \"\"\"Extract text content from document.\"\"\"\n        doc_type = doc_info[\"type\"]\n        path = doc_info[\"path\"]\n\n        # Route to appropriate extractor\n        if doc_type == \"pdf\":\n            content = extract_pdf(path)\n        elif doc_type == \"docx\":\n            content = extract_docx(path)\n        elif doc_type == \"html\":\n            content = extract_html(path)\n        else:\n            content = extract_text(path)\n\n        return {\n            \"path\": path,\n            \"content\": content,\n            \"char_count\": len(content),\n            \"extracted_at\": datetime.now().isoformat(),\n        }\n\n    @task\n    def chunk_document(extracted: dict) -&gt; list[dict]:\n        \"\"\"Split document into semantic chunks.\"\"\"\n        content = extracted[\"content\"]\n        path = extracted[\"path\"]\n\n        # Semantic chunking with overlap\n        chunks = semantic_chunk(\n            text=content,\n            chunk_size=512,\n            overlap=50,\n            separators=[\"\\n\\n\", \"\\n\", \". \", \" \"],\n        )\n\n        return [\n            {\n                \"doc_path\": path,\n                \"chunk_index\": i,\n                \"content\": chunk,\n                \"char_count\": len(chunk),\n            }\n            for i, chunk in enumerate(chunks)\n        ]\n\n    @task(outlets=[document_chunks])\n    def persist_chunks(all_chunks: list[list[dict]]) -&gt; dict:\n        \"\"\"Flatten and persist all chunks.\"\"\"\n        flat_chunks = [c for chunks in all_chunks for c in chunks]\n\n        # Store chunks with metadata\n        chunk_ids = store_chunks(flat_chunks)\n\n        return {\n            \"documents_processed\": len(all_chunks),\n            \"total_chunks\": len(flat_chunks),\n            \"chunk_ids\": chunk_ids,\n        }\n\n    # Pipeline flow with dynamic mapping\n    docs = discover_documents(\"/data/documents\")\n    extracted = extract_content.expand(doc_info=docs)\n    chunks = chunk_document.expand(extracted=extracted)\n    persist_chunks(chunks)\n</code></pre>"},{"location":"case-studies/modern-rag-architecture/#2-parallel-embedding-generation-with-rate-limiting","title":"2. Parallel Embedding Generation with Rate Limiting","text":"<p>Generate embeddings efficiently while respecting API limits:</p> <pre><code>import time\nfrom dataclasses import dataclass, field\n\n\n@dataclass\nclass EmbeddingRateLimiter:\n    \"\"\"Rate limiter for embedding API calls.\"\"\"\n\n    tokens_per_minute: int = 150000\n    requests_per_minute: int = 3000\n    _token_count: int = 0\n    _request_count: int = 0\n    _window_start: float = field(default_factory=time.time)\n\n    def wait_if_needed(self, estimated_tokens: int):\n        \"\"\"Wait if rate limits would be exceeded.\"\"\"\n        current_time = time.time()\n        elapsed = current_time - self._window_start\n\n        # Reset window every minute\n        if elapsed &gt;= 60:\n            self._token_count = 0\n            self._request_count = 0\n            self._window_start = current_time\n            return\n\n        # Check if we need to wait\n        if (\n            self._token_count + estimated_tokens &gt; self.tokens_per_minute\n            or self._request_count + 1 &gt; self.requests_per_minute\n        ):\n            sleep_time = 60 - elapsed\n            time.sleep(sleep_time)\n            self._token_count = 0\n            self._request_count = 0\n            self._window_start = time.time()\n\n        self._token_count += estimated_tokens\n        self._request_count += 1\n\n\n@dag(\n    schedule=[document_chunks],  # Triggered by chunk asset\n    tags=[\"rag\", \"embeddings\"],\n)\ndef rag_embedding_pipeline():\n    @task\n    def get_pending_chunks() -&gt; list[dict]:\n        \"\"\"Get chunks that need embedding generation.\"\"\"\n        return query_chunks_without_embeddings(limit=10000)\n\n    @task\n    def batch_chunks(chunks: list[dict], batch_size: int = 100) -&gt; list[list[dict]]:\n        \"\"\"Batch chunks for efficient API calls.\"\"\"\n        return [chunks[i : i + batch_size] for i in range(0, len(chunks), batch_size)]\n\n    @task\n    def generate_embeddings(batch: list[dict]) -&gt; list[dict]:\n        \"\"\"Generate embeddings for a batch of chunks.\"\"\"\n        rate_limiter = EmbeddingRateLimiter()\n\n        results = []\n        texts = [chunk[\"content\"] for chunk in batch]\n\n        # Estimate tokens (rough approximation)\n        estimated_tokens = sum(len(t) // 4 for t in texts)\n        rate_limiter.wait_if_needed(estimated_tokens)\n\n        # Generate embeddings\n        embeddings = embedding_model.embed(texts)\n\n        for chunk, embedding in zip(batch, embeddings):\n            results.append(\n                {\n                    \"chunk_id\": chunk[\"chunk_id\"],\n                    \"embedding\": embedding,\n                    \"model\": \"text-embedding-3-small\",\n                    \"dimensions\": len(embedding),\n                }\n            )\n\n        return results\n\n    @task(outlets=[vector_embeddings])\n    def store_embeddings(all_embeddings: list[list[dict]]) -&gt; dict:\n        \"\"\"Store embeddings in vector database.\"\"\"\n        flat_embeddings = [e for batch in all_embeddings for e in batch]\n\n        # Upsert to vector store\n        upsert_result = vector_store.upsert(\n            vectors=[\n                {\n                    \"id\": e[\"chunk_id\"],\n                    \"values\": e[\"embedding\"],\n                    \"metadata\": {\"model\": e[\"model\"]},\n                }\n                for e in flat_embeddings\n            ]\n        )\n\n        return {\n            \"embeddings_stored\": len(flat_embeddings),\n            \"upsert_status\": upsert_result,\n        }\n\n    chunks = get_pending_chunks()\n    batches = batch_chunks(chunks)\n    embeddings = generate_embeddings.expand(batch=batches)\n    store_embeddings(embeddings)\n</code></pre>"},{"location":"case-studies/modern-rag-architecture/#3-vector-store-sensor-for-index-readiness","title":"3. Vector Store Sensor for Index Readiness","text":"<p>Use deferrable sensors to wait for index updates:</p> <pre><code>import asyncio\nfrom collections.abc import AsyncIterator\n\nfrom airflow.sensors.base import BaseSensorOperator\nfrom airflow.triggers.base import BaseTrigger, TriggerEvent\n\n\nclass VectorIndexTrigger(BaseTrigger):\n    \"\"\"Trigger that waits for vector index to be ready.\"\"\"\n\n    def __init__(\n        self,\n        index_name: str,\n        expected_vectors: int,\n        poll_interval: float = 30.0,\n        timeout: float = 3600.0,\n    ):\n        super().__init__()\n        self.index_name = index_name\n        self.expected_vectors = expected_vectors\n        self.poll_interval = poll_interval\n        self.timeout = timeout\n\n    def serialize(self):\n        return (\n            f\"{self.__class__.__module__}.{self.__class__.__name__}\",\n            {\n                \"index_name\": self.index_name,\n                \"expected_vectors\": self.expected_vectors,\n                \"poll_interval\": self.poll_interval,\n                \"timeout\": self.timeout,\n            },\n        )\n\n    async def run(self) -&gt; AsyncIterator[TriggerEvent]:\n        \"\"\"Poll for index readiness.\"\"\"\n        from datetime import datetime, timedelta\n\n        start_time = datetime.utcnow()\n        timeout_at = start_time + timedelta(seconds=self.timeout)\n\n        while datetime.utcnow() &lt; timeout_at:\n            stats = await self._get_index_stats()\n\n            if stats[\"vector_count\"] &gt;= self.expected_vectors:\n                if stats[\"status\"] == \"ready\":\n                    yield TriggerEvent(\n                        {\n                            \"status\": \"success\",\n                            \"index\": self.index_name,\n                            \"vector_count\": stats[\"vector_count\"],\n                        }\n                    )\n                    return\n\n            self.log.info(f\"Index {self.index_name}: {stats['vector_count']}/{self.expected_vectors} vectors\")\n            await asyncio.sleep(self.poll_interval)\n\n        yield TriggerEvent(\n            {\n                \"status\": \"timeout\",\n                \"message\": f\"Index not ready after {self.timeout}s\",\n            }\n        )\n\n\nclass VectorIndexSensor(BaseSensorOperator):\n    \"\"\"Deferrable sensor for vector index readiness.\"\"\"\n\n    def __init__(\n        self,\n        index_name: str,\n        expected_vectors: int,\n        deferrable: bool = True,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n        self.index_name = index_name\n        self.expected_vectors = expected_vectors\n        self._deferrable = deferrable\n\n    def execute(self, context):\n        # Quick check first\n        stats = get_index_stats(self.index_name)\n        if stats[\"vector_count\"] &gt;= self.expected_vectors:\n            return stats\n\n        if self._deferrable:\n            self.defer(\n                trigger=VectorIndexTrigger(\n                    index_name=self.index_name,\n                    expected_vectors=self.expected_vectors,\n                    timeout=self.timeout,\n                ),\n                method_name=\"execute_complete\",\n            )\n        else:\n            super().execute(context)\n\n    def execute_complete(self, context, event):\n        if event[\"status\"] == \"success\":\n            return event\n        raise AirflowException(event[\"message\"])\n</code></pre>"},{"location":"case-studies/modern-rag-architecture/#4-llm-chain-orchestration-with-cost-tracking","title":"4. LLM Chain Orchestration with Cost Tracking","text":"<p>Multi-step LLM pipelines with cost monitoring:</p> <pre><code>from dataclasses import dataclass, field\n\n\n@dataclass\nclass LLMCostTracker:\n    \"\"\"Track LLM API costs across pipeline.\"\"\"\n\n    daily_budget: float = 100.0\n    current_spend: float = 0.0\n    call_history: list = field(default_factory=list)\n\n    # Pricing per 1K tokens (example rates)\n    PRICING = {\n        \"gpt-4o\": {\"input\": 0.005, \"output\": 0.015},\n        \"gpt-4o-mini\": {\"input\": 0.00015, \"output\": 0.0006},\n        \"claude-3-5-sonnet\": {\"input\": 0.003, \"output\": 0.015},\n    }\n\n    def track_call(\n        self,\n        model: str,\n        input_tokens: int,\n        output_tokens: int,\n        operation: str,\n    ) -&gt; float:\n        \"\"\"Track an LLM call and return cost.\"\"\"\n        pricing = self.PRICING.get(model, self.PRICING[\"gpt-4o-mini\"])\n\n        cost = input_tokens * pricing[\"input\"] / 1000 + output_tokens * pricing[\"output\"] / 1000\n\n        self.current_spend += cost\n        self.call_history.append(\n            {\n                \"model\": model,\n                \"operation\": operation,\n                \"input_tokens\": input_tokens,\n                \"output_tokens\": output_tokens,\n                \"cost\": cost,\n            }\n        )\n\n        if self.current_spend &gt; self.daily_budget * 0.8:\n            self.log.warning(f\"LLM spend at {self.current_spend / self.daily_budget:.0%} of budget\")\n\n        return cost\n\n    def get_summary(self) -&gt; dict:\n        \"\"\"Get cost summary.\"\"\"\n        return {\n            \"total_spend\": self.current_spend,\n            \"budget_remaining\": self.daily_budget - self.current_spend,\n            \"calls\": len(self.call_history),\n            \"by_operation\": self._group_by_operation(),\n        }\n\n\n@dag(schedule=\"@hourly\", tags=[\"rag\", \"generation\"])\ndef rag_query_pipeline():\n    @task\n    def get_pending_queries() -&gt; list[dict]:\n        \"\"\"Get queries waiting for processing.\"\"\"\n        return fetch_pending_queries(limit=100)\n\n    @task\n    def retrieve_context(query: dict) -&gt; dict:\n        \"\"\"Retrieve relevant context from vector store.\"\"\"\n        # Generate query embedding\n        query_embedding = embedding_model.embed(query[\"text\"])\n\n        # Search vector store\n        results = vector_store.query(\n            vector=query_embedding,\n            top_k=5,\n            include_metadata=True,\n        )\n\n        return {\n            \"query_id\": query[\"id\"],\n            \"query_text\": query[\"text\"],\n            \"context\": [r[\"metadata\"][\"content\"] for r in results],\n            \"scores\": [r[\"score\"] for r in results],\n        }\n\n    @task\n    def generate_response(retrieval_result: dict) -&gt; dict:\n        \"\"\"Generate response using retrieved context.\"\"\"\n        cost_tracker = LLMCostTracker()\n\n        query = retrieval_result[\"query_text\"]\n        context = \"\\n\\n\".join(retrieval_result[\"context\"])\n\n        # Build prompt\n        prompt = f\"\"\"Based on the following context, answer the question.\n\nContext:\n{context}\n\nQuestion: {query}\n\nAnswer:\"\"\"\n\n        # Generate response\n        response = llm.generate(\n            model=\"gpt-4o-mini\",\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n        )\n\n        # Track cost\n        cost_tracker.track_call(\n            model=\"gpt-4o-mini\",\n            input_tokens=response[\"usage\"][\"prompt_tokens\"],\n            output_tokens=response[\"usage\"][\"completion_tokens\"],\n            operation=\"generate_response\",\n        )\n\n        return {\n            \"query_id\": retrieval_result[\"query_id\"],\n            \"response\": response[\"content\"],\n            \"cost\": cost_tracker.get_summary(),\n            \"sources\": retrieval_result[\"context\"][:3],\n        }\n\n    @task\n    def store_responses(responses: list[dict]) -&gt; dict:\n        \"\"\"Store generated responses.\"\"\"\n        total_cost = sum(r[\"cost\"][\"total_spend\"] for r in responses)\n\n        store_query_responses(responses)\n\n        return {\n            \"responses_generated\": len(responses),\n            \"total_cost\": total_cost,\n        }\n\n    queries = get_pending_queries()\n    retrieval_results = retrieve_context.expand(query=queries)\n    responses = generate_response.expand(retrieval_result=retrieval_results)\n    store_responses(responses)\n</code></pre>"},{"location":"case-studies/modern-rag-architecture/#lessons-learned","title":"Lessons Learned","text":""},{"location":"case-studies/modern-rag-architecture/#what-worked","title":"What Worked","text":"<ol> <li>Asset-driven pipelines - Documents \u2192 Chunks \u2192 Embeddings cascade automatically</li> <li>Dynamic mapping for parallelism - Scales with document volume</li> <li>Deferrable sensors for indexing - Long indexing doesn't block workers</li> <li>Cost tracking integration - Prevents runaway LLM costs</li> </ol>"},{"location":"case-studies/modern-rag-architecture/#challenges-encountered","title":"Challenges Encountered","text":"<ol> <li>Chunking strategy selection - Required experimentation for optimal retrieval</li> <li>Embedding model versioning - Model changes require re-embedding</li> <li>Context window management - Large retrievals exceeded LLM limits</li> <li>Latency vs. freshness - Batch processing adds delay to knowledge updates</li> </ol>"},{"location":"case-studies/modern-rag-architecture/#key-metrics","title":"Key Metrics","text":"Metric Typical Values Document \u2192 Queryable latency 15-60 minutes Embedding generation cost $0.0001/chunk Vector upsert throughput 1000 vectors/second Query generation latency 2-5 seconds"},{"location":"case-studies/modern-rag-architecture/#code-patterns","title":"Code Patterns","text":""},{"location":"case-studies/modern-rag-architecture/#pattern-asset-driven-rag-pipeline","title":"Pattern: Asset-Driven RAG Pipeline","text":"<pre><code>from airflow.sdk import Asset\n\nraw_docs = Asset(\"rag/documents\")\nchunks = Asset(\"rag/chunks\")\nembeddings = Asset(\"rag/embeddings\")\n\n\n# Triggered by document changes\n@dag(schedule=[raw_docs])\ndef chunk_pipeline():\n    pass\n\n\n# Triggered by new chunks\n@dag(schedule=[chunks])\ndef embedding_pipeline():\n    pass\n</code></pre>"},{"location":"case-studies/modern-rag-architecture/#pattern-batch-embedding-with-rate-limiting","title":"Pattern: Batch Embedding with Rate Limiting","text":"<pre><code>@task\ndef generate_embeddings_batch(texts: list[str]) -&gt; list[list[float]]:\n    \"\"\"Generate embeddings with rate limiting.\"\"\"\n    limiter = TokenBucketRateLimiter(tokens_per_minute=150000)\n\n    embeddings = []\n    for batch in batch_texts(texts, size=100):\n        tokens = estimate_tokens(batch)\n        limiter.wait_if_needed(tokens)\n        embeddings.extend(embed_batch(batch))\n\n    return embeddings\n</code></pre>"},{"location":"case-studies/modern-rag-architecture/#related-exercises","title":"Related Exercises","text":"Exercise Concepts Applied Exercise 15.1: RAG Pipeline Complete RAG ingestion pipeline Exercise 15.2: LLM Chain Multi-step LLM orchestration Exercise 11.4: Vector Store Sensor Deferrable sensors for indexing Exercise 6.4: Parallel Embeddings Dynamic mapping for parallel processing"},{"location":"case-studies/modern-rag-architecture/#further-reading","title":"Further Reading","text":"<ul> <li>LangChain + Airflow Integration</li> <li>Pinecone Best Practices</li> <li>OpenAI Embeddings Guide</li> <li>Anthropic Claude Best Practices</li> </ul> <p>\u2190 Airbnb Experimentation | Back to Case Studies</p> <p>_[DAG]: Directed Acyclic Graph - Airflow's core workflow abstraction _[XCom]: Cross-Communication - Airflow's mechanism for task-to-task data passing _[Executor]: The component that runs tasks (Local, Celery, Kubernetes, etc.) _[Scheduler]: The component that triggers DAGs and determines task execution order _[Webserver]: The component that serves the Airflow UI _[Provider]: An Airflow package that integrates with external systems _[TaskFlow]: The modern Python-native API for defining Airflow tasks using decorators _[Asset]: A logical representation of data that DAGs can produce or consume _[SDK]: Software Development Kit - the airflow.sdk module in Airflow 3.x _[K8s]: Kubernetes - container orchestration platform _[uv]: A fast Python package manager by Astral _[ruff]: A fast Python linter and formatter by Astral _[CI]: Continuous Integration _[CD]: Continuous Deployment *[PR]: Pull Request</p>"},{"location":"case-studies/spotify-recommendations/","title":"Case Study: Spotify's Recommendation Pipelines","text":""},{"location":"case-studies/spotify-recommendations/#company-context","title":"Company Context","text":"<p>Scale: 600+ million users, 100+ million tracks, billions of daily streams Challenge: Generate personalized recommendations across multiple surfaces (Discover Weekly, Release Radar, Home feed) Requirements: Train models on petabytes of listening data, serve fresh recommendations daily</p>"},{"location":"case-studies/spotify-recommendations/#architecture-overview","title":"Architecture Overview","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                         SPOTIFY RECOMMENDATIONS                              \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                             \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                  \u2502\n\u2502  \u2502   Event      \u2502\u2500\u2500\u2500\u25b6\u2502   Feature    \u2502\u2500\u2500\u2500\u25b6\u2502   Model      \u2502                  \u2502\n\u2502  \u2502   Ingestion  \u2502    \u2502   Pipeline   \u2502    \u2502   Training   \u2502                  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                  \u2502\n\u2502        \u2502                   \u2502                    \u2502                           \u2502\n\u2502        \u25bc                   \u25bc                    \u25bc                           \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                  \u2502\n\u2502  \u2502   Stream     \u2502    \u2502   Feature    \u2502    \u2502   Model      \u2502                  \u2502\n\u2502  \u2502   Processing \u2502    \u2502   Store      \u2502    \u2502   Registry   \u2502                  \u2502\n\u2502  \u2502   (Kafka)    \u2502    \u2502   (BigQuery) \u2502    \u2502   (MLflow)   \u2502                  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                  \u2502\n\u2502                                                \u2502                           \u2502\n\u2502                                                \u25bc                           \u2502\n\u2502                                          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                  \u2502\n\u2502                                          \u2502   A/B Test   \u2502                  \u2502\n\u2502                                          \u2502   Framework  \u2502                  \u2502\n\u2502                                          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                  \u2502\n\u2502                                                \u2502                           \u2502\n\u2502                                                \u25bc                           \u2502\n\u2502                                          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                  \u2502\n\u2502                                          \u2502   Serving    \u2502                  \u2502\n\u2502                                          \u2502   Layer      \u2502                  \u2502\n\u2502                                          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                  \u2502\n\u2502                                                                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"case-studies/spotify-recommendations/#key-patterns-used","title":"Key Patterns Used","text":""},{"location":"case-studies/spotify-recommendations/#1-dynamic-task-mapping-for-user-segments","title":"1. Dynamic Task Mapping for User Segments","text":"<p>Spotify processes different user segments with tailored models. Dynamic mapping enables parallel processing:</p> <pre><code>from datetime import datetime\n\nfrom airflow.sdk import dag, task\n\n\n@dag(schedule=\"@daily\", start_date=datetime(2024, 1, 1))\ndef recommendation_pipeline():\n    @task\n    def get_user_segments() -&gt; list[dict]:\n        \"\"\"Identify user segments for parallel processing.\"\"\"\n        return [\n            {\"segment\": \"new_users\", \"model\": \"explore_heavy\"},\n            {\"segment\": \"power_listeners\", \"model\": \"personalization_deep\"},\n            {\"segment\": \"casual_listeners\", \"model\": \"popularity_blend\"},\n            {\"segment\": \"genre_focused\", \"model\": \"genre_specialist\"},\n        ]\n\n    @task\n    def generate_recommendations(segment_config: dict) -&gt; dict:\n        \"\"\"Generate recommendations for a user segment.\"\"\"\n        segment = segment_config[\"segment\"]\n        model = segment_config[\"model\"]\n\n        # Each segment processes millions of users in parallel\n        # Uses segment-specific model and features\n        recommendations = run_recommendation_model(\n            segment=segment,\n            model_id=model,\n        )\n\n        return {\n            \"segment\": segment,\n            \"users_processed\": recommendations[\"count\"],\n            \"model_version\": model,\n        }\n\n    @task\n    def aggregate_metrics(results: list[dict]) -&gt; dict:\n        \"\"\"Combine metrics across all segments.\"\"\"\n        total_users = sum(r[\"users_processed\"] for r in results)\n        return {\"total_users\": total_users, \"segments\": len(results)}\n\n    # Dynamic mapping: one task per segment, running in parallel\n    segments = get_user_segments()\n    results = generate_recommendations.expand(segment_config=segments)\n    aggregate_metrics(results)\n</code></pre>"},{"location":"case-studies/spotify-recommendations/#2-asset-driven-feature-pipelines","title":"2. Asset-Driven Feature Pipelines","text":"<p>Features are computed as Assets, triggering downstream model training:</p> <pre><code>from airflow.sdk import Asset, dag, task\n\n# Feature assets that models depend on\nlistening_features = Asset(\"features/listening_history\")\naudio_features = Asset(\"features/audio_embeddings\")\nsocial_features = Asset(\"features/social_graph\")\n\n\n@dag(schedule=None)  # Triggered by upstream data\ndef feature_pipeline():\n    @task(outlets=[listening_features])\n    def compute_listening_features():\n        \"\"\"Daily listening pattern features.\"\"\"\n        # Compute user listening patterns from event logs\n        pass\n\n    @task(outlets=[audio_features])\n    def compute_audio_features():\n        \"\"\"Audio embedding features from track analysis.\"\"\"\n        # Generate embeddings from audio analysis\n        pass\n\n\n# Model training triggered when features update\n@dag(\n    schedule=[listening_features, audio_features, social_features],\n    start_date=datetime(2024, 1, 1),\n)\ndef model_training():\n    @task\n    def train_recommendation_model():\n        \"\"\"Train on latest features.\"\"\"\n        # Triggered automatically when any feature asset updates\n        pass\n</code></pre>"},{"location":"case-studies/spotify-recommendations/#3-ab-test-orchestration","title":"3. A/B Test Orchestration","text":"<p>New models are tested through controlled experiments:</p> <pre><code>@dag(schedule=\"@daily\")\ndef ab_test_pipeline():\n    @task\n    def setup_experiment(experiment_id: str) -&gt; dict:\n        \"\"\"Configure A/B test parameters.\"\"\"\n        return {\n            \"experiment_id\": experiment_id,\n            \"control\": \"model_v2.1\",\n            \"treatment\": \"model_v2.2\",\n            \"allocation\": 0.10,  # 10% of users\n        }\n\n    @task\n    def allocate_users(config: dict) -&gt; dict:\n        \"\"\"Assign users to control/treatment groups.\"\"\"\n        # Consistent hashing ensures stable assignment\n        pass\n\n    @task\n    def generate_variant_recommendations(config: dict, variant: str):\n        \"\"\"Generate recommendations for each variant.\"\"\"\n        pass\n\n    @task\n    def collect_metrics(experiment_id: str) -&gt; dict:\n        \"\"\"Collect streaming metrics for analysis.\"\"\"\n        # Metrics: streams, skips, saves, time spent\n        pass\n\n    @task\n    def evaluate_significance(metrics: dict) -&gt; dict:\n        \"\"\"Statistical analysis of experiment results.\"\"\"\n        # Calculate p-values, confidence intervals\n        pass\n</code></pre>"},{"location":"case-studies/spotify-recommendations/#lessons-learned","title":"Lessons Learned","text":""},{"location":"case-studies/spotify-recommendations/#what-worked","title":"What Worked","text":"<ol> <li>Dynamic mapping for user segments - Enabled horizontal scaling without code changes</li> <li>Asset-driven dependencies - Features and models stay in sync automatically</li> <li>Gradual rollout with A/B testing - Caught regressions before full deployment</li> <li>KubernetesExecutor for burst capacity - Handled holiday traffic spikes</li> </ol>"},{"location":"case-studies/spotify-recommendations/#challenges-encountered","title":"Challenges Encountered","text":"<ol> <li>Feature freshness vs. cost - Had to balance real-time features against compute costs</li> <li>Model versioning complexity - Multiple concurrent experiments required careful tracking</li> <li>Cross-DAG dependencies - Needed clear Asset contracts between teams</li> </ol>"},{"location":"case-studies/spotify-recommendations/#key-metrics","title":"Key Metrics","text":"Metric Before Airflow After Airflow Pipeline reliability 94% 99.5% Feature freshness 24 hours 4 hours Experiment velocity 2/month 10/month Developer productivity Baseline +40%"},{"location":"case-studies/spotify-recommendations/#code-patterns","title":"Code Patterns","text":""},{"location":"case-studies/spotify-recommendations/#pattern-parallel-user-segment-processing","title":"Pattern: Parallel User Segment Processing","text":"<pre><code># From Module 06: Dynamic Task Mapping\n@task\ndef get_segments() -&gt; list[str]:\n    return [\"segment_a\", \"segment_b\", \"segment_c\"]\n\n\n@task\ndef process_segment(segment: str) -&gt; dict:\n    # Heavy computation for each segment\n    return {\"segment\": segment, \"status\": \"complete\"}\n\n\n# Creates parallel tasks automatically\nsegments = get_segments()\nresults = process_segment.expand(segment=segments)\n</code></pre>"},{"location":"case-studies/spotify-recommendations/#pattern-feature-store-integration","title":"Pattern: Feature Store Integration","text":"<pre><code># From Module 05: Assets\n@task(outlets=[Asset(\"features/user_vectors\")])\ndef update_user_vectors():\n    \"\"\"Update feature store, triggering downstream consumers.\"\"\"\n    vectors = compute_vectors()\n    write_to_feature_store(vectors)\n    return {\"vectors_updated\": len(vectors)}\n</code></pre>"},{"location":"case-studies/spotify-recommendations/#related-exercises","title":"Related Exercises","text":"Exercise Concepts Applied Exercise 6.4: Parallel Embeddings Dynamic mapping for parallel processing Exercise 5.4: Embedding Assets Asset-driven feature pipelines Exercise 15.1: RAG Pipeline ML pipeline orchestration"},{"location":"case-studies/spotify-recommendations/#further-reading","title":"Further Reading","text":"<ul> <li>Spotify Engineering: How Spotify's Algorithm Works</li> <li>Luigi to Airflow Migration</li> <li>Personalization at Spotify Using Cassandra</li> </ul> <p>\u2190 Back to Case Studies | Stripe Fraud Detection \u2192</p> <p>_[DAG]: Directed Acyclic Graph - Airflow's core workflow abstraction _[XCom]: Cross-Communication - Airflow's mechanism for task-to-task data passing _[Executor]: The component that runs tasks (Local, Celery, Kubernetes, etc.) _[Scheduler]: The component that triggers DAGs and determines task execution order _[Webserver]: The component that serves the Airflow UI _[Provider]: An Airflow package that integrates with external systems _[TaskFlow]: The modern Python-native API for defining Airflow tasks using decorators _[Asset]: A logical representation of data that DAGs can produce or consume _[SDK]: Software Development Kit - the airflow.sdk module in Airflow 3.x _[K8s]: Kubernetes - container orchestration platform _[uv]: A fast Python package manager by Astral _[ruff]: A fast Python linter and formatter by Astral _[CI]: Continuous Integration _[CD]: Continuous Deployment *[PR]: Pull Request</p>"},{"location":"case-studies/stripe-fraud-detection/","title":"Case Study: Stripe's Fraud Detection Pipelines","text":""},{"location":"case-studies/stripe-fraud-detection/#company-context","title":"Company Context","text":"<p>Scale: Processes billions of dollars in payments annually Challenge: Detect fraudulent transactions in real-time while minimizing false positives Requirements: Sub-100ms inference latency, continuous model updates, regulatory compliance</p>"},{"location":"case-studies/stripe-fraud-detection/#architecture-overview","title":"Architecture Overview","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                         STRIPE FRAUD DETECTION                               \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                             \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                  \u2502\n\u2502  \u2502   Payment    \u2502\u2500\u2500\u2500\u25b6\u2502   Feature    \u2502\u2500\u2500\u2500\u25b6\u2502   Real-Time  \u2502                  \u2502\n\u2502  \u2502   Events     \u2502    \u2502   Extraction \u2502    \u2502   Scoring    \u2502                  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                  \u2502\n\u2502        \u2502                                        \u2502                           \u2502\n\u2502        \u25bc                                        \u25bc                           \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                  \u2502\n\u2502  \u2502   Batch      \u2502                         \u2502   Decision   \u2502                  \u2502\n\u2502  \u2502   Pipeline   \u2502                         \u2502   Engine     \u2502                  \u2502\n\u2502  \u2502   (Airflow)  \u2502                         \u2502              \u2502                  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                  \u2502\n\u2502        \u2502                                                                    \u2502\n\u2502        \u25bc                                                                    \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                  \u2502\n\u2502  \u2502   Feature    \u2502\u2500\u2500\u2500\u25b6\u2502   Model      \u2502\u2500\u2500\u2500\u25b6\u2502   Model      \u2502                  \u2502\n\u2502  \u2502   Store      \u2502    \u2502   Training   \u2502    \u2502   Validation \u2502                  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                  \u2502\n\u2502                                                \u2502                           \u2502\n\u2502                                                \u25bc                           \u2502\n\u2502                                          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                  \u2502\n\u2502                                          \u2502   Canary     \u2502                  \u2502\n\u2502                                          \u2502   Deployment \u2502                  \u2502\n\u2502                                          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                  \u2502\n\u2502                                                                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"case-studies/stripe-fraud-detection/#key-patterns-used","title":"Key Patterns Used","text":""},{"location":"case-studies/stripe-fraud-detection/#1-production-retry-patterns-for-ml-inference","title":"1. Production Retry Patterns for ML Inference","text":"<p>Fraud scoring requires high reliability with graceful degradation:</p> <pre><code>from datetime import timedelta\n\nfrom airflow.sdk import dag, task\n\n\n@dag(schedule=\"@hourly\")\ndef fraud_model_pipeline():\n    @task(\n        retries=5,\n        retry_delay=timedelta(seconds=1),\n        retry_exponential_backoff=True,\n        max_retry_delay=timedelta(minutes=2),\n    )\n    def score_transactions(batch: list[dict]) -&gt; list[dict]:\n        \"\"\"Score transactions with production-grade retry logic.\"\"\"\n        results = []\n\n        for txn in batch:\n            try:\n                score = call_fraud_model(txn)\n                results.append({\"id\": txn[\"id\"], \"score\": score})\n            except RateLimitError:\n                # Exponential backoff handles this\n                raise\n            except ModelTimeoutError:\n                # Fallback to rule-based scoring\n                score = rule_based_fallback(txn)\n                results.append(\n                    {\n                        \"id\": txn[\"id\"],\n                        \"score\": score,\n                        \"fallback\": True,\n                    }\n                )\n\n        return results\n\n    @task\n    def aggregate_results(scores: list[dict]) -&gt; dict:\n        \"\"\"Aggregate scoring results with SLA tracking.\"\"\"\n        total = len(scores)\n        fallbacks = sum(1 for s in scores if s.get(\"fallback\"))\n\n        return {\n            \"total_scored\": total,\n            \"fallback_rate\": fallbacks / total if total else 0,\n            \"sla_met\": fallbacks / total &lt; 0.05,  # 5% threshold\n        }\n</code></pre>"},{"location":"case-studies/stripe-fraud-detection/#2-deferrable-sensors-for-model-training-completion","title":"2. Deferrable Sensors for Model Training Completion","text":"<p>Training jobs run on dedicated infrastructure. Deferrable sensors wait efficiently:</p> <pre><code>from airflow.sensors.base import BaseSensorOperator\nfrom airflow.triggers.base import BaseTrigger, TriggerEvent\n\n\nclass ModelTrainingTrigger(BaseTrigger):\n    \"\"\"Async trigger for ML training job completion.\"\"\"\n\n    def __init__(self, job_id: str, poll_interval: float = 30.0):\n        super().__init__()\n        self.job_id = job_id\n        self.poll_interval = poll_interval\n\n    def serialize(self):\n        return (\n            f\"{self.__class__.__module__}.{self.__class__.__name__}\",\n            {\"job_id\": self.job_id, \"poll_interval\": self.poll_interval},\n        )\n\n    async def run(self):\n        import asyncio\n\n        while True:\n            status = await self._check_job_status()\n\n            if status[\"state\"] == \"COMPLETED\":\n                yield TriggerEvent(\n                    {\n                        \"status\": \"success\",\n                        \"job_id\": self.job_id,\n                        \"metrics\": status[\"metrics\"],\n                    }\n                )\n                return\n\n            if status[\"state\"] == \"FAILED\":\n                yield TriggerEvent(\n                    {\n                        \"status\": \"error\",\n                        \"message\": status.get(\"error\"),\n                    }\n                )\n                return\n\n            await asyncio.sleep(self.poll_interval)\n\n\nclass ModelTrainingSensor(BaseSensorOperator):\n    \"\"\"Deferrable sensor for ML training jobs.\"\"\"\n\n    def __init__(self, job_id: str, **kwargs):\n        super().__init__(**kwargs)\n        self.job_id = job_id\n\n    def execute(self, context):\n        # Quick check first\n        status = check_training_status(self.job_id)\n        if status[\"state\"] == \"COMPLETED\":\n            return status[\"metrics\"]\n\n        # Defer to trigger (releases worker slot)\n        self.defer(\n            trigger=ModelTrainingTrigger(job_id=self.job_id),\n            method_name=\"execute_complete\",\n        )\n\n    def execute_complete(self, context, event):\n        if event[\"status\"] == \"success\":\n            return event[\"metrics\"]\n        raise AirflowException(f\"Training failed: {event['message']}\")\n</code></pre>"},{"location":"case-studies/stripe-fraud-detection/#3-feature-engineering-with-cost-tracking","title":"3. Feature Engineering with Cost Tracking","text":"<p>ML feature computation is expensive. Cost tracking prevents budget overruns:</p> <pre><code>from dataclasses import dataclass, field\nfrom typing import Callable\n\n\n@dataclass\nclass FeatureComputeTracker:\n    \"\"\"Track feature computation costs and performance.\"\"\"\n\n    budget_limit: float = 1000.0  # Daily compute budget\n    current_spend: float = 0.0\n    compute_history: list = field(default_factory=list)\n\n    def track_computation(\n        self,\n        feature_name: str,\n        compute_fn: Callable,\n        cost_per_row: float,\n        row_count: int,\n    ):\n        \"\"\"Execute computation with cost tracking.\"\"\"\n        estimated_cost = cost_per_row * row_count\n\n        if self.current_spend + estimated_cost &gt; self.budget_limit:\n            raise BudgetExceededError(\n                f\"Feature {feature_name} would exceed budget: \"\n                f\"${self.current_spend:.2f} + ${estimated_cost:.2f} &gt; \"\n                f\"${self.budget_limit:.2f}\"\n            )\n\n        # Execute computation\n        result = compute_fn()\n\n        # Track actual cost\n        self.current_spend += estimated_cost\n        self.compute_history.append(\n            {\n                \"feature\": feature_name,\n                \"cost\": estimated_cost,\n                \"rows\": row_count,\n            }\n        )\n\n        return result\n\n\n@task\ndef compute_velocity_features(transactions: list[dict]) -&gt; dict:\n    \"\"\"Compute transaction velocity features with cost tracking.\"\"\"\n    tracker = FeatureComputeTracker(budget_limit=500.0)\n\n    # Track each feature computation\n    hourly_velocity = tracker.track_computation(\n        feature_name=\"hourly_velocity\",\n        compute_fn=lambda: calculate_hourly_velocity(transactions),\n        cost_per_row=0.001,\n        row_count=len(transactions),\n    )\n\n    merchant_velocity = tracker.track_computation(\n        feature_name=\"merchant_velocity\",\n        compute_fn=lambda: calculate_merchant_velocity(transactions),\n        cost_per_row=0.002,\n        row_count=len(transactions),\n    )\n\n    return {\n        \"features\": {\n            \"hourly_velocity\": hourly_velocity,\n            \"merchant_velocity\": merchant_velocity,\n        },\n        \"compute_cost\": tracker.current_spend,\n    }\n</code></pre>"},{"location":"case-studies/stripe-fraud-detection/#4-circuit-breaker-for-external-services","title":"4. Circuit Breaker for External Services","text":"<p>Fraud detection relies on external data sources. Circuit breakers prevent cascading failures:</p> <pre><code>from dataclasses import dataclass\nfrom datetime import datetime, timedelta\n\n\n@dataclass\nclass CircuitBreaker:\n    \"\"\"Prevent cascading failures to external services.\"\"\"\n\n    failure_threshold: int = 5\n    reset_timeout: timedelta = timedelta(minutes=5)\n    failure_count: int = 0\n    last_failure_time: datetime | None = None\n    state: str = \"closed\"  # closed, open, half-open\n\n    def can_execute(self) -&gt; bool:\n        \"\"\"Check if circuit allows execution.\"\"\"\n        if self.state == \"closed\":\n            return True\n\n        if self.state == \"open\":\n            if datetime.now() - self.last_failure_time &gt; self.reset_timeout:\n                self.state = \"half-open\"\n                return True\n            return False\n\n        return True  # half-open allows one attempt\n\n    def record_success(self):\n        \"\"\"Record successful execution.\"\"\"\n        self.failure_count = 0\n        self.state = \"closed\"\n\n    def record_failure(self):\n        \"\"\"Record failed execution.\"\"\"\n        self.failure_count += 1\n        self.last_failure_time = datetime.now()\n        if self.failure_count &gt;= self.failure_threshold:\n            self.state = \"open\"\n\n\n# Usage in feature enrichment\nidentity_service_breaker = CircuitBreaker()\n\n\n@task\ndef enrich_with_identity_data(transactions: list[dict]) -&gt; list[dict]:\n    \"\"\"Enrich transactions with identity verification data.\"\"\"\n    enriched = []\n\n    for txn in transactions:\n        if identity_service_breaker.can_execute():\n            try:\n                identity_data = call_identity_service(txn[\"user_id\"])\n                txn[\"identity\"] = identity_data\n                identity_service_breaker.record_success()\n            except ServiceError:\n                identity_service_breaker.record_failure()\n                txn[\"identity\"] = {\"status\": \"unavailable\"}\n        else:\n            # Circuit is open, use cached/default data\n            txn[\"identity\"] = get_cached_identity(txn[\"user_id\"])\n\n        enriched.append(txn)\n\n    return enriched\n</code></pre>"},{"location":"case-studies/stripe-fraud-detection/#lessons-learned","title":"Lessons Learned","text":""},{"location":"case-studies/stripe-fraud-detection/#what-worked","title":"What Worked","text":"<ol> <li>Deferrable sensors for training jobs - Training can take hours; deferrable mode freed worker slots</li> <li>Circuit breakers for external services - Prevented identity service outages from blocking all scoring</li> <li>Cost tracking on feature computation - Caught runaway queries before they impacted budget</li> <li>Gradual rollout with shadow scoring - New models ran alongside production before full deployment</li> </ol>"},{"location":"case-studies/stripe-fraud-detection/#challenges-encountered","title":"Challenges Encountered","text":"<ol> <li>Latency requirements - Batch Airflow unsuitable for real-time; used hybrid architecture</li> <li>Feature consistency - Training/serving skew required careful feature store design</li> <li>Regulatory compliance - Model explainability requirements added pipeline complexity</li> </ol>"},{"location":"case-studies/stripe-fraud-detection/#key-metrics","title":"Key Metrics","text":"Metric Before Patterns After Patterns Model training reliability 87% 99.2% Feature pipeline cost overruns 12/month 0/month Service dependency failures 8% impact &lt;1% impact False positive rate 2.3% 1.8%"},{"location":"case-studies/stripe-fraud-detection/#code-patterns","title":"Code Patterns","text":""},{"location":"case-studies/stripe-fraud-detection/#pattern-exponential-backoff-with-jitter","title":"Pattern: Exponential Backoff with Jitter","text":"<pre><code>import random\nfrom datetime import timedelta\n\n\ndef calculate_backoff(attempt: int, base: float = 1.0, max_delay: float = 60.0) -&gt; float:\n    \"\"\"Calculate delay with exponential backoff and jitter.\"\"\"\n    delay = min(base * (2**attempt), max_delay)\n    jitter = random.uniform(0, delay * 0.1)  # 10% jitter\n    return delay + jitter\n\n\n# In task decorator\n@task(\n    retries=5,\n    retry_delay=timedelta(seconds=1),\n    retry_exponential_backoff=True,\n    max_retry_delay=timedelta(minutes=2),\n)\ndef reliable_scoring(data):\n    \"\"\"Score with automatic exponential backoff.\"\"\"\n    pass\n</code></pre>"},{"location":"case-studies/stripe-fraud-detection/#pattern-fallback-chain","title":"Pattern: Fallback Chain","text":"<pre><code>def score_with_fallback(transaction: dict) -&gt; dict:\n    \"\"\"Try multiple scoring methods in order.\"\"\"\n    providers = [\n        {\"name\": \"primary_model\", \"fn\": primary_model_score},\n        {\"name\": \"secondary_model\", \"fn\": secondary_model_score},\n        {\"name\": \"rule_based\", \"fn\": rule_based_score},\n    ]\n\n    for provider in providers:\n        try:\n            score = provider[\"fn\"](transaction)\n            return {\n                \"score\": score,\n                \"provider\": provider[\"name\"],\n                \"fallback_used\": provider != providers[0],\n            }\n        except Exception as e:\n            logger.warning(f\"{provider['name']} failed: {e}\")\n            continue\n\n    raise AllProvidersFailedError(\"All scoring methods failed\")\n</code></pre>"},{"location":"case-studies/stripe-fraud-detection/#related-exercises","title":"Related Exercises","text":"Exercise Concepts Applied Exercise 9.4: LLM Retry Patterns Retry patterns, circuit breakers, cost tracking Exercise 11.4: Vector Store Sensor Deferrable sensors for long operations Exercise 15.2: LLM Chain Multi-step ML workflows"},{"location":"case-studies/stripe-fraud-detection/#further-reading","title":"Further Reading","text":"<ul> <li>Stripe Engineering: Machine Learning Infrastructure</li> <li>Building Fraud Detection Systems at Scale</li> <li>Apache Airflow for ML Pipelines</li> </ul> <p>\u2190 Spotify Recommendations | Airbnb Experimentation \u2192</p> <p>_[DAG]: Directed Acyclic Graph - Airflow's core workflow abstraction _[XCom]: Cross-Communication - Airflow's mechanism for task-to-task data passing _[Executor]: The component that runs tasks (Local, Celery, Kubernetes, etc.) _[Scheduler]: The component that triggers DAGs and determines task execution order _[Webserver]: The component that serves the Airflow UI _[Provider]: An Airflow package that integrates with external systems _[TaskFlow]: The modern Python-native API for defining Airflow tasks using decorators _[Asset]: A logical representation of data that DAGs can produce or consume _[SDK]: Software Development Kit - the airflow.sdk module in Airflow 3.x _[K8s]: Kubernetes - container orchestration platform _[uv]: A fast Python package manager by Astral _[ruff]: A fast Python linter and formatter by Astral _[CI]: Continuous Integration _[CD]: Continuous Deployment *[PR]: Pull Request</p>"},{"location":"getting-started/manual-setup/","title":"Manual Setup","text":"<p>Prefer to set things up step by step? Follow this guide.</p>"},{"location":"getting-started/manual-setup/#clone-the-repository","title":"Clone the Repository","text":"<pre><code>git clone https://github.com/YOUR_ORG/airflow-mastery.git\ncd airflow-mastery\n</code></pre>"},{"location":"getting-started/manual-setup/#install-uv-python-package-manager","title":"Install uv (Python Package Manager)","text":"macOS / LinuxWindows (PowerShell)Homebrew (macOS) <pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre> <pre><code>powershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n</code></pre> <pre><code>brew install uv\n</code></pre> <p>Verify installation:</p> <pre><code>uv --version\n# Expected: uv 0.5.x or higher\n</code></pre>"},{"location":"getting-started/manual-setup/#install-just-command-runner","title":"Install just (Command Runner)","text":"macOS (Homebrew)LinuxWindows <pre><code>brew install just\n</code></pre> <pre><code># Using cargo\ncargo install just\n\n# Or download binary\ncurl --proto '=https' --tlsv1.2 -sSf https://just.systems/install.sh | bash -s -- --to ~/bin\n</code></pre> <pre><code># Using scoop\nscoop install just\n\n# Or using winget\nwinget install Casey.Just\n</code></pre>"},{"location":"getting-started/manual-setup/#install-dependencies","title":"Install Dependencies","text":"<pre><code># Install all dependency groups (dev, docs, test)\njust install\n</code></pre> <p>This runs <code>uv sync --all-groups</code> under the hood.</p>"},{"location":"getting-started/manual-setup/#start-airflow","title":"Start Airflow","text":"<pre><code># Start the Docker Compose stack\njust up\n</code></pre> <p>Wait for the services to be healthy (~1-2 minutes on first run).</p> <p>Check status:</p> <pre><code>just ps\n</code></pre> <p>You should see:</p> <pre><code>NAME                    STATUS\nairflow-postgres        running (healthy)\nairflow-webserver       running (healthy)\nairflow-scheduler       running (healthy)\n</code></pre>"},{"location":"getting-started/manual-setup/#access-airflow","title":"Access Airflow","text":"<p>Open http://localhost:8080 in your browser.</p> <ul> <li>Username: <code>admin</code></li> <li>Password: <code>admin</code></li> </ul>"},{"location":"getting-started/manual-setup/#start-the-documentation-optional","title":"Start the Documentation (Optional)","text":"<p>To serve the docs locally:</p> <pre><code>just docs\n</code></pre> <p>Open http://localhost:8000.</p>"},{"location":"getting-started/manual-setup/#available-commands","title":"Available Commands","text":"<p>Run <code>just</code> to see all available commands:</p> <pre><code>just\n</code></pre> Command Description <code>just install</code> Install all dependencies <code>just up</code> Start Airflow <code>just down</code> Stop Airflow <code>just logs</code> View Airflow logs <code>just docs</code> Serve docs locally <code>just lint</code> Run linter <code>just test</code> Run tests <code>just check</code> Run all checks (lint + test)"},{"location":"getting-started/manual-setup/#next_track_button-next-steps","title":":next_track_button: Next Steps","text":"<ol> <li>Start learning \u2014 Module 00: Environment Setup</li> <li>Explore the codebase \u2014 Check out the <code>dags/</code> and <code>modules/</code> directories</li> <li>Run the sample DAGs \u2014 Unpause them in the Airflow UI</li> </ol>"},{"location":"getting-started/manual-setup/#having-issues","title":"Having Issues?","text":"<p>See the Troubleshooting Guide for common problems and solutions.</p> <p>_[DAG]: Directed Acyclic Graph - Airflow's core workflow abstraction _[XCom]: Cross-Communication - Airflow's mechanism for task-to-task data passing _[Executor]: The component that runs tasks (Local, Celery, Kubernetes, etc.) _[Scheduler]: The component that triggers DAGs and determines task execution order _[Webserver]: The component that serves the Airflow UI _[Provider]: An Airflow package that integrates with external systems _[TaskFlow]: The modern Python-native API for defining Airflow tasks using decorators _[Asset]: A logical representation of data that DAGs can produce or consume _[SDK]: Software Development Kit - the airflow.sdk module in Airflow 3.x _[K8s]: Kubernetes - container orchestration platform _[uv]: A fast Python package manager by Astral _[ruff]: A fast Python linter and formatter by Astral _[CI]: Continuous Integration _[CD]: Continuous Deployment *[PR]: Pull Request</p>"},{"location":"getting-started/quickstart/","title":"Quickstart","text":"<p>Get Airflow Mastery running in under 5 minutes with our one-command setup.</p>"},{"location":"getting-started/quickstart/#one-command-setup","title":"One-Command Setup","text":"macOS / LinuxWindows (PowerShell) <pre><code>curl -fsSL https://raw.githubusercontent.com/YOUR_ORG/airflow-mastery/main/scripts/quickstart.sh | bash\n</code></pre> <pre><code>irm https://raw.githubusercontent.com/YOUR_ORG/airflow-mastery/main/scripts/quickstart.ps1 | iex\n</code></pre>"},{"location":"getting-started/quickstart/#what-the-script-does","title":"What the Script Does","text":"<p>The quickstart script automates everything:</p> Step Action Time 1 Check prerequisites (Git, Docker) ~1s 2 Clone repository to <code>~/airflow-mastery</code> ~5s 3 Install uv (if needed) ~3s 4 Install Python dependencies ~5s 5 Start Airflow with Docker Compose ~60-120s 6 Wait for Airflow to be healthy ~30s 7 Open browser tabs ~1s <p>Total time: ~2-3 minutes (mostly Docker image download on first run)</p>"},{"location":"getting-started/quickstart/#what-opens-in-your-browser","title":"What Opens in Your Browser","text":"<p>After the script completes, you'll have three tabs:</p> <ol> <li> <p>Airflow UI \u2014 http://localhost:8080</p> <ul> <li>Username: <code>admin</code></li> <li>Password: <code>admin</code></li> </ul> </li> <li> <p>Documentation \u2014 This site</p> </li> <li> <p>First Exercise \u2014 Module 00, Exercise 0.1</p> </li> </ol>"},{"location":"getting-started/quickstart/#verify-its-working","title":"Verify It's Working","text":"<p>Check these endpoints:</p> <pre><code># Airflow health check\ncurl http://localhost:8080/health\n\n# Should return: {\"metadatabase\":{\"status\":\"healthy\"},\"scheduler\":{\"status\":\"healthy\"}}\n</code></pre> <p>In the Airflow UI, you should see:</p> <ul> <li> Green \"scheduler\" status in the top bar</li> <li> Sample DAGs listed (may be paused)</li> <li> No import errors</li> </ul>"},{"location":"getting-started/quickstart/#prerequisites","title":"Prerequisites","text":"<p>The script checks for these automatically, but ensure you have:</p> Requirement Minimum Version Check Command Git Any <code>git --version</code> Docker 20.10+ <code>docker --version</code> Docker Compose 2.0+ <code>docker compose version</code> 5GB disk space \u2014 <code>df -h</code> <p>Docker Desktop</p> <p>On macOS and Windows, Docker Desktop is the easiest way to get Docker and Docker Compose.</p>"},{"location":"getting-started/quickstart/#next_track_button-next-steps","title":":next_track_button: Next Steps","text":"<p>Once Airflow is running:</p> <ol> <li>Explore the UI \u2014 Familiarize yourself with DAGs, runs, and logs</li> <li>Start Module 00 \u2014 Exercise 0.1: Project Initialization</li> <li>Join the community \u2014 GitHub Discussions</li> </ol>"},{"location":"getting-started/quickstart/#need-help","title":"Need Help?","text":"<ul> <li> Troubleshooting Guide</li> <li> Report an Issue</li> <li> Ask a Question</li> </ul> <p>_[DAG]: Directed Acyclic Graph - Airflow's core workflow abstraction _[XCom]: Cross-Communication - Airflow's mechanism for task-to-task data passing _[Executor]: The component that runs tasks (Local, Celery, Kubernetes, etc.) _[Scheduler]: The component that triggers DAGs and determines task execution order _[Webserver]: The component that serves the Airflow UI _[Provider]: An Airflow package that integrates with external systems _[TaskFlow]: The modern Python-native API for defining Airflow tasks using decorators _[Asset]: A logical representation of data that DAGs can produce or consume _[SDK]: Software Development Kit - the airflow.sdk module in Airflow 3.x _[K8s]: Kubernetes - container orchestration platform _[uv]: A fast Python package manager by Astral _[ruff]: A fast Python linter and formatter by Astral _[CI]: Continuous Integration _[CD]: Continuous Deployment *[PR]: Pull Request</p>"},{"location":"getting-started/troubleshooting/","title":"Troubleshooting","text":"<p>Common issues and their solutions.</p>"},{"location":"getting-started/troubleshooting/#docker-issues","title":"Docker Issues","text":""},{"location":"getting-started/troubleshooting/#docker-daemon-not-running","title":"Docker daemon not running","text":"<p>Error:</p> <pre><code>Cannot connect to the Docker daemon at unix:///var/run/docker.sock\n</code></pre> <p>Solution:</p> macOSLinuxWindows <p>Start Docker Desktop from Applications, or: <pre><code>open -a Docker\n</code></pre></p> <pre><code>sudo systemctl start docker\n</code></pre> <p>Start Docker Desktop from the Start menu.</p>"},{"location":"getting-started/troubleshooting/#port-8080-already-in-use","title":"Port 8080 already in use","text":"<p>Error:</p> <pre><code>Bind for 0.0.0.0:8080 failed: port is already allocated\n</code></pre> <p>Solution:</p> <ol> <li> <p>Find what's using the port:</p> <pre><code>lsof -i :8080\n# or on Linux\nss -tlnp | grep 8080\n</code></pre> </li> <li> <p>Either stop that service, or modify <code>docker-compose.yml</code> to use a different port:     <pre><code>ports:\n    - \"8081:8080\" # Use 8081 instead\n</code></pre></p> </li> </ol>"},{"location":"getting-started/troubleshooting/#containers-keep-restarting","title":"Containers keep restarting","text":"<p>Error:</p> <pre><code>airflow-scheduler exited with code 1\n</code></pre> <p>Solution:</p> <ol> <li> <p>Check the logs:</p> <pre><code>just logs\n# or\ndocker compose -f infrastructure/docker-compose/docker-compose.yml logs scheduler\n</code></pre> </li> <li> <p>Common causes:</p> <ul> <li>Database not ready \u2014 wait and retry</li> <li>Out of memory \u2014 increase Docker memory allocation</li> <li>Invalid DAG \u2014 check for Python syntax errors in <code>dags/</code></li> </ul> </li> </ol>"},{"location":"getting-started/troubleshooting/#out-of-disk-space","title":"Out of disk space","text":"<p>Error:</p> <pre><code>No space left on device\n</code></pre> <p>Solution:</p> <pre><code># Remove unused Docker resources\ndocker system prune -a\n\n# Remove all volumes (WARNING: deletes data)\ndocker volume prune\n</code></pre>"},{"location":"getting-started/troubleshooting/#python-uv-issues","title":"Python / uv Issues","text":""},{"location":"getting-started/troubleshooting/#uv-command-not-found","title":"uv command not found","text":"<p>Error:</p> <pre><code>command not found: uv\n</code></pre> <p>Solution:</p> <ol> <li> <p>Reinstall uv:</p> <pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre> </li> <li> <p>Reload your shell:</p> <pre><code>source ~/.bashrc  # or ~/.zshrc\n</code></pre> </li> <li> <p>Or add to PATH manually:     <pre><code>export PATH=\"$HOME/.local/bin:$PATH\"\n</code></pre></p> </li> </ol>"},{"location":"getting-started/troubleshooting/#python-version-mismatch","title":"Python version mismatch","text":"<p>Error:</p> <pre><code>No interpreter found for Python &gt;=3.11,&lt;3.13\n</code></pre> <p>Solution:</p> <p>Let uv install Python:</p> <pre><code>uv python install 3.11\n</code></pre>"},{"location":"getting-started/troubleshooting/#dependency-resolution-failed","title":"Dependency resolution failed","text":"<p>Error:</p> <pre><code>error: No solution found when resolving dependencies\n</code></pre> <p>Solution:</p> <ol> <li> <p>Clear the cache:</p> <pre><code>uv cache clean\n</code></pre> </li> <li> <p>Remove lockfile and regenerate:     <pre><code>rm uv.lock\nuv lock\nuv sync\n</code></pre></p> </li> </ol>"},{"location":"getting-started/troubleshooting/#airflow-issues","title":"Airflow Issues","text":""},{"location":"getting-started/troubleshooting/#dag-import-errors","title":"DAG import errors","text":"<p>Symptom: Red error banner in Airflow UI</p> <p>Solution:</p> <ol> <li>Check the import errors page in the UI</li> <li> <p>Or via CLI:</p> <pre><code>docker compose -f infrastructure/docker-compose/docker-compose.yml exec webserver airflow dags list-import-errors\n</code></pre> </li> <li> <p>Common causes:</p> <ul> <li>Missing dependencies \u2014 add to <code>pyproject.toml</code></li> <li>Syntax errors \u2014 run <code>uv run ruff check dags/</code></li> <li>Wrong imports \u2014 check Airflow 3.x migration notes</li> </ul> </li> </ol>"},{"location":"getting-started/troubleshooting/#tasks-stuck-in-queued-state","title":"Tasks stuck in queued state","text":"<p>Symptom: Tasks never start running</p> <p>Solution:</p> <ol> <li> <p>Check scheduler is running:</p> <pre><code>just ps\n</code></pre> </li> <li> <p>Restart scheduler:</p> <pre><code>docker compose -f infrastructure/docker-compose/docker-compose.yml restart scheduler\n</code></pre> </li> <li> <p>Check for resource limits in Docker settings</p> </li> </ol>"},{"location":"getting-started/troubleshooting/#broken-dag-after-code-changes","title":"\"Broken DAG\" after code changes","text":"<p>Symptom: DAG worked before, now shows errors</p> <p>Solution:</p> <ol> <li> <p>Check for syntax errors:</p> <pre><code>uv run ruff check dags/\n</code></pre> </li> <li> <p>Validate the DAG:     <pre><code>docker compose -f infrastructure/docker-compose/docker-compose.yml exec webserver python /opt/airflow/dags/your_dag.py\n</code></pre></p> </li> </ol>"},{"location":"getting-started/troubleshooting/#network-issues","title":"Network Issues","text":""},{"location":"getting-started/troubleshooting/#cannot-access-localhost8080","title":"Cannot access localhost:8080","text":"<p>Symptom: Browser shows \"connection refused\"</p> <p>Solution:</p> <ol> <li> <p>Verify containers are running:</p> <pre><code>just ps\n</code></pre> </li> <li> <p>Check webserver logs:</p> <pre><code>docker compose -f infrastructure/docker-compose/docker-compose.yml logs webserver\n</code></pre> </li> <li> <p>Ensure port mapping is correct:     <pre><code>docker compose -f infrastructure/docker-compose/docker-compose.yml port webserver 8080\n</code></pre></p> </li> </ol>"},{"location":"getting-started/troubleshooting/#reset-everything","title":"Reset Everything","text":"<p>If all else fails, start fresh:</p> <pre><code># Stop and remove everything\njust down-clean\n\n# Remove Python environment\nrm -rf .venv\n\n# Reinstall\njust install\n\n# Start fresh\njust up\n</code></pre>"},{"location":"getting-started/troubleshooting/#still-stuck","title":"Still Stuck?","text":"<ul> <li> Search existing issues</li> <li> Open a new issue</li> <li> Ask in Discussions</li> </ul> <p>When reporting issues, include:</p> <ol> <li>Operating system and version</li> <li>Docker version (<code>docker --version</code>)</li> <li>uv version (<code>uv --version</code>)</li> <li>Full error message</li> <li>Steps to reproduce</li> </ol> <p>_[DAG]: Directed Acyclic Graph - Airflow's core workflow abstraction _[XCom]: Cross-Communication - Airflow's mechanism for task-to-task data passing _[Executor]: The component that runs tasks (Local, Celery, Kubernetes, etc.) _[Scheduler]: The component that triggers DAGs and determines task execution order _[Webserver]: The component that serves the Airflow UI _[Provider]: An Airflow package that integrates with external systems _[TaskFlow]: The modern Python-native API for defining Airflow tasks using decorators _[Asset]: A logical representation of data that DAGs can produce or consume _[SDK]: Software Development Kit - the airflow.sdk module in Airflow 3.x _[K8s]: Kubernetes - container orchestration platform _[uv]: A fast Python package manager by Astral _[ruff]: A fast Python linter and formatter by Astral _[CI]: Continuous Integration _[CD]: Continuous Deployment *[PR]: Pull Request</p>"},{"location":"includes/abbreviations/","title":"Abbreviations","text":"<p>_[DAG]: Directed Acyclic Graph - Airflow's core workflow abstraction _[XCom]: Cross-Communication - Airflow's mechanism for task-to-task data passing _[Executor]: The component that runs tasks (Local, Celery, Kubernetes, etc.) _[Scheduler]: The component that triggers DAGs and determines task execution order _[Webserver]: The component that serves the Airflow UI _[Provider]: An Airflow package that integrates with external systems _[TaskFlow]: The modern Python-native API for defining Airflow tasks using decorators _[Asset]: A logical representation of data that DAGs can produce or consume _[SDK]: Software Development Kit - the airflow.sdk module in Airflow 3.x _[K8s]: Kubernetes - container orchestration platform _[uv]: A fast Python package manager by Astral _[ruff]: A fast Python linter and formatter by Astral _[CI]: Continuous Integration _[CD]: Continuous Deployment *[PR]: Pull Request</p> <p>_[DAG]: Directed Acyclic Graph - Airflow's core workflow abstraction _[XCom]: Cross-Communication - Airflow's mechanism for task-to-task data passing _[Executor]: The component that runs tasks (Local, Celery, Kubernetes, etc.) _[Scheduler]: The component that triggers DAGs and determines task execution order _[Webserver]: The component that serves the Airflow UI _[Provider]: An Airflow package that integrates with external systems _[TaskFlow]: The modern Python-native API for defining Airflow tasks using decorators _[Asset]: A logical representation of data that DAGs can produce or consume _[SDK]: Software Development Kit - the airflow.sdk module in Airflow 3.x _[K8s]: Kubernetes - container orchestration platform _[uv]: A fast Python package manager by Astral _[ruff]: A fast Python linter and formatter by Astral _[CI]: Continuous Integration _[CD]: Continuous Deployment *[PR]: Pull Request</p>"},{"location":"modules/","title":"Airflow Mastery Curriculum","text":"<p>Complete Apache Airflow 3.x learning path \u2014 from environment setup to production deployment.</p>"},{"location":"modules/#learning-modules","title":"Learning Modules","text":""},{"location":"modules/#fundamentals","title":"Fundamentals","text":"Module Topic Description 00 Environment Setup Modern Python tooling with uv, ruff, Docker 01 Airflow Foundations Core concepts, TaskFlow API, first DAGs 02 TaskFlow API Deep Dive @dag, @task decorators, XCom 03 Operators &amp; Hooks Built-in operators, custom hooks 04 Scheduling &amp; Triggers Cron, timetables, external triggers"},{"location":"modules/#intermediate","title":"Intermediate","text":"Module Topic Description 05 Assets &amp; Data-Aware Modern Asset-based scheduling 06 Dynamic Tasks Task mapping, runtime task generation 07 Testing &amp; Debugging pytest, DAG validation, debugging"},{"location":"modules/#advanced","title":"Advanced","text":"Module Topic Description 08 Kubernetes Executor K8s pods, resource management, Helm 09 Production Patterns Error handling, retry strategies, monitoring 10 Advanced Topics Custom executors, plugins, optimization"},{"location":"modules/#specialized","title":"Specialized","text":"Module Topic Description 11 Sensors &amp; Deferrable Efficient waiting, async execution 12 REST API Programmatic Airflow control 13 Connections &amp; Secrets Secure credential management 14 Resource Management Pools, priorities, SLAs 15 AI/ML Orchestration LLM pipelines, embeddings, RAG"},{"location":"modules/#recommended-path","title":"Recommended Path","text":"<ol> <li>Start with Module 00 \u2014 Get your environment properly configured</li> <li>Complete Modules 01-04 \u2014 Build solid fundamentals</li> <li>Progress through 05-07 \u2014 Intermediate patterns</li> <li>Advanced topics 08-10 \u2014 Production readiness</li> <li>Specialized modules \u2014 As needed for your use case</li> </ol>"},{"location":"modules/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.11+</li> <li>Docker Desktop</li> <li>Basic command line knowledge</li> <li>Familiarity with Python syntax</li> </ul> <p> Back to Home |  Start with Module 00</p> <p>_[DAG]: Directed Acyclic Graph - Airflow's core workflow abstraction _[XCom]: Cross-Communication - Airflow's mechanism for task-to-task data passing _[Executor]: The component that runs tasks (Local, Celery, Kubernetes, etc.) _[Scheduler]: The component that triggers DAGs and determines task execution order _[Webserver]: The component that serves the Airflow UI _[Provider]: An Airflow package that integrates with external systems _[TaskFlow]: The modern Python-native API for defining Airflow tasks using decorators _[Asset]: A logical representation of data that DAGs can produce or consume _[SDK]: Software Development Kit - the airflow.sdk module in Airflow 3.x _[K8s]: Kubernetes - container orchestration platform _[uv]: A fast Python package manager by Astral _[ruff]: A fast Python linter and formatter by Astral _[CI]: Continuous Integration _[CD]: Continuous Deployment *[PR]: Pull Request</p>"},{"location":"modules/00-environment-setup/","title":"Module 00: Development Environment &amp; Modern Python Tooling","text":""},{"location":"modules/00-environment-setup/#prerequisites","title":"\u26a0\ufe0f Prerequisites","text":"<p>Before starting this module, ensure you have:</p> <ul> <li>Operating System: macOS, Linux, or Windows with WSL2</li> <li>Terminal Access: Comfortable with basic command-line operations</li> <li>Docker: Docker Desktop or Docker Engine installed and running (<code>docker --version</code>)</li> <li>Git: Version control installed (<code>git --version</code>)</li> <li>Text Editor: VS Code (recommended) or any code editor</li> <li>No Python Required: uv will handle Python installation for you!</li> </ul>"},{"location":"modules/00-environment-setup/#system-requirements","title":"System Requirements","text":"Component Minimum Recommended RAM 4 GB 8+ GB Disk Space 5 GB free 10+ GB free Docker 20.10+ Latest stable"},{"location":"modules/00-environment-setup/#quick-environment-check","title":"Quick Environment Check","text":"<pre><code># Run these commands to verify your environment\ndocker --version      # Should show Docker 20.10+\ngit --version         # Should show git 2.x+\n</code></pre>"},{"location":"modules/00-environment-setup/#learning-objectives","title":"\ud83c\udfaf Learning Objectives","text":"<p>By the end of this module, you will:</p> <ul> <li>Install and configure uv for lightning-fast Python package management</li> <li>Create a professional pyproject.toml with proper Airflow dependency groups</li> <li>Set up ruff for linting and formatting (replaces flake8, black, isort)</li> <li>Configure pre-commit hooks for automated code quality</li> <li>Build optimized Docker images using uv for Airflow deployments</li> <li>Establish a testing workflow with pytest and DAG validation</li> </ul>"},{"location":"modules/00-environment-setup/#estimated-time-2-3-hours","title":"\u23f1\ufe0f Estimated Time: 2-3 hours","text":""},{"location":"modules/00-environment-setup/#1-why-modern-python-tooling-matters","title":"1. Why Modern Python Tooling Matters","text":"<p>If you've worked with Python projects before, you've likely experienced:</p> <ul> <li>Slow installs: <code>pip install</code> taking minutes for large dependency trees</li> <li>Tool fragmentation: Juggling black, flake8, isort, pyupgrade separately</li> <li>Lock file confusion: requirements.txt vs Pipfile.lock vs poetry.lock</li> <li>Environment mismatches: \"Works on my machine\" across team members</li> </ul>"},{"location":"modules/00-environment-setup/#the-modern-stack","title":"The Modern Stack","text":"Traditional Modern Improvement pip + venv uv 10-100x faster, unified tool black + flake8 + isort ruff Single tool, instant feedback requirements.txt pyproject.toml Standard format, rich metadata setup.py pyproject.toml Declarative, no code execution"},{"location":"modules/00-environment-setup/#why-uv","title":"Why uv?","text":"<p>uv (by Astral, the ruff creators) is a Python package manager written in Rust:</p> <ul> <li>Speed: Installs packages 10-100x faster than pip</li> <li>Unified: Replaces pip, pip-tools, virtualenv, pyenv in one tool</li> <li>Reproducible: Cross-platform lock files (<code>uv.lock</code>)</li> <li>Compatible: Works with existing pyproject.toml files</li> </ul> <pre><code># Speed comparison (example: installing airflow + dependencies)\n# pip:  45-90 seconds\n# uv:   3-8 seconds\n</code></pre>"},{"location":"modules/00-environment-setup/#2-installing-and-understanding-uv","title":"2. Installing and Understanding uv","text":""},{"location":"modules/00-environment-setup/#installation","title":"Installation","text":"<p>Choose one method:</p> <pre><code># Recommended: Standalone installer (no Python required)\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Alternative: Via pip (if you have Python already)\npip install uv\n\n# Alternative: Homebrew (macOS)\nbrew install uv\n</code></pre> <p>After installation, verify:</p> <pre><code>uv --version\n# Output: uv 0.5.x (or higher)\n</code></pre>"},{"location":"modules/00-environment-setup/#uv-vs-pip-mental-model","title":"uv vs pip: Mental Model","text":"pip Command uv Equivalent Notes <code>python -m venv .venv</code> <code>uv venv</code> Creates virtual environment <code>pip install package</code> <code>uv pip install package</code> Installs package <code>pip install -r requirements.txt</code> <code>uv pip install -r requirements.txt</code> From requirements <code>pip freeze &gt; requirements.txt</code> <code>uv pip compile</code> Generate lock file N/A <code>uv sync</code> Sync from pyproject.toml + lock N/A <code>uv add package</code> Add dependency to pyproject.toml"},{"location":"modules/00-environment-setup/#project-workflow-with-uv","title":"Project Workflow with uv","text":"<pre><code># Initialize a new project\nuv init my-airflow-project\ncd my-airflow-project\n\n# Add dependencies\nuv add apache-airflow\nuv add --dev pytest ruff pre-commit\n\n# Create virtual environment and install\nuv venv\nuv sync\n\n# Run commands in the environment\nuv run python -c \"import airflow; print(airflow.__version__)\"\nuv run pytest\n</code></pre>"},{"location":"modules/00-environment-setup/#understanding-lock-files","title":"Understanding Lock Files","text":"<p>uv generates <code>uv.lock</code>\u2014a cross-platform lock file that:</p> <ul> <li>Records exact versions of all dependencies (including transitive)</li> <li>Ensures reproducible builds across machines</li> <li>Is human-readable TOML but managed by uv</li> </ul> <p>Key Rule: Commit <code>uv.lock</code> to version control. Never edit it manually.</p>"},{"location":"modules/00-environment-setup/#3-project-configuration-with-pyprojecttoml","title":"3. Project Configuration with pyproject.toml","text":"<p><code>pyproject.toml</code> is the modern standard for Python project configuration (PEP 518, 621). It replaces <code>setup.py</code>, <code>setup.cfg</code>, and <code>requirements.txt</code>.</p>"},{"location":"modules/00-environment-setup/#anatomy-for-airflow-projects","title":"Anatomy for Airflow Projects","text":"<pre><code>[project]\nname = \"my-airflow-project\"\nversion = \"0.1.0\"\ndescription = \"My Airflow DAGs and workflows\"\nreadme = \"README.md\"\nrequires-python = \"&gt;=3.9,&lt;3.13\"\nlicense = { text = \"MIT\" }\nauthors = [\n    { name = \"Your Name\", email = \"you@example.com\" }\n]\n\n# Core dependencies (installed by default)\ndependencies = [\n    \"apache-airflow&gt;=3.0.0,&lt;4.0.0\",\n]\n\n# Optional dependency groups\n[project.optional-dependencies]\n# Development tools\ndev = [\n    \"ruff&gt;=0.8.0\",\n    \"pre-commit&gt;=4.0.0\",\n    \"mypy&gt;=1.0.0\",\n]\n\n# Testing dependencies\ntest = [\n    \"pytest&gt;=8.0.0\",\n    \"pytest-cov&gt;=4.0.0\",\n    \"pytest-airflow&gt;=0.1.0\",\n]\n\n# Common Airflow providers\nproviders = [\n    \"apache-airflow-providers-postgres&gt;=5.0.0\",\n    \"apache-airflow-providers-http&gt;=4.0.0\",\n    \"apache-airflow-providers-celery&gt;=3.0.0\",\n    \"apache-airflow-providers-standard&gt;=0.1.0\",\n]\n\n# All optional dependencies\nall = [\n    \"my-airflow-project[dev,test,providers]\",\n]\n\n[build-system]\nrequires = [\"hatchling\"]\nbuild-backend = \"hatchling.build\"\n</code></pre>"},{"location":"modules/00-environment-setup/#dependency-groups-explained","title":"Dependency Groups Explained","text":"Group Purpose When to Install <code>dependencies</code> Core runtime Always (production) <code>dev</code> Development tools Local development <code>test</code> Testing tools CI/CD + local <code>providers</code> Airflow providers As needed"},{"location":"modules/00-environment-setup/#installing-groups-with-uv","title":"Installing Groups with uv","text":"<pre><code># Core dependencies only\nuv sync\n\n# With development tools\nuv sync --group dev\n\n# With testing\nuv sync --group test\n\n# Everything\nuv sync --all-groups\n</code></pre>"},{"location":"modules/00-environment-setup/#4-code-quality-with-ruff","title":"4. Code Quality with Ruff","text":"<p>Ruff is a Python linter and formatter written in Rust that replaces:</p> <ul> <li>flake8 (linting)</li> <li>black (formatting)</li> <li>isort (import sorting)</li> <li>pyupgrade (Python version upgrades)</li> <li>And 20+ other tools</li> </ul>"},{"location":"modules/00-environment-setup/#speed-comparison","title":"Speed Comparison","text":"<pre><code># Linting a large codebase (example: 1000 files)\n# flake8: 45 seconds\n# ruff:   0.3 seconds (150x faster)\n</code></pre>"},{"location":"modules/00-environment-setup/#configuration-in-pyprojecttoml","title":"Configuration in pyproject.toml","text":"<pre><code>[tool.ruff]\n# Same line length as Black default\nline-length = 88\n\n# Target Python version\ntarget-version = \"py39\"\n\n# Directories to exclude\nexclude = [\n    \".git\",\n    \".venv\",\n    \"__pycache__\",\n    \"build\",\n    \"dist\",\n]\n\n[tool.ruff.lint]\n# Enable these rule sets\nselect = [\n    \"E\",      # pycodestyle errors\n    \"W\",      # pycodestyle warnings\n    \"F\",      # Pyflakes\n    \"I\",      # isort (import sorting)\n    \"B\",      # flake8-bugbear\n    \"C4\",     # flake8-comprehensions\n    \"UP\",     # pyupgrade\n    \"SIM\",    # flake8-simplify\n    \"AIR\",    # Airflow-specific rules\n]\n\n# Ignore specific rules\nignore = [\n    \"E501\",   # Line too long (handled by formatter)\n]\n\n# Allow autofix for these\nfixable = [\"ALL\"]\n\n[tool.ruff.lint.isort]\nknown-first-party = [\"dags\", \"plugins\", \"tests\"]\n\n[tool.ruff.format]\n# Use double quotes (like Black)\nquote-style = \"double\"\n\n# Indent with spaces\nindent-style = \"space\"\n</code></pre>"},{"location":"modules/00-environment-setup/#airflow-specific-rules-air","title":"Airflow-Specific Rules (AIR)","text":"<p>Ruff includes Airflow-specific rules:</p> Rule Description AIR001 Task variable name doesn't match <code>task_id</code> AIR301 DAG uses deprecated <code>schedule_interval</code> AIR302 Uses deprecated Airflow 2.x import paths"},{"location":"modules/00-environment-setup/#running-ruff","title":"Running Ruff","text":"<pre><code># Check for issues\nuv run ruff check .\n\n# Check and fix auto-fixable issues\nuv run ruff check --fix .\n\n# Format code\nuv run ruff format .\n\n# Check if code is formatted\nuv run ruff format --check .\n</code></pre>"},{"location":"modules/00-environment-setup/#ide-integration","title":"IDE Integration","text":"<p>VS Code: Install the \"Ruff\" extension by Astral:</p> <pre><code>// .vscode/settings.json\n{\n  \"editor.formatOnSave\": true,\n  \"editor.codeActionsOnSave\": {\n    \"source.fixAll.ruff\": \"explicit\",\n    \"source.organizeImports.ruff\": \"explicit\"\n  },\n  \"[python]\": {\n    \"editor.defaultFormatter\": \"charliermarsh.ruff\"\n  }\n}\n</code></pre>"},{"location":"modules/00-environment-setup/#5-pre-commit-hooks","title":"5. Pre-commit Hooks","text":"<p>Pre-commit hooks run automated checks before every commit, catching issues early.</p>"},{"location":"modules/00-environment-setup/#installation_1","title":"Installation","text":"<pre><code># Add to dev dependencies\nuv add --dev pre-commit\n\n# Create configuration file\ntouch .pre-commit-config.yaml\n</code></pre>"},{"location":"modules/00-environment-setup/#configuration","title":"Configuration","text":"<pre><code># .pre-commit-config.yaml\nrepos:\n  # Ruff for linting and formatting\n  - repo: https://github.com/astral-sh/ruff-pre-commit\n    rev: v0.8.0\n    hooks:\n      - id: ruff\n        args: [--fix]\n      - id: ruff-format\n\n  # General file checks\n  - repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v5.0.0\n    hooks:\n      - id: trailing-whitespace\n      - id: end-of-file-fixer\n      - id: check-yaml\n        args: [--unsafe] # Allows custom YAML tags\n      - id: check-added-large-files\n        args: [--maxkb=500]\n      - id: check-merge-conflict\n\n  # Security checks\n  - repo: https://github.com/Yelp/detect-secrets\n    rev: v1.5.0\n    hooks:\n      - id: detect-secrets\n        args: [--baseline, .secrets.baseline]\n</code></pre>"},{"location":"modules/00-environment-setup/#activating-hooks","title":"Activating Hooks","text":"<pre><code># Install hooks into your git repository\nuv run pre-commit install\n\n# Run manually on all files (first time)\nuv run pre-commit run --all-files\n</code></pre> <p>After installation, hooks run automatically on <code>git commit</code>. If a hook fails, the commit is blocked until you fix the issue.</p>"},{"location":"modules/00-environment-setup/#ci-integration","title":"CI Integration","text":"<p>Add to your CI pipeline to ensure consistency:</p> <pre><code># .github/workflows/lint.yml\nname: Lint\non: [push, pull_request]\njobs:\n  lint:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - uses: astral-sh/setup-uv@v4\n      - run: uv sync --group dev\n      - run: uv run pre-commit run --all-files\n</code></pre>"},{"location":"modules/00-environment-setup/#6-docker-builds-with-uv","title":"6. Docker Builds with uv","text":"<p>Building Docker images for Airflow with uv is faster and produces smaller images.</p>"},{"location":"modules/00-environment-setup/#multi-stage-dockerfile-pattern","title":"Multi-Stage Dockerfile Pattern","text":"<pre><code># syntax=docker/dockerfile:1\n\n# ============================================\n# Stage 1: Build environment with uv\n# ============================================\nFROM ghcr.io/astral-sh/uv:python3.11-bookworm-slim AS builder\n\n# Set environment variables\nENV UV_COMPILE_BYTECODE=1 \\\n    UV_LINK_MODE=copy \\\n    UV_PYTHON_DOWNLOADS=0\n\nWORKDIR /app\n\n# Copy dependency files first (for layer caching)\nCOPY pyproject.toml uv.lock ./\n\n# Install dependencies (without the project itself)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv sync --locked --no-install-project --no-dev\n\n# Copy the rest of the application\nCOPY dags/ ./dags/\nCOPY plugins/ ./plugins/\n\n# Install the project\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv sync --locked --no-dev\n\n# ============================================\n# Stage 2: Runtime image (no uv)\n# ============================================\nFROM python:3.11-slim-bookworm\n\n# Create non-root user for security\nRUN groupadd --system --gid 1000 airflow \\\n    &amp;&amp; useradd --system --gid 1000 --uid 1000 airflow\n\n# Copy virtual environment from builder\nCOPY --from=builder --chown=airflow:airflow /app/.venv /app/.venv\n\n# Copy application code\nCOPY --from=builder --chown=airflow:airflow /app/dags /app/dags\nCOPY --from=builder --chown=airflow:airflow /app/plugins /app/plugins\n\n# Set path to use virtual environment\nENV PATH=\"/app/.venv/bin:$PATH\" \\\n    AIRFLOW_HOME=/app\n\nUSER airflow\nWORKDIR /app\n</code></pre>"},{"location":"modules/00-environment-setup/#why-this-pattern","title":"Why This Pattern?","text":"<ol> <li> <p>Layer Caching: Dependencies change less often than code. By copying <code>pyproject.toml</code> and <code>uv.lock</code> first, Docker can cache the dependency installation layer.</p> </li> <li> <p>Smaller Images: The final image doesn't include uv, build tools, or cache\u2014only the runtime environment.</p> </li> <li> <p>Security: Runs as non-root user with minimal installed packages.</p> </li> </ol>"},{"location":"modules/00-environment-setup/#build-commands","title":"Build Commands","text":"<pre><code># Build the image\ndocker build -t my-airflow:latest .\n\n# Build with cache (for faster rebuilds)\ndocker build --build-arg BUILDKIT_INLINE_CACHE=1 -t my-airflow:latest .\n</code></pre>"},{"location":"modules/00-environment-setup/#comparison-pip-vs-uv-docker-builds","title":"Comparison: pip vs uv Docker Builds","text":"Metric pip uv Build time 45-90s 5-15s Layer cache hit Often invalidated Reliable Image size Similar Similar"},{"location":"modules/00-environment-setup/#7-testing-setup","title":"7. Testing Setup","text":"<p>A proper testing setup catches DAG issues before deployment.</p>"},{"location":"modules/00-environment-setup/#pytest-configuration-in-pyprojecttoml","title":"pytest Configuration in pyproject.toml","text":"<pre><code>[tool.pytest.ini_options]\ntestpaths = [\"tests\"]\npython_files = [\"test_*.py\"]\npython_functions = [\"test_*\"]\naddopts = [\n    \"-v\",\n    \"--tb=short\",\n    \"-ra\",\n]\nfilterwarnings = [\n    \"ignore::DeprecationWarning\",\n]\n\n[tool.coverage.run]\nsource = [\"dags\", \"plugins\"]\nomit = [\"tests/*\"]\n\n[tool.coverage.report]\nexclude_lines = [\n    \"pragma: no cover\",\n    \"if TYPE_CHECKING:\",\n]\n</code></pre>"},{"location":"modules/00-environment-setup/#conftest-for-airflow-testing","title":"Conftest for Airflow Testing","text":"<pre><code># tests/conftest.py\n\"\"\"Pytest fixtures for Airflow DAG testing.\"\"\"\n\nimport os\nfrom pathlib import Path\n\nimport pytest\n\n# Set Airflow home before importing airflow\nos.environ.setdefault(\"AIRFLOW_HOME\", str(Path(__file__).parent.parent))\n\n\n@pytest.fixture(scope=\"session\")\ndef dags_folder() -&gt; Path:\n    \"\"\"Return path to DAGs folder.\"\"\"\n    return Path(__file__).parent.parent / \"dags\"\n\n\n@pytest.fixture(scope=\"session\")\ndef dag_files(dags_folder: Path) -&gt; list[Path]:\n    \"\"\"Return list of all DAG files.\"\"\"\n    return list(dags_folder.rglob(\"*.py\"))\n</code></pre>"},{"location":"modules/00-environment-setup/#dag-integrity-tests","title":"DAG Integrity Tests","text":"<pre><code># tests/test_dag_integrity.py\n\"\"\"Tests for DAG integrity and validity.\"\"\"\n\nimport importlib.util\nfrom pathlib import Path\n\nimport pytest\n\n\ndef test_dags_can_be_imported(dag_files: list[Path]) -&gt; None:\n    \"\"\"Verify all DAG files can be imported without errors.\"\"\"\n    for dag_file in dag_files:\n        spec = importlib.util.spec_from_file_location(dag_file.stem, dag_file)\n        module = importlib.util.module_from_spec(spec)\n        try:\n            spec.loader.exec_module(module)\n        except Exception as e:\n            pytest.fail(f\"Failed to import {dag_file}: {e}\")\n\n\ndef test_no_import_errors(dag_files: list[Path]) -&gt; None:\n    \"\"\"Check DAGs don't have import errors.\"\"\"\n    from airflow.models import DagBag\n\n    dag_bag = DagBag(include_examples=False)\n    assert len(dag_bag.import_errors) == 0, f\"DAG import errors: {dag_bag.import_errors}\"\n\n\ndef test_no_cycles(dag_files: list[Path]) -&gt; None:\n    \"\"\"Verify DAGs don't have circular dependencies.\"\"\"\n    from airflow.models import DagBag\n\n    dag_bag = DagBag(include_examples=False)\n    for dag_id, dag in dag_bag.dags.items():\n        # This will raise if there are cycles\n        _ = dag.topological_sort()\n</code></pre>"},{"location":"modules/00-environment-setup/#running-tests","title":"Running Tests","text":"<pre><code># Run all tests\nuv run pytest\n\n# Run with coverage\nuv run pytest --cov=dags --cov-report=term-missing\n\n# Run specific test file\nuv run pytest tests/test_dag_integrity.py -v\n</code></pre>"},{"location":"modules/00-environment-setup/#8-putting-it-all-together","title":"8. Putting It All Together","text":""},{"location":"modules/00-environment-setup/#quick-reference-commands","title":"Quick Reference Commands","text":"<pre><code># Project setup\nuv init my-project &amp;&amp; cd my-project\nuv add apache-airflow\nuv add --dev ruff pre-commit pytest pytest-cov\nuv venv &amp;&amp; uv sync --all-groups\nuv run pre-commit install\n\n# Daily workflow\nuv run ruff check --fix .     # Lint and fix\nuv run ruff format .          # Format\nuv run pytest                 # Test\ngit commit -m \"feature\"       # Pre-commit runs automatically\n\n# Docker\ndocker build -t my-airflow .\ndocker run -p 8080:8080 my-airflow\n\n# Dependency management\nuv add package               # Add dependency\nuv add --dev package         # Add dev dependency\nuv remove package            # Remove dependency\nuv sync                      # Install from lock file\nuv lock --upgrade            # Update all dependencies\n</code></pre>"},{"location":"modules/00-environment-setup/#complete-project-template","title":"Complete Project Template","text":"<pre><code>my-airflow-project/\n\u251c\u2500\u2500 pyproject.toml           # Project configuration\n\u251c\u2500\u2500 uv.lock                  # Lock file (auto-generated)\n\u251c\u2500\u2500 .pre-commit-config.yaml  # Pre-commit hooks\n\u251c\u2500\u2500 .gitignore\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 Dockerfile\n\u251c\u2500\u2500 docker-compose.yaml\n\u251c\u2500\u2500 dags/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2514\u2500\u2500 example_dag.py\n\u251c\u2500\u2500 plugins/\n\u2502   \u2514\u2500\u2500 __init__.py\n\u2514\u2500\u2500 tests/\n    \u251c\u2500\u2500 __init__.py\n    \u251c\u2500\u2500 conftest.py\n    \u2514\u2500\u2500 test_dag_integrity.py\n</code></pre>"},{"location":"modules/00-environment-setup/#github-actions-cicd-example","title":"GitHub Actions CI/CD Example","text":"<pre><code># .github/workflows/ci.yml\nname: CI\n\non:\n  push:\n    branches: [main]\n  pull_request:\n    branches: [main]\n\njobs:\n  lint-and-test:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Set up uv\n        uses: astral-sh/setup-uv@v4\n\n      - name: Install dependencies\n        run: uv sync --all-groups\n\n      - name: Run linting\n        run: uv run ruff check .\n\n      - name: Check formatting\n        run: uv run ruff format --check .\n\n      - name: Run tests\n        run: uv run pytest --cov=dags\n\n  docker:\n    runs-on: ubuntu-latest\n    needs: lint-and-test\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Build Docker image\n        run: docker build -t airflow-test .\n</code></pre>"},{"location":"modules/00-environment-setup/#exercises","title":"\ud83d\udcdd Exercises","text":"<p>Complete these exercises to set up your own development environment:</p> <ol> <li>Exercise 0.1: Install uv and initialize an Airflow project</li> <li>Exercise 0.2: Configure pyproject.toml with dependency groups</li> <li>Exercise 0.3: Set up ruff and pre-commit hooks</li> <li>Exercise 0.4: Build an optimized Airflow Docker image</li> <li>Exercise 0.5: Create a pytest testing workflow</li> </ol>"},{"location":"modules/00-environment-setup/#checkpoint","title":"\u2705 Checkpoint","text":"<p>Before moving to Module 01, ensure you can:</p> <ul> <li> Create a new project with <code>uv init</code> and add dependencies</li> <li> Understand the structure of pyproject.toml</li> <li> Run ruff to lint and format your code</li> <li> Have pre-commit hooks running on every commit</li> <li> Build a Docker image with uv</li> <li> Run pytest to validate DAGs</li> </ul>"},{"location":"modules/00-environment-setup/#verification-commands","title":"Verification Commands","text":"<p>Run these commands to verify your environment is properly configured:</p> <pre><code># 1. Verify uv installation\nuv --version\n# Expected: uv 0.5.x or higher\n\n# 2. Verify project structure\nls pyproject.toml uv.lock\n# Expected: Both files should exist\n\n# 3. Verify Airflow installation\nuv run python -c \"import airflow; print(f'Airflow {airflow.__version__}')\"\n# Expected: Airflow 3.x.x\n\n# 4. Verify ruff works\nuv run ruff check --version\n# Expected: ruff 0.8.x\n\n# 5. Verify pre-commit hooks\ntest -f .git/hooks/pre-commit &amp;&amp; echo \"Pre-commit installed\" || echo \"Not installed\"\n# Expected: Pre-commit installed\n\n# 6. Verify Docker build\ndocker images | grep airflow-learning\n# Expected: airflow-learning image listed\n\n# 7. Verify tests pass\nuv run pytest tests/ -q\n# Expected: All tests passed\n</code></pre> <p>If any command fails, revisit the corresponding exercise before proceeding.</p>"},{"location":"modules/00-environment-setup/#further-reading","title":"\ud83d\udcda Further Reading","text":"<ul> <li>uv Documentation</li> <li>Ruff Documentation</li> <li>PEP 621 - pyproject.toml metadata</li> <li>Pre-commit Documentation</li> <li>Airflow Docker Best Practices</li> </ul> <p>Next: Module 01: Foundations \u2192</p> <p>_[DAG]: Directed Acyclic Graph - Airflow's core workflow abstraction _[XCom]: Cross-Communication - Airflow's mechanism for task-to-task data passing _[Executor]: The component that runs tasks (Local, Celery, Kubernetes, etc.) _[Scheduler]: The component that triggers DAGs and determines task execution order _[Webserver]: The component that serves the Airflow UI _[Provider]: An Airflow package that integrates with external systems _[TaskFlow]: The modern Python-native API for defining Airflow tasks using decorators _[Asset]: A logical representation of data that DAGs can produce or consume _[SDK]: Software Development Kit - the airflow.sdk module in Airflow 3.x _[K8s]: Kubernetes - container orchestration platform _[uv]: A fast Python package manager by Astral _[ruff]: A fast Python linter and formatter by Astral _[CI]: Continuous Integration _[CD]: Continuous Deployment *[PR]: Pull Request</p>"},{"location":"modules/00-environment-setup/exercises/exercise_0_1/","title":"Exercise 0.1: Project Initialization with uv","text":""},{"location":"modules/00-environment-setup/exercises/exercise_0_1/#objective","title":"\ud83c\udfaf Objective","text":"<p>Install uv and create a new Airflow project from scratch with proper virtual environment management.</p>"},{"location":"modules/00-environment-setup/exercises/exercise_0_1/#estimated-time-15-20-minutes","title":"\u23f1\ufe0f Estimated Time: 15-20 minutes","text":""},{"location":"modules/00-environment-setup/exercises/exercise_0_1/#prerequisites","title":"Prerequisites","text":"<ul> <li>macOS, Linux, or Windows with WSL</li> <li>Terminal/command line access</li> <li>No prior Python installation required (uv handles this)</li> </ul>"},{"location":"modules/00-environment-setup/exercises/exercise_0_1/#tasks","title":"Tasks","text":""},{"location":"modules/00-environment-setup/exercises/exercise_0_1/#task-1-install-uv","title":"Task 1: Install uv","text":"<p>Choose the appropriate installation method for your system:</p> <p>macOS/Linux (recommended):</p> <pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre> <p>Windows (PowerShell):</p> <pre><code>powershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n</code></pre> <p>Alternative: Via Homebrew (macOS):</p> <pre><code>brew install uv\n</code></pre> <p>After installation, restart your terminal and verify:</p> <pre><code>uv --version\n</code></pre> <p>Expected output: <code>uv 0.5.x</code> or higher</p>"},{"location":"modules/00-environment-setup/exercises/exercise_0_1/#task-2-create-a-new-project-directory","title":"Task 2: Create a New Project Directory","text":"<pre><code># Create and enter project directory\nmkdir airflow-learning\ncd airflow-learning\n</code></pre>"},{"location":"modules/00-environment-setup/exercises/exercise_0_1/#task-3-initialize-the-project-with-uv","title":"Task 3: Initialize the Project with uv","text":"<pre><code># Initialize a new Python project\nuv init\n</code></pre> <p>This creates:</p> <ul> <li><code>pyproject.toml</code> - Project configuration</li> <li><code>.python-version</code> - Python version specification</li> <li><code>README.md</code> - Project readme</li> <li><code>hello.py</code> - Sample Python file</li> </ul> <p>Examine the generated <code>pyproject.toml</code>:</p> <pre><code>cat pyproject.toml\n</code></pre>"},{"location":"modules/00-environment-setup/exercises/exercise_0_1/#task-4-set-python-version","title":"Task 4: Set Python Version","text":"<p>Ensure you're using a compatible Python version:</p> <pre><code># Set Python version (uv will download if needed)\nuv python pin 3.11\n</code></pre>"},{"location":"modules/00-environment-setup/exercises/exercise_0_1/#task-5-add-apache-airflow-as-a-dependency","title":"Task 5: Add Apache Airflow as a Dependency","text":"<pre><code># Add Airflow to the project\nuv add apache-airflow\n</code></pre> <p>Watch the output\u2014notice how fast uv resolves and installs dependencies compared to pip!</p>"},{"location":"modules/00-environment-setup/exercises/exercise_0_1/#task-6-create-virtual-environment-and-sync","title":"Task 6: Create Virtual Environment and Sync","text":"<pre><code># Create virtual environment\nuv venv\n\n# Sync dependencies from lock file\nuv sync\n</code></pre>"},{"location":"modules/00-environment-setup/exercises/exercise_0_1/#task-7-verify-the-installation","title":"Task 7: Verify the Installation","text":"<pre><code># Run Python in the virtual environment\nuv run python -c \"import airflow; print(f'Airflow version: {airflow.__version__}')\"\n</code></pre> <p>Expected output: <code>Airflow version: 3.x.x</code></p>"},{"location":"modules/00-environment-setup/exercises/exercise_0_1/#task-8-explore-the-lock-file","title":"Task 8: Explore the Lock File","text":"<p>Open <code>uv.lock</code> and examine its contents:</p> <pre><code>head -50 uv.lock\n</code></pre> <p>Notice how it records:</p> <ul> <li>Exact versions of all packages</li> <li>Package hashes for security</li> <li>Platform-specific markers</li> </ul>"},{"location":"modules/00-environment-setup/exercises/exercise_0_1/#verification-checklist","title":"Verification Checklist","text":"<p>Confirm your project has:</p> <ul> <li> <code>pyproject.toml</code> with <code>apache-airflow</code> in dependencies</li> <li> <code>uv.lock</code> file (do not edit manually)</li> <li> <code>.venv/</code> directory (virtual environment)</li> <li> <code>.python-version</code> file</li> </ul> <p>Your directory structure should look like:</p> <pre><code>airflow-learning/\n\u251c\u2500\u2500 .python-version\n\u251c\u2500\u2500 .venv/\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 hello.py\n\u251c\u2500\u2500 pyproject.toml\n\u2514\u2500\u2500 uv.lock\n</code></pre>"},{"location":"modules/00-environment-setup/exercises/exercise_0_1/#key-learnings","title":"\ud83d\udca1 Key Learnings","text":"<ol> <li>uv handles everything: Python installation, virtual environments, package installation</li> <li>Speed: Notice how fast dependency resolution is compared to pip</li> <li>Lock files: <code>uv.lock</code> ensures reproducible builds across machines</li> <li>No activation needed: <code>uv run</code> executes commands in the virtual environment without manual activation</li> </ol>"},{"location":"modules/00-environment-setup/exercises/exercise_0_1/#troubleshooting","title":"\ud83d\udd27 Troubleshooting","text":""},{"location":"modules/00-environment-setup/exercises/exercise_0_1/#command-not-found-uv","title":"\"command not found: uv\"","text":"<p>Cause: uv is not in your PATH or shell configuration wasn't reloaded.</p> <p>Solution:</p> <pre><code># Reload your shell configuration\nsource ~/.bashrc  # or ~/.zshrc for zsh\n\n# Or add manually to PATH\nexport PATH=\"$HOME/.local/bin:$PATH\"\n\n# Verify\nwhich uv\n</code></pre>"},{"location":"modules/00-environment-setup/exercises/exercise_0_1/#permission-denied-during-installation","title":"\"Permission denied\" during installation","text":"<p>Cause: Installation script doesn't have execute permissions.</p> <p>Solution:</p> <pre><code># Use sudo for system-wide installation\ncurl -LsSf https://astral.sh/uv/install.sh | sudo sh\n\n# Or install to user directory (preferred)\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre>"},{"location":"modules/00-environment-setup/exercises/exercise_0_1/#python-311-not-found-error","title":"\"Python 3.11 not found\" error","text":"<p>Cause: uv needs to download Python but can't.</p> <p>Solution:</p> <pre><code># Let uv download Python\nuv python install 3.11\n\n# Or use system Python if available\nuv venv --python $(which python3)\n</code></pre>"},{"location":"modules/00-environment-setup/exercises/exercise_0_1/#no-space-left-on-device","title":"\"No space left on device\"","text":"<p>Cause: Insufficient disk space for dependencies.</p> <p>Solution: Free up at least 5GB of disk space. Airflow with dependencies requires significant space.</p>"},{"location":"modules/00-environment-setup/exercises/exercise_0_1/#windows-specific-issues","title":"Windows-Specific Issues","text":"<p>If using Windows without WSL, some commands may differ:</p> <ul> <li>Use PowerShell instead of bash</li> <li>Paths use backslashes (<code>\\</code>) instead of forward slashes</li> <li>Consider using WSL2 for a more consistent Linux-like experience</li> </ul>"},{"location":"modules/00-environment-setup/exercises/exercise_0_1/#bonus-challenge","title":"\ud83d\ude80 Bonus Challenge","text":"<p>Try these additional commands to explore uv:</p> <pre><code># List installed packages\nuv pip list\n\n# Show package info\nuv pip show apache-airflow\n\n# Check for outdated packages\nuv pip list --outdated\n\n# Add a development dependency\nuv add --dev ipython\n</code></pre>"},{"location":"modules/00-environment-setup/exercises/exercise_0_1/#reference","title":"\ud83d\udcda Reference","text":"<ul> <li>uv Documentation</li> <li>uv Project Management</li> </ul> <p>Next: Exercise 0.2: Configure pyproject.toml \u2192</p> <p>_[DAG]: Directed Acyclic Graph - Airflow's core workflow abstraction _[XCom]: Cross-Communication - Airflow's mechanism for task-to-task data passing _[Executor]: The component that runs tasks (Local, Celery, Kubernetes, etc.) _[Scheduler]: The component that triggers DAGs and determines task execution order _[Webserver]: The component that serves the Airflow UI _[Provider]: An Airflow package that integrates with external systems _[TaskFlow]: The modern Python-native API for defining Airflow tasks using decorators _[Asset]: A logical representation of data that DAGs can produce or consume _[SDK]: Software Development Kit - the airflow.sdk module in Airflow 3.x _[K8s]: Kubernetes - container orchestration platform _[uv]: A fast Python package manager by Astral _[ruff]: A fast Python linter and formatter by Astral _[CI]: Continuous Integration _[CD]: Continuous Deployment *[PR]: Pull Request</p>"},{"location":"modules/00-environment-setup/exercises/exercise_0_2/","title":"Exercise 0.2: Configure pyproject.toml for Airflow","text":""},{"location":"modules/00-environment-setup/exercises/exercise_0_2/#objective","title":"\ud83c\udfaf Objective","text":"<p>Set up a production-ready <code>pyproject.toml</code> with proper dependency groups for an Airflow project.</p>"},{"location":"modules/00-environment-setup/exercises/exercise_0_2/#estimated-time-20-25-minutes","title":"\u23f1\ufe0f Estimated Time: 20-25 minutes","text":""},{"location":"modules/00-environment-setup/exercises/exercise_0_2/#prerequisites","title":"Prerequisites","text":"<ul> <li>Completed Exercise 0.1</li> <li>uv installed and working</li> <li>Project directory with initial <code>pyproject.toml</code></li> </ul>"},{"location":"modules/00-environment-setup/exercises/exercise_0_2/#background","title":"Background","text":"<p>A well-structured <code>pyproject.toml</code> separates dependencies into groups:</p> Group Purpose Example Packages <code>dependencies</code> Runtime (production) apache-airflow <code>dev</code> Development tools ruff, pre-commit <code>test</code> Testing pytest, pytest-cov <code>providers</code> Airflow providers postgres, http providers"},{"location":"modules/00-environment-setup/exercises/exercise_0_2/#tasks","title":"Tasks","text":""},{"location":"modules/00-environment-setup/exercises/exercise_0_2/#task-1-start-fresh-or-continue","title":"Task 1: Start Fresh or Continue","text":"<p>If continuing from Exercise 0.1:</p> <pre><code>cd airflow-learning\n</code></pre> <p>Or create a new project:</p> <pre><code>mkdir airflow-config-exercise\ncd airflow-config-exercise\nuv init\nuv python pin 3.11\n</code></pre>"},{"location":"modules/00-environment-setup/exercises/exercise_0_2/#task-2-edit-pyprojecttoml","title":"Task 2: Edit pyproject.toml","text":"<p>Open <code>pyproject.toml</code> in your editor and replace its contents with:</p> <pre><code>[project]\nname = \"airflow-learning\"\nversion = \"0.1.0\"\ndescription = \"Learning Apache Airflow 3.x\"\nreadme = \"README.md\"\nrequires-python = \"&gt;=3.9,&lt;3.13\"\nlicense = { text = \"MIT\" }\nauthors = [\n    { name = \"Your Name\", email = \"you@example.com\" }\n]\n\n# Core dependencies - always installed\ndependencies = [\n    \"apache-airflow&gt;=3.0.0,&lt;4.0.0\",\n]\n\n# Optional dependency groups\n[project.optional-dependencies]\n# Development tools\ndev = [\n    \"ruff&gt;=0.8.0\",\n    \"pre-commit&gt;=4.0.0\",\n    \"ipython&gt;=8.0.0\",\n]\n\n# Testing dependencies\ntest = [\n    \"pytest&gt;=8.0.0\",\n    \"pytest-cov&gt;=4.0.0\",\n]\n\n# Common Airflow providers\nproviders = [\n    \"apache-airflow-providers-postgres&gt;=5.0.0\",\n    \"apache-airflow-providers-http&gt;=4.0.0\",\n    \"apache-airflow-providers-standard&gt;=0.1.0\",\n]\n\n# Install all optional dependencies\nall = [\n    \"airflow-learning[dev,test,providers]\",\n]\n\n[build-system]\nrequires = [\"hatchling\"]\nbuild-backend = \"hatchling.build\"\n\n# Tool configurations will be added in later exercises\n[tool.ruff]\nline-length = 88\ntarget-version = \"py311\"\n\n[tool.pytest.ini_options]\ntestpaths = [\"tests\"]\npython_files = [\"test_*.py\"]\n</code></pre>"},{"location":"modules/00-environment-setup/exercises/exercise_0_2/#task-3-update-the-lock-file","title":"Task 3: Update the Lock File","text":"<p>After editing <code>pyproject.toml</code>, regenerate the lock file:</p> <pre><code>uv lock\n</code></pre> <p>This resolves all dependencies (including optional groups) and updates <code>uv.lock</code>.</p>"},{"location":"modules/00-environment-setup/exercises/exercise_0_2/#task-4-install-core-dependencies-only","title":"Task 4: Install Core Dependencies Only","text":"<pre><code>uv sync\n</code></pre> <p>This installs only the packages in <code>dependencies</code> (apache-airflow).</p>"},{"location":"modules/00-environment-setup/exercises/exercise_0_2/#task-5-install-development-dependencies","title":"Task 5: Install Development Dependencies","text":"<pre><code>uv sync --group dev\n</code></pre> <p>Verify the dev tools are installed:</p> <pre><code>uv run ruff --version\nuv run ipython --version\n</code></pre>"},{"location":"modules/00-environment-setup/exercises/exercise_0_2/#task-6-install-testing-dependencies","title":"Task 6: Install Testing Dependencies","text":"<pre><code>uv sync --group test\n</code></pre> <p>Verify pytest is available:</p> <pre><code>uv run pytest --version\n</code></pre>"},{"location":"modules/00-environment-setup/exercises/exercise_0_2/#task-7-install-airflow-providers","title":"Task 7: Install Airflow Providers","text":"<pre><code>uv sync --group providers\n</code></pre> <p>Verify a provider is available:</p> <pre><code>uv run python -c \"from airflow.providers.http.operators.http import HttpOperator; print('HTTP provider loaded!')\"\n</code></pre>"},{"location":"modules/00-environment-setup/exercises/exercise_0_2/#task-8-install-everything","title":"Task 8: Install Everything","text":"<p>To install all dependency groups at once:</p> <pre><code>uv sync --all-groups\n</code></pre>"},{"location":"modules/00-environment-setup/exercises/exercise_0_2/#task-9-view-installed-packages","title":"Task 9: View Installed Packages","text":"<pre><code># List all installed packages\nuv pip list\n\n# Count packages (should be 50+)\nuv pip list | wc -l\n</code></pre>"},{"location":"modules/00-environment-setup/exercises/exercise_0_2/#understanding-version-constraints","title":"Understanding Version Constraints","text":"Syntax Meaning Example <code>&gt;=3.0.0</code> At least this version <code>&gt;=3.0.0</code> means 3.0.0 or higher <code>&lt;4.0.0</code> Below this version <code>&lt;4.0.0</code> means any 3.x version <code>&gt;=3.0.0,&lt;4.0.0</code> Range Any 3.x version <code>~=3.0</code> Compatible release 3.0.x (patch updates only) <code>==3.0.0</code> Exact version Only 3.0.0 <p>For Airflow projects, we typically use <code>&gt;=3.0.0,&lt;4.0.0</code> to accept any Airflow 3.x version while preventing accidental upgrades to Airflow 4.x (which may have breaking changes).</p>"},{"location":"modules/00-environment-setup/exercises/exercise_0_2/#verification-checklist","title":"Verification Checklist","text":"<p>Your <code>pyproject.toml</code> should have:</p> <ul> <li> Project metadata (name, version, description)</li> <li> Python version constraint (<code>requires-python</code>)</li> <li> Core <code>dependencies</code> with apache-airflow</li> <li> <code>dev</code> optional group with ruff, pre-commit</li> <li> <code>test</code> optional group with pytest</li> <li> <code>providers</code> optional group with at least one provider</li> <li> <code>all</code> group that combines all optional groups</li> </ul> <p>Verify all groups install correctly:</p> <pre><code>uv sync --all-groups\nuv run python -c \"import airflow; import ruff; import pytest; print('All groups installed!')\"\n</code></pre>"},{"location":"modules/00-environment-setup/exercises/exercise_0_2/#key-learnings","title":"\ud83d\udca1 Key Learnings","text":"<ol> <li>Dependency groups keep production images lean (no dev/test tools)</li> <li>Version constraints prevent unexpected breaking changes</li> <li><code>uv lock</code> must be run after changing dependencies</li> <li><code>uv sync</code> installs from the lock file for reproducibility</li> </ol>"},{"location":"modules/00-environment-setup/exercises/exercise_0_2/#bonus-challenge","title":"\ud83d\ude80 Bonus Challenge","text":"<p>Add these additional configurations to your <code>pyproject.toml</code>:</p> <ol> <li>Add your own custom metadata:</li> </ol> <pre><code>keywords = [\"airflow\", \"data-engineering\", \"etl\"]\nclassifiers = [\n    \"Development Status :: 3 - Alpha\",\n    \"Intended Audience :: Developers\",\n    \"Programming Language :: Python :: 3.11\",\n]\n</code></pre> <ol> <li>Add a <code>docs</code> dependency group:</li> </ol> <pre><code>docs = [\n    \"mkdocs&gt;=1.5.0\",\n    \"mkdocs-material&gt;=9.0.0\",\n]\n</code></pre> <ol> <li>Update the <code>all</code> group to include <code>docs</code></li> </ol>"},{"location":"modules/00-environment-setup/exercises/exercise_0_2/#reference","title":"\ud83d\udcda Reference","text":"<ul> <li>PEP 621 - pyproject.toml metadata</li> <li>uv Dependency Groups</li> <li>Airflow Installation</li> </ul> <p>Next: Exercise 0.3: Set up ruff and pre-commit \u2192</p> <p>_[DAG]: Directed Acyclic Graph - Airflow's core workflow abstraction _[XCom]: Cross-Communication - Airflow's mechanism for task-to-task data passing _[Executor]: The component that runs tasks (Local, Celery, Kubernetes, etc.) _[Scheduler]: The component that triggers DAGs and determines task execution order _[Webserver]: The component that serves the Airflow UI _[Provider]: An Airflow package that integrates with external systems _[TaskFlow]: The modern Python-native API for defining Airflow tasks using decorators _[Asset]: A logical representation of data that DAGs can produce or consume _[SDK]: Software Development Kit - the airflow.sdk module in Airflow 3.x _[K8s]: Kubernetes - container orchestration platform _[uv]: A fast Python package manager by Astral _[ruff]: A fast Python linter and formatter by Astral _[CI]: Continuous Integration _[CD]: Continuous Deployment *[PR]: Pull Request</p>"},{"location":"modules/00-environment-setup/exercises/exercise_0_3/","title":"Exercise 0.3: Code Quality Setup (Ruff + Pre-commit)","text":""},{"location":"modules/00-environment-setup/exercises/exercise_0_3/#objective","title":"\ud83c\udfaf Objective","text":"<p>Configure ruff for linting and formatting, and set up pre-commit hooks for automated code quality checks.</p>"},{"location":"modules/00-environment-setup/exercises/exercise_0_3/#estimated-time-20-25-minutes","title":"\u23f1\ufe0f Estimated Time: 20-25 minutes","text":""},{"location":"modules/00-environment-setup/exercises/exercise_0_3/#prerequisites","title":"Prerequisites","text":"<ul> <li>Completed Exercise 0.2</li> <li>Project with <code>pyproject.toml</code> and dev dependencies installed</li> <li>Git installed (<code>git --version</code>)</li> </ul>"},{"location":"modules/00-environment-setup/exercises/exercise_0_3/#tasks","title":"Tasks","text":""},{"location":"modules/00-environment-setup/exercises/exercise_0_3/#task-1-initialize-git-repository","title":"Task 1: Initialize Git Repository","text":"<p>If not already initialized:</p> <pre><code>cd airflow-learning\ngit init\n</code></pre>"},{"location":"modules/00-environment-setup/exercises/exercise_0_3/#task-2-configure-ruff-in-pyprojecttoml","title":"Task 2: Configure Ruff in pyproject.toml","text":"<p>Add or update the ruff configuration in your <code>pyproject.toml</code>:</p> <pre><code>[tool.ruff]\n# Match Black's default line length\nline-length = 88\n\n# Target Python version\ntarget-version = \"py311\"\n\n# Exclude common directories\nexclude = [\n    \".git\",\n    \".venv\",\n    \"__pycache__\",\n    \"build\",\n    \"dist\",\n    \".ruff_cache\",\n]\n\n[tool.ruff.lint]\n# Enable these rule sets\nselect = [\n    \"E\",      # pycodestyle errors\n    \"W\",      # pycodestyle warnings\n    \"F\",      # Pyflakes (unused imports, variables)\n    \"I\",      # isort (import sorting)\n    \"B\",      # flake8-bugbear (common bugs)\n    \"C4\",     # flake8-comprehensions\n    \"UP\",     # pyupgrade (modern Python syntax)\n    \"SIM\",    # flake8-simplify\n    \"AIR\",    # Airflow-specific rules\n]\n\n# Rules to ignore\nignore = [\n    \"E501\",   # Line too long (handled by formatter)\n]\n\n# Allow autofix for all rules\nfixable = [\"ALL\"]\n\n[tool.ruff.lint.isort]\n# Your project's first-party packages\nknown-first-party = [\"dags\", \"plugins\", \"tests\"]\n\n[tool.ruff.format]\n# Use double quotes like Black\nquote-style = \"double\"\n\n# Use spaces for indentation\nindent-style = \"space\"\n\n# Format docstrings\ndocstring-code-format = true\n</code></pre>"},{"location":"modules/00-environment-setup/exercises/exercise_0_3/#task-3-create-a-sample-dag-file","title":"Task 3: Create a Sample DAG File","text":"<p>Create a DAG file with intentional style issues:</p> <pre><code>mkdir -p dags\n</code></pre> <p>Create <code>dags/sample_dag.py</code> with this content (intentionally messy):</p> <pre><code>\"\"\"Sample DAG demonstrating TaskFlow API with ruff linting.\"\"\"\n\nfrom datetime import datetime\n\nfrom airflow.sdk import dag, task\n\ndefault_args = {\n    \"owner\": \"learner\",\n    \"retries\": 1,\n}\n\n\n@dag(\n    dag_id=\"sample_dag\",\n    start_date=datetime(2024, 1, 1),\n    schedule=None,\n    catchup=False,\n    default_args=default_args,\n    tags=[\"exercise\"],\n    description=\"Sample DAG for Exercise 0.3\",\n)\ndef sample_dag():\n    \"\"\"Sample ETL pipeline using TaskFlow API.\"\"\"\n\n    @task\n    def extract():\n        \"\"\"Extract data from source.\"\"\"\n        data = {\"users\": 100, \"events\": 500}\n        return data\n\n    @task\n    def transform(data: dict) -&gt; dict:\n        \"\"\"Transform extracted data.\"\"\"\n        data[\"processed\"] = True\n        return data\n\n    @task\n    def load(data: dict) -&gt; None:\n        \"\"\"Load data to destination.\"\"\"\n        print(f\"Loading: {data}\")\n\n    raw = extract()\n    transformed = transform(raw)\n    load(transformed)\n\n\n# Instantiate the DAG\nsample_dag()\n</code></pre>"},{"location":"modules/00-environment-setup/exercises/exercise_0_3/#task-4-run-ruff-linter","title":"Task 4: Run Ruff Linter","text":"<p>Check for issues:</p> <pre><code>uv run ruff check dags/\n</code></pre> <p>You should see errors about:</p> <ul> <li>Import sorting (I001)</li> <li>Line length (if any lines are too long)</li> <li>Unused imports (F401 - json, os are unused)</li> </ul>"},{"location":"modules/00-environment-setup/exercises/exercise_0_3/#task-5-auto-fix-issues","title":"Task 5: Auto-fix Issues","text":"<p>Let ruff fix what it can:</p> <pre><code>uv run ruff check --fix dags/\n</code></pre> <p>Check the file again\u2014many issues should be fixed automatically.</p>"},{"location":"modules/00-environment-setup/exercises/exercise_0_3/#task-6-format-the-code","title":"Task 6: Format the Code","text":"<pre><code>uv run ruff format dags/\n</code></pre> <p>View the formatted file:</p> <pre><code>cat dags/sample_dag.py\n</code></pre> <p>The code should now be properly formatted with:</p> <ul> <li>Correct spacing around operators</li> <li>Proper line breaks</li> <li>Sorted imports</li> </ul>"},{"location":"modules/00-environment-setup/exercises/exercise_0_3/#task-7-create-pre-commit-configuration","title":"Task 7: Create Pre-commit Configuration","text":"<p>Create <code>.pre-commit-config.yaml</code>:</p> <pre><code># .pre-commit-config.yaml\nrepos:\n  # Ruff for linting and formatting\n  - repo: https://github.com/astral-sh/ruff-pre-commit\n    rev: v0.8.0\n    hooks:\n      - id: ruff\n        name: ruff (lint)\n        args: [--fix]\n      - id: ruff-format\n        name: ruff (format)\n\n  # General file hygiene\n  - repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v5.0.0\n    hooks:\n      - id: trailing-whitespace\n        name: trim trailing whitespace\n      - id: end-of-file-fixer\n        name: fix end of files\n      - id: check-yaml\n        name: check yaml syntax\n        args: [--unsafe] # Allow custom YAML tags (Airflow uses them)\n      - id: check-added-large-files\n        name: check for large files\n        args: [--maxkb=500]\n      - id: check-merge-conflict\n        name: check for merge conflicts\n      - id: check-toml\n        name: check toml syntax\n</code></pre>"},{"location":"modules/00-environment-setup/exercises/exercise_0_3/#task-8-install-pre-commit-hooks","title":"Task 8: Install Pre-commit Hooks","text":"<pre><code># Ensure pre-commit is installed\nuv sync --group dev\n\n# Install hooks into git\nuv run pre-commit install\n</code></pre> <p>Expected output: <code>pre-commit installed at .git/hooks/pre-commit</code></p>"},{"location":"modules/00-environment-setup/exercises/exercise_0_3/#task-9-run-pre-commit-on-all-files","title":"Task 9: Run Pre-commit on All Files","text":"<pre><code>uv run pre-commit run --all-files\n</code></pre> <p>On first run, this will:</p> <ol> <li>Download and cache the hook environments</li> <li>Run all hooks on all files</li> <li>Report any issues found</li> </ol>"},{"location":"modules/00-environment-setup/exercises/exercise_0_3/#task-10-test-the-hook-with-a-commit","title":"Task 10: Test the Hook with a Commit","text":"<p>Make a change and try to commit:</p> <pre><code># Add all files\ngit add .\n\n# Try to commit\ngit commit -m \"Add sample DAG with code quality tools\"\n</code></pre> <p>If pre-commit finds issues, it will:</p> <ol> <li>Block the commit</li> <li>Fix what it can automatically</li> <li>Show you what was changed</li> </ol> <p>After fixes, re-add and commit:</p> <pre><code>git add .\ngit commit -m \"Add sample DAG with code quality tools\"\n</code></pre>"},{"location":"modules/00-environment-setup/exercises/exercise_0_3/#verification-checklist","title":"Verification Checklist","text":"<ul> <li> <code>[tool.ruff]</code> configuration in pyproject.toml</li> <li> <code>.pre-commit-config.yaml</code> file exists</li> <li> <code>uv run ruff check .</code> runs without errors</li> <li> <code>uv run ruff format --check .</code> shows no formatting needed</li> <li> <code>uv run pre-commit run --all-files</code> passes</li> <li> Git hooks installed (<code>.git/hooks/pre-commit</code> exists)</li> </ul>"},{"location":"modules/00-environment-setup/exercises/exercise_0_3/#key-learnings","title":"\ud83d\udca1 Key Learnings","text":"<ol> <li>Ruff replaces multiple tools: One tool for linting, formatting, and import sorting</li> <li>Auto-fix is powerful: <code>--fix</code> automatically corrects many issues</li> <li>Pre-commit catches issues early: Before code enters version control</li> <li>Airflow-specific rules: AIR rules catch common Airflow mistakes</li> </ol>"},{"location":"modules/00-environment-setup/exercises/exercise_0_3/#airflow-specific-rules-reference","title":"Airflow-Specific Rules Reference","text":"Rule Description Example AIR001 Task variable name should match task_id <code>extract = extract()</code> \u2713 AIR301 Avoid deprecated <code>schedule_interval</code> Use <code>schedule</code> instead AIR302 Avoid deprecated imports Use <code>airflow.sdk</code>"},{"location":"modules/00-environment-setup/exercises/exercise_0_3/#bonus-challenge","title":"\ud83d\ude80 Bonus Challenge","text":"<ol> <li>Add a security check hook:</li> </ol> <pre><code>- repo: https://github.com/Yelp/detect-secrets\n  rev: v1.5.0\n  hooks:\n    - id: detect-secrets\n      args: [--baseline, .secrets.baseline]\n</code></pre> <ol> <li>Create VS Code settings for automatic formatting:</li> </ol> <pre><code>mkdir -p .vscode\n</code></pre> <p>Create <code>.vscode/settings.json</code>:</p> <pre><code>{\n  \"editor.formatOnSave\": true,\n  \"editor.codeActionsOnSave\": {\n    \"source.fixAll.ruff\": \"explicit\",\n    \"source.organizeImports.ruff\": \"explicit\"\n  },\n  \"[python]\": {\n    \"editor.defaultFormatter\": \"charliermarsh.ruff\"\n  }\n}\n</code></pre>"},{"location":"modules/00-environment-setup/exercises/exercise_0_3/#reference","title":"\ud83d\udcda Reference","text":"<ul> <li>Ruff Configuration</li> <li>Ruff Rules</li> <li>Pre-commit Hooks</li> </ul> <p>Next: Exercise 0.4: Docker Image Build \u2192</p> <p>_[DAG]: Directed Acyclic Graph - Airflow's core workflow abstraction _[XCom]: Cross-Communication - Airflow's mechanism for task-to-task data passing _[Executor]: The component that runs tasks (Local, Celery, Kubernetes, etc.) _[Scheduler]: The component that triggers DAGs and determines task execution order _[Webserver]: The component that serves the Airflow UI _[Provider]: An Airflow package that integrates with external systems _[TaskFlow]: The modern Python-native API for defining Airflow tasks using decorators _[Asset]: A logical representation of data that DAGs can produce or consume _[SDK]: Software Development Kit - the airflow.sdk module in Airflow 3.x _[K8s]: Kubernetes - container orchestration platform _[uv]: A fast Python package manager by Astral _[ruff]: A fast Python linter and formatter by Astral _[CI]: Continuous Integration _[CD]: Continuous Deployment *[PR]: Pull Request</p>"},{"location":"modules/00-environment-setup/exercises/exercise_0_4/","title":"Exercise 0.4: Build Airflow Docker Image with uv","text":""},{"location":"modules/00-environment-setup/exercises/exercise_0_4/#objective","title":"\ud83c\udfaf Objective","text":"<p>Build an optimized Airflow Docker image using uv with multi-stage builds and proper layer caching.</p>"},{"location":"modules/00-environment-setup/exercises/exercise_0_4/#estimated-time-25-30-minutes","title":"\u23f1\ufe0f Estimated Time: 25-30 minutes","text":""},{"location":"modules/00-environment-setup/exercises/exercise_0_4/#prerequisites","title":"Prerequisites","text":"<ul> <li>Completed Exercise 0.3</li> <li>Docker installed (<code>docker --version</code>)</li> <li>Docker daemon running</li> </ul>"},{"location":"modules/00-environment-setup/exercises/exercise_0_4/#background","title":"Background","text":"<p>Building Airflow images with uv provides:</p> <ul> <li>Faster builds: 5-15 seconds vs 45-90 seconds with pip</li> <li>Better caching: Dependency layers cache reliably</li> <li>Smaller images: Multi-stage builds exclude build tools</li> </ul>"},{"location":"modules/00-environment-setup/exercises/exercise_0_4/#tasks","title":"Tasks","text":""},{"location":"modules/00-environment-setup/exercises/exercise_0_4/#task-1-prepare-project-structure","title":"Task 1: Prepare Project Structure","text":"<p>Ensure your project has this structure:</p> <pre><code>cd airflow-learning\n\n# Create required directories\nmkdir -p dags plugins\ntouch plugins/__init__.py\n</code></pre> <p>Verify structure:</p> <pre><code>airflow-learning/\n\u251c\u2500\u2500 dags/\n\u2502   \u2514\u2500\u2500 sample_dag.py\n\u251c\u2500\u2500 plugins/\n\u2502   \u2514\u2500\u2500 __init__.py\n\u251c\u2500\u2500 pyproject.toml\n\u2514\u2500\u2500 uv.lock\n</code></pre>"},{"location":"modules/00-environment-setup/exercises/exercise_0_4/#task-2-create-the-dockerfile","title":"Task 2: Create the Dockerfile","text":"<p>Create <code>Dockerfile</code>:</p> <pre><code># syntax=docker/dockerfile:1\n\n# ============================================\n# Stage 1: Build environment with uv\n# ============================================\nFROM ghcr.io/astral-sh/uv:python3.11-bookworm-slim AS builder\n\n# Configure uv for optimal Docker builds\nENV UV_COMPILE_BYTECODE=1 \\\n    UV_LINK_MODE=copy \\\n    UV_PYTHON_DOWNLOADS=0\n\nWORKDIR /app\n\n# Copy dependency files first (for layer caching)\n# These files change less frequently than source code\nCOPY pyproject.toml uv.lock ./\n\n# Install dependencies WITHOUT installing the project\n# This layer is cached unless pyproject.toml or uv.lock changes\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv sync --locked --no-install-project --no-dev\n\n# Copy application source code\nCOPY dags/ ./dags/\nCOPY plugins/ ./plugins/\n\n# Install the project itself\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv sync --locked --no-dev\n\n# ============================================\n# Stage 2: Runtime image (minimal, no uv)\n# ============================================\nFROM python:3.11-slim-bookworm\n\n# Install runtime dependencies\nRUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends \\\n    libpq5 \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n# Create non-root user for security\nRUN groupadd --system --gid 1000 airflow \\\n    &amp;&amp; useradd --system --gid 1000 --uid 1000 --create-home airflow\n\n# Copy virtual environment from builder stage\nCOPY --from=builder --chown=airflow:airflow /app/.venv /app/.venv\n\n# Copy application code\nCOPY --from=builder --chown=airflow:airflow /app/dags /opt/airflow/dags\nCOPY --from=builder --chown=airflow:airflow /app/plugins /opt/airflow/plugins\n\n# Set environment variables\nENV PATH=\"/app/.venv/bin:$PATH\" \\\n    AIRFLOW_HOME=/opt/airflow \\\n    PYTHONUNBUFFERED=1\n\n# Switch to non-root user\nUSER airflow\nWORKDIR /opt/airflow\n\n# Default command (can be overridden)\nCMD [\"airflow\", \"version\"]\n</code></pre>"},{"location":"modules/00-environment-setup/exercises/exercise_0_4/#task-3-create-docker-composeyaml","title":"Task 3: Create docker-compose.yaml","text":"<p>Create <code>docker-compose.yaml</code> for local testing:</p> <pre><code># docker-compose.yaml\nservices:\n  # Database for Airflow metadata\n  postgres:\n    image: postgres:16-alpine\n    environment:\n      POSTGRES_USER: airflow\n      POSTGRES_PASSWORD: airflow\n      POSTGRES_DB: airflow\n    volumes:\n      - postgres_data:/var/lib/postgresql/data\n    healthcheck:\n      test: [\"CMD-SHELL\", \"pg_isready -U airflow\"]\n      interval: 5s\n      timeout: 5s\n      retries: 5\n\n  # Initialize the database\n  airflow-init:\n    build: .\n    depends_on:\n      postgres:\n        condition: service_healthy\n    environment: &amp;airflow-env\n      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow\n      AIRFLOW__CORE__EXECUTOR: LocalExecutor\n      AIRFLOW__CORE__LOAD_EXAMPLES: \"false\"\n      AIRFLOW__WEBSERVER__SECRET_KEY: \"local-dev-secret-key\"\n    command: &gt;\n      bash -c \"\n        airflow db migrate &amp;&amp;\n        airflow users create \\\n          --username admin \\\n          --password admin \\\n          --firstname Admin \\\n          --lastname User \\\n          --role Admin \\\n          --email admin@example.com || true\n      \"\n\n  # Airflow API Server (Web UI)\n  airflow-webserver:\n    build: .\n    depends_on:\n      airflow-init:\n        condition: service_completed_successfully\n    environment:\n      &lt;&lt;: *airflow-env\n    ports:\n      - \"8080:8080\"\n    command: airflow api-server --port 8080\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8080/health\"]\n      interval: 30s\n      timeout: 10s\n      retries: 5\n\n  # Airflow Scheduler\n  airflow-scheduler:\n    build: .\n    depends_on:\n      airflow-init:\n        condition: service_completed_successfully\n    environment:\n      &lt;&lt;: *airflow-env\n    command: airflow scheduler\n\nvolumes:\n  postgres_data:\n</code></pre>"},{"location":"modules/00-environment-setup/exercises/exercise_0_4/#task-4-add-dockerignore","title":"Task 4: Add .dockerignore","text":"<p>Create <code>.dockerignore</code> to exclude unnecessary files:</p> <pre><code># .dockerignore\n.git\n.gitignore\n.venv\n__pycache__\n*.pyc\n*.pyo\n.pytest_cache\n.ruff_cache\n.coverage\nhtmlcov/\n*.egg-info/\ndist/\nbuild/\n.env\n.env.*\n*.md\n!README.md\ntests/\n.pre-commit-config.yaml\n.vscode/\n.idea/\n</code></pre>"},{"location":"modules/00-environment-setup/exercises/exercise_0_4/#task-5-build-the-docker-image","title":"Task 5: Build the Docker Image","text":"<pre><code># Build the image\ndocker build -t airflow-learning:latest .\n\n# Watch the build output - notice:\n# 1. First run downloads and caches dependencies\n# 2. Subsequent runs are fast if only DAGs change\n</code></pre> <p>Expected Results:</p> Metric First Build Subsequent Builds Build Time 2-5 minutes 10-30 seconds Image Size ~800 MB - 1.2 GB Same Layer Cache Miss (downloading) Hit (cached) <p>Check your image size:</p> <pre><code>docker images airflow-learning:latest --format \"table {{.Repository}}\\t{{.Tag}}\\t{{.Size}}\"\n</code></pre>"},{"location":"modules/00-environment-setup/exercises/exercise_0_4/#task-6-verify-the-build","title":"Task 6: Verify the Build","text":"<pre><code># Check Airflow version in the image\ndocker run --rm airflow-learning:latest airflow version\n\n# List installed providers\ndocker run --rm airflow-learning:latest airflow providers list\n</code></pre>"},{"location":"modules/00-environment-setup/exercises/exercise_0_4/#task-7-test-layer-caching","title":"Task 7: Test Layer Caching","text":"<p>Make a small change to a DAG file:</p> <pre><code># Edit dags/sample_dag.py - add a comment\necho \"# Updated\" &gt;&gt; dags/sample_dag.py\n\n# Rebuild - notice how fast it is!\ndocker build -t airflow-learning:latest .\n</code></pre> <p>The dependency installation layer should be cached, making the rebuild very fast.</p>"},{"location":"modules/00-environment-setup/exercises/exercise_0_4/#task-8-run-the-full-stack","title":"Task 8: Run the Full Stack","text":"<pre><code># Start all services\ndocker compose up -d\n\n# Watch the logs\ndocker compose logs -f airflow-webserver\n\n# Wait for \"Running on http://0.0.0.0:8080\"\n</code></pre>"},{"location":"modules/00-environment-setup/exercises/exercise_0_4/#task-9-access-the-ui","title":"Task 9: Access the UI","text":"<ol> <li>Open http://localhost:8080 in your browser</li> <li>Login with:</li> <li>Username: <code>admin</code></li> <li>Password: <code>admin</code></li> <li>Navigate to DAGs and verify <code>sample_dag</code> is visible</li> </ol>"},{"location":"modules/00-environment-setup/exercises/exercise_0_4/#task-10-cleanup","title":"Task 10: Cleanup","text":"<pre><code># Stop all services\ndocker compose down\n\n# Remove volumes (optional, removes database)\ndocker compose down -v\n</code></pre>"},{"location":"modules/00-environment-setup/exercises/exercise_0_4/#understanding-the-dockerfile","title":"Understanding the Dockerfile","text":""},{"location":"modules/00-environment-setup/exercises/exercise_0_4/#why-multi-stage","title":"Why Multi-Stage?","text":"Stage Purpose Included in Final Image Builder Install dependencies with uv No Runtime Run Airflow Yes <p>Benefits:</p> <ul> <li>Final image doesn't contain uv, build tools, or cache</li> <li>Smaller image size</li> <li>Faster container startup</li> </ul>"},{"location":"modules/00-environment-setup/exercises/exercise_0_4/#why-copy-dependencies-first","title":"Why Copy Dependencies First?","text":"<pre><code># These change rarely\nCOPY pyproject.toml uv.lock ./\nRUN uv sync --locked --no-install-project\n\n# These change often\nCOPY dags/ ./dags/\n</code></pre> <p>Docker caches layers. If <code>pyproject.toml</code> and <code>uv.lock</code> don't change, the dependency installation layer is cached, making subsequent builds very fast.</p>"},{"location":"modules/00-environment-setup/exercises/exercise_0_4/#verification-checklist","title":"Verification Checklist","text":"<ul> <li> <code>Dockerfile</code> created with multi-stage build</li> <li> <code>docker-compose.yaml</code> created with postgres, init, webserver, scheduler</li> <li> <code>.dockerignore</code> excludes unnecessary files</li> <li> <code>docker build</code> completes successfully</li> <li> <code>docker run airflow-learning:latest airflow version</code> works</li> <li> <code>docker compose up</code> starts all services</li> <li> Web UI accessible at http://localhost:8080</li> <li> DAG visible in the UI</li> </ul>"},{"location":"modules/00-environment-setup/exercises/exercise_0_4/#key-learnings","title":"\ud83d\udca1 Key Learnings","text":"<ol> <li>Multi-stage builds separate build and runtime environments</li> <li>Layer caching requires ordering COPY commands by change frequency</li> <li>BuildKit cache mounts speed up repeated builds</li> <li>Non-root user improves security</li> </ol>"},{"location":"modules/00-environment-setup/exercises/exercise_0_4/#bonus-challenge","title":"\ud83d\ude80 Bonus Challenge","text":"<ol> <li>Add health checks to the Dockerfile:</li> </ol> <pre><code>HEALTHCHECK --interval=30s --timeout=10s --start-period=30s --retries=3 \\\n    CMD airflow jobs check || exit 1\n</code></pre> <ol> <li>Create a development docker-compose override:</li> </ol> <pre><code># docker-compose.override.yaml\nservices:\n  airflow-webserver:\n    volumes:\n      - ./dags:/opt/airflow/dags:ro\n</code></pre> <p>This mounts local DAGs for live development.</p> <ol> <li>Add a Makefile for convenience:</li> </ol> <pre><code>.PHONY: build up down logs\n\nbuild:\n docker build -t airflow-learning:latest .\n\nup:\n docker compose up -d\n\ndown:\n docker compose down\n\nlogs:\n docker compose logs -f\n</code></pre>"},{"location":"modules/00-environment-setup/exercises/exercise_0_4/#reference","title":"\ud83d\udcda Reference","text":"<ul> <li>uv Docker Guide</li> <li>Airflow Docker Stack</li> <li>Docker Multi-Stage Builds</li> </ul> <p>Next: Exercise 0.5: Testing Integration \u2192</p> <p>_[DAG]: Directed Acyclic Graph - Airflow's core workflow abstraction _[XCom]: Cross-Communication - Airflow's mechanism for task-to-task data passing _[Executor]: The component that runs tasks (Local, Celery, Kubernetes, etc.) _[Scheduler]: The component that triggers DAGs and determines task execution order _[Webserver]: The component that serves the Airflow UI _[Provider]: An Airflow package that integrates with external systems _[TaskFlow]: The modern Python-native API for defining Airflow tasks using decorators _[Asset]: A logical representation of data that DAGs can produce or consume _[SDK]: Software Development Kit - the airflow.sdk module in Airflow 3.x _[K8s]: Kubernetes - container orchestration platform _[uv]: A fast Python package manager by Astral _[ruff]: A fast Python linter and formatter by Astral _[CI]: Continuous Integration _[CD]: Continuous Deployment *[PR]: Pull Request</p>"},{"location":"modules/00-environment-setup/exercises/exercise_0_5/","title":"Exercise 0.5: Testing Integration with pytest","text":""},{"location":"modules/00-environment-setup/exercises/exercise_0_5/#objective","title":"\ud83c\udfaf Objective","text":"<p>Set up a complete pytest testing workflow for Airflow DAG validation with coverage reporting.</p>"},{"location":"modules/00-environment-setup/exercises/exercise_0_5/#estimated-time-25-30-minutes","title":"\u23f1\ufe0f Estimated Time: 25-30 minutes","text":""},{"location":"modules/00-environment-setup/exercises/exercise_0_5/#prerequisites","title":"Prerequisites","text":"<ul> <li>Completed Exercise 0.4</li> <li>Project with DAGs and uv configured</li> <li>Test dependencies installed (<code>uv sync --group test</code>)</li> </ul>"},{"location":"modules/00-environment-setup/exercises/exercise_0_5/#tasks","title":"Tasks","text":""},{"location":"modules/00-environment-setup/exercises/exercise_0_5/#task-1-create-tests-directory-structure","title":"Task 1: Create Tests Directory Structure","text":"<pre><code>cd airflow-learning\n\n# Create tests directory\nmkdir -p tests\ntouch tests/__init__.py\n</code></pre>"},{"location":"modules/00-environment-setup/exercises/exercise_0_5/#task-2-configure-pytest-in-pyprojecttoml","title":"Task 2: Configure pytest in pyproject.toml","text":"<p>Add or update the pytest configuration:</p> <pre><code>[tool.pytest.ini_options]\ntestpaths = [\"tests\"]\npython_files = [\"test_*.py\"]\npython_functions = [\"test_*\"]\naddopts = [\n    \"-v\",\n    \"--tb=short\",\n    \"-ra\",\n]\nfilterwarnings = [\n    \"ignore::DeprecationWarning\",\n    \"ignore::PendingDeprecationWarning\",\n]\nmarkers = [\n    \"slow: marks tests as slow (deselect with '-m \\\"not slow\\\"')\",\n    \"integration: marks tests as integration tests\",\n]\n\n[tool.coverage.run]\nsource = [\"dags\", \"plugins\"]\nomit = [\n    \"tests/*\",\n    \"*/__pycache__/*\",\n]\nbranch = true\n\n[tool.coverage.report]\nexclude_lines = [\n    \"pragma: no cover\",\n    \"if TYPE_CHECKING:\",\n    \"if __name__ == .__main__.:\",\n    \"raise NotImplementedError\",\n]\nshow_missing = true\nfail_under = 80\n</code></pre>"},{"location":"modules/00-environment-setup/exercises/exercise_0_5/#task-3-create-conftestpy","title":"Task 3: Create conftest.py","text":"<p>Create <code>tests/conftest.py</code> with shared fixtures:</p> <pre><code>\"\"\"Pytest fixtures for Airflow DAG testing.\"\"\"\n\nimport os\nimport sys\nfrom pathlib import Path\n\nimport pytest\n\n# Ensure the project root is in Python path\nPROJECT_ROOT = Path(__file__).parent.parent\nsys.path.insert(0, str(PROJECT_ROOT))\n\n# Set Airflow home before importing airflow\nos.environ.setdefault(\"AIRFLOW_HOME\", str(PROJECT_ROOT))\nos.environ.setdefault(\"AIRFLOW__CORE__UNIT_TEST_MODE\", \"True\")\n\n\n@pytest.fixture(scope=\"session\")\ndef project_root() -&gt; Path:\n    \"\"\"Return path to project root.\"\"\"\n    return PROJECT_ROOT\n\n\n@pytest.fixture(scope=\"session\")\ndef dags_folder(project_root: Path) -&gt; Path:\n    \"\"\"Return path to DAGs folder.\"\"\"\n    return project_root / \"dags\"\n\n\n@pytest.fixture(scope=\"session\")\ndef dag_files(dags_folder: Path) -&gt; list[Path]:\n    \"\"\"Return list of all DAG Python files.\"\"\"\n    if not dags_folder.exists():\n        return []\n    return [f for f in dags_folder.rglob(\"*.py\") if not f.name.startswith(\"_\") and f.name != \"__init__.py\"]\n\n\n@pytest.fixture(scope=\"session\")\ndef dag_bag():\n    \"\"\"Create a DagBag for testing.\"\"\"\n    from airflow.models import DagBag\n\n    return DagBag(include_examples=False)\n</code></pre>"},{"location":"modules/00-environment-setup/exercises/exercise_0_5/#task-4-create-dag-integrity-tests","title":"Task 4: Create DAG Integrity Tests","text":"<p>Create <code>tests/test_dag_integrity.py</code>:</p> <pre><code>\"\"\"Tests for DAG integrity and validity.\"\"\"\n\nimport importlib.util\nfrom pathlib import Path\n\nimport pytest\n\n\nclass TestDagImports:\n    \"\"\"Test that all DAG files can be imported.\"\"\"\n\n    def test_dags_folder_exists(self, dags_folder: Path) -&gt; None:\n        \"\"\"Verify DAGs folder exists.\"\"\"\n        assert dags_folder.exists(), f\"DAGs folder not found: {dags_folder}\"\n\n    def test_dag_files_found(self, dag_files: list[Path]) -&gt; None:\n        \"\"\"Verify at least one DAG file exists.\"\"\"\n        assert len(dag_files) &gt; 0, \"No DAG files found\"\n\n    def test_dags_can_be_imported(self, dag_files: list[Path]) -&gt; None:\n        \"\"\"Verify all DAG files can be imported without errors.\"\"\"\n        for dag_file in dag_files:\n            spec = importlib.util.spec_from_file_location(\n                dag_file.stem,\n                dag_file,\n            )\n            assert spec is not None, f\"Could not load spec for {dag_file}\"\n            assert spec.loader is not None, f\"No loader for {dag_file}\"\n\n            module = importlib.util.module_from_spec(spec)\n            try:\n                spec.loader.exec_module(module)\n            except Exception as e:\n                pytest.fail(f\"Failed to import {dag_file.name}: {e}\")\n\n\nclass TestDagBag:\n    \"\"\"Test DagBag integrity.\"\"\"\n\n    def test_no_import_errors(self, dag_bag) -&gt; None:\n        \"\"\"Check DAGs don't have import errors.\"\"\"\n        assert len(dag_bag.import_errors) == 0, \"DAG import errors found:\\n\" + \"\\n\".join(\n            f\"  {k}: {v}\" for k, v in dag_bag.import_errors.items()\n        )\n\n    def test_dags_loaded(self, dag_bag) -&gt; None:\n        \"\"\"Verify at least one DAG was loaded.\"\"\"\n        assert len(dag_bag.dags) &gt; 0, \"No DAGs loaded in DagBag\"\n\n    def test_no_cycles(self, dag_bag) -&gt; None:\n        \"\"\"Verify DAGs don't have circular dependencies.\"\"\"\n        for dag_id, dag in dag_bag.dags.items():\n            try:\n                # topological_sort raises if there are cycles\n                _ = dag.topological_sort()\n            except Exception as e:\n                pytest.fail(f\"DAG {dag_id} has circular dependencies: {e}\")\n\n\nclass TestDagConfiguration:\n    \"\"\"Test DAG configuration best practices.\"\"\"\n\n    def test_dag_ids_are_unique(self, dag_bag) -&gt; None:\n        \"\"\"Verify all DAG IDs are unique.\"\"\"\n        dag_ids = list(dag_bag.dags.keys())\n        assert len(dag_ids) == len(set(dag_ids)), f\"Duplicate DAG IDs found: {dag_ids}\"\n\n    def test_dags_have_tags(self, dag_bag) -&gt; None:\n        \"\"\"Verify all DAGs have at least one tag.\"\"\"\n        for dag_id, dag in dag_bag.dags.items():\n            assert dag.tags, f\"DAG {dag_id} has no tags\"\n\n    def test_dags_have_description_or_doc(self, dag_bag) -&gt; None:\n        \"\"\"Verify all DAGs have a description or docstring.\"\"\"\n        for dag_id, dag in dag_bag.dags.items():\n            has_description = bool(dag.description)\n            has_doc = bool(dag.doc_md)\n            assert has_description or has_doc, f\"DAG {dag_id} has no description or docstring\"\n\n    def test_catchup_is_disabled(self, dag_bag) -&gt; None:\n        \"\"\"Verify catchup is explicitly disabled for safety.\"\"\"\n        for dag_id, dag in dag_bag.dags.items():\n            assert dag.catchup is False, f\"DAG {dag_id} has catchup enabled. Set catchup=False unless intentional.\"\n</code></pre>"},{"location":"modules/00-environment-setup/exercises/exercise_0_5/#task-5-create-task-tests","title":"Task 5: Create Task Tests","text":"<p>Create <code>tests/test_dag_tasks.py</code>:</p> <pre><code>\"\"\"Tests for DAG task configurations.\"\"\"\n\n\nclass TestTaskDefaults:\n    \"\"\"Test task default configurations.\"\"\"\n\n    def test_tasks_have_owners(self, dag_bag) -&gt; None:\n        \"\"\"Verify all tasks have owners defined.\"\"\"\n        for dag_id, dag in dag_bag.dags.items():\n            for task in dag.tasks:\n                assert task.owner != \"airflow\", (\n                    f\"Task {dag_id}.{task.task_id} uses default owner. Set a specific owner in default_args.\"\n                )\n\n    def test_tasks_have_retries(self, dag_bag) -&gt; None:\n        \"\"\"Verify tasks have retry configuration.\"\"\"\n        for dag_id, dag in dag_bag.dags.items():\n            for task in dag.tasks:\n                # Allow 0 retries if explicitly set\n                assert task.retries is not None, f\"Task {dag_id}.{task.task_id} has no retries configured\"\n\n    def test_email_on_failure_configured(self, dag_bag) -&gt; None:\n        \"\"\"Verify email notifications are configured.\"\"\"\n        for dag_id, dag in dag_bag.dags.items():\n            for task in dag.tasks:\n                # This is a soft check - just verify it's explicitly set\n                _ = task.email_on_failure  # Should not raise\n\n\nclass TestTaskDependencies:\n    \"\"\"Test task dependency configurations.\"\"\"\n\n    def test_no_orphan_tasks(self, dag_bag) -&gt; None:\n        \"\"\"Verify no tasks are orphaned (no upstream or downstream).\"\"\"\n        for dag_id, dag in dag_bag.dags.items():\n            # Skip DAGs with only one task\n            if len(dag.tasks) &lt;= 1:\n                continue\n\n            for task in dag.tasks:\n                has_upstream = bool(task.upstream_list)\n                has_downstream = bool(task.downstream_list)\n                assert has_upstream or has_downstream, (\n                    f\"Task {dag_id}.{task.task_id} is orphaned (no upstream or downstream dependencies)\"\n                )\n\n    def test_reasonable_task_count(self, dag_bag) -&gt; None:\n        \"\"\"Verify DAGs don't have too many tasks.\"\"\"\n        max_tasks = 100  # Adjust based on your standards\n\n        for dag_id, dag in dag_bag.dags.items():\n            assert len(dag.tasks) &lt;= max_tasks, (\n                f\"DAG {dag_id} has {len(dag.tasks)} tasks (max: {max_tasks}). Consider splitting into sub-DAGs.\"\n            )\n</code></pre>"},{"location":"modules/00-environment-setup/exercises/exercise_0_5/#task-6-install-test-dependencies","title":"Task 6: Install Test Dependencies","text":"<pre><code>uv sync --group test\n</code></pre>"},{"location":"modules/00-environment-setup/exercises/exercise_0_5/#task-7-run-the-tests","title":"Task 7: Run the Tests","text":"<pre><code># Run all tests\nuv run pytest\n\n# Run with verbose output\nuv run pytest -v\n\n# Run a specific test file\nuv run pytest tests/test_dag_integrity.py -v\n\n# Run a specific test class\nuv run pytest tests/test_dag_integrity.py::TestDagImports -v\n</code></pre>"},{"location":"modules/00-environment-setup/exercises/exercise_0_5/#task-8-run-with-coverage","title":"Task 8: Run with Coverage","text":"<pre><code># Run tests with coverage\nuv run pytest --cov=dags --cov-report=term-missing\n\n# Generate HTML coverage report\nuv run pytest --cov=dags --cov-report=html\n\n# Open coverage report (macOS)\nopen htmlcov/index.html\n</code></pre>"},{"location":"modules/00-environment-setup/exercises/exercise_0_5/#task-9-add-tests-to-pre-commit-optional","title":"Task 9: Add Tests to Pre-commit (Optional)","text":"<p>Update <code>.pre-commit-config.yaml</code> to run tests:</p> <pre><code># Local hooks\n- repo: local\n  hooks:\n    - id: pytest\n      name: pytest\n      entry: uv run pytest\n      language: system\n      types: [python]\n      pass_filenames: false\n      always_run: true\n      stages: [pre-push] # Only run on push, not every commit\n</code></pre>"},{"location":"modules/00-environment-setup/exercises/exercise_0_5/#task-10-create-a-test-helper-script","title":"Task 10: Create a Test Helper Script","text":"<p>Create <code>scripts/test.sh</code>:</p> <pre><code>#!/bin/bash\n# scripts/test.sh - Run tests with common options\n\nset -e\n\n# Default to running all tests\nTEST_PATH=\"${1:-tests/}\"\n\necho \"\ud83e\uddea Running tests...\"\nuv run pytest \"$TEST_PATH\" \\\n    --cov=dags \\\n    --cov-report=term-missing \\\n    --cov-fail-under=80 \\\n    -v\n\necho \"\u2705 All tests passed!\"\n</code></pre> <p>Make it executable:</p> <pre><code>chmod +x scripts/test.sh\n</code></pre>"},{"location":"modules/00-environment-setup/exercises/exercise_0_5/#verification-checklist","title":"Verification Checklist","text":"<ul> <li> <code>tests/</code> directory exists with <code>__init__.py</code></li> <li> <code>tests/conftest.py</code> with fixtures</li> <li> <code>tests/test_dag_integrity.py</code> with import and structure tests</li> <li> <code>tests/test_dag_tasks.py</code> with task configuration tests</li> <li> <code>uv run pytest</code> runs without errors</li> <li> Coverage report generated with <code>--cov</code></li> </ul>"},{"location":"modules/00-environment-setup/exercises/exercise_0_5/#key-learnings","title":"\ud83d\udca1 Key Learnings","text":"<ol> <li>pytest fixtures share setup across tests efficiently</li> <li>DagBag is Airflow's mechanism for loading and validating DAGs</li> <li>Test categories: Import tests catch syntax errors, structure tests catch configuration issues</li> <li>Coverage thresholds ensure adequate test coverage</li> </ol>"},{"location":"modules/00-environment-setup/exercises/exercise_0_5/#common-edge-cases-and-how-tests-handle-them","title":"\u26a0\ufe0f Common Edge Cases and How Tests Handle Them","text":"<p>Understanding these edge cases helps you write more robust DAGs:</p> Edge Case What Happens Test That Catches It Empty dags/ folder DagBag loads nothing <code>test_dag_files_found</code> Syntax error in DAG Import fails with SyntaxError <code>test_dags_can_be_imported</code> Missing dependency import ImportError raised <code>test_dags_can_be_imported</code> Circular task dependency topological_sort() fails <code>test_no_cycles</code> Duplicate DAG IDs Only one DAG loads <code>test_dag_ids_are_unique</code> Missing tags DAG loads but hard to find in UI <code>test_dags_have_tags</code> catchup=True (default) Backfills on first run <code>test_catchup_is_disabled</code> Orphaned tasks Tasks never execute <code>test_no_orphan_tasks</code>"},{"location":"modules/00-environment-setup/exercises/exercise_0_5/#tests-you-should-always-have","title":"Tests You Should Always Have","text":"<p>Minimum Test Suite (must pass before deployment):</p> <ol> <li><code>test_dags_can_be_imported</code> - No syntax/import errors</li> <li><code>test_no_import_errors</code> - DagBag loads successfully</li> <li><code>test_no_cycles</code> - DAG structure is valid</li> </ol> <p>Recommended Additional Tests: 4. <code>test_dags_have_tags</code> - Organization in UI 5. <code>test_catchup_is_disabled</code> - Prevent accidental backfills 6. <code>test_tasks_have_owners</code> - Accountability</p>"},{"location":"modules/00-environment-setup/exercises/exercise_0_5/#test-types-for-airflow","title":"Test Types for Airflow","text":"Test Type Purpose Example Import tests Catch syntax/import errors <code>test_dags_can_be_imported</code> Structure tests Validate DAG configuration <code>test_no_cycles</code> Task tests Verify task configuration <code>test_tasks_have_retries</code> Unit tests Test individual functions Test custom operators Integration tests Test end-to-end flows <code>dag.test()</code>"},{"location":"modules/00-environment-setup/exercises/exercise_0_5/#bonus-challenge","title":"\ud83d\ude80 Bonus Challenge","text":"<ol> <li>Add a unit test for a custom operator or task function:</li> </ol> <pre><code># tests/test_tasks.py\ndef test_extract_returns_dict():\n    \"\"\"Test that extract task returns expected format.\"\"\"\n    from dags.sample_dag import extract\n\n    # Call the underlying function\n    result = extract.function()\n    assert isinstance(result, dict)\n    assert \"users\" in result\n</code></pre> <ol> <li>Add a marker for slow tests:</li> </ol> <pre><code>@pytest.mark.slow\ndef test_full_dag_execution(dag_bag):\n    \"\"\"Test full DAG execution (slow).\"\"\"\n    dag = dag_bag.get_dag(\"sample_dag\")\n    dag.test()\n</code></pre> <p>Run fast tests only:</p> <pre><code>uv run pytest -m \"not slow\"\n</code></pre> <ol> <li>Set up GitHub Actions for automated testing:    <pre><code># .github/workflows/test.yml\nname: Tests\non: [push, pull_request]\njobs:\n  test:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - uses: astral-sh/setup-uv@v4\n      - run: uv sync --all-groups\n      - run: uv run pytest --cov=dags\n</code></pre></li> </ol>"},{"location":"modules/00-environment-setup/exercises/exercise_0_5/#reference","title":"\ud83d\udcda Reference","text":"<ul> <li>pytest Documentation</li> <li>pytest-cov</li> <li>Airflow Testing Guide</li> </ul> <p>Congratulations! You've completed Module 00 and set up a professional Airflow development environment.</p> <p>Next: Module 01: Foundations \u2192</p> <p>_[DAG]: Directed Acyclic Graph - Airflow's core workflow abstraction _[XCom]: Cross-Communication - Airflow's mechanism for task-to-task data passing _[Executor]: The component that runs tasks (Local, Celery, Kubernetes, etc.) _[Scheduler]: The component that triggers DAGs and determines task execution order _[Webserver]: The component that serves the Airflow UI _[Provider]: An Airflow package that integrates with external systems _[TaskFlow]: The modern Python-native API for defining Airflow tasks using decorators _[Asset]: A logical representation of data that DAGs can produce or consume _[SDK]: Software Development Kit - the airflow.sdk module in Airflow 3.x _[K8s]: Kubernetes - container orchestration platform _[uv]: A fast Python package manager by Astral _[ruff]: A fast Python linter and formatter by Astral _[CI]: Continuous Integration _[CD]: Continuous Deployment *[PR]: Pull Request</p>"},{"location":"modules/01-foundations/","title":"Module 01: Foundations","text":""},{"location":"modules/01-foundations/#learning-objectives","title":"\ud83c\udfaf Learning Objectives","text":"<p>By the end of this module, you will: - Understand Airflow's core architecture and how components interact - Grasp the key differences between Airflow 2.x and Airflow 3 - Know what a DAG is and how it represents a workflow - Understand Tasks, Operators, and the execution model - Be able to write and run your first DAG</p>"},{"location":"modules/01-foundations/#estimated-time-3-4-hours","title":"\u23f1\ufe0f Estimated Time: 3-4 hours","text":""},{"location":"modules/01-foundations/#1-what-is-apache-airflow","title":"1. What is Apache Airflow?","text":"<p>Airflow is a workflow orchestration platform \u2014 it doesn't process data itself, but coordinates when and how data processing tasks run. Think of it as a conductor for your data orchestra.</p>"},{"location":"modules/01-foundations/#key-insight","title":"Key Insight","text":"<p>Airflow is about when and in what order, not what. The actual data processing happens in external systems (databases, Spark clusters, APIs, etc.)</p>"},{"location":"modules/01-foundations/#core-use-cases","title":"Core Use Cases","text":"<ul> <li>ETL/ELT pipelines</li> <li>ML model training and deployment</li> <li>Report generation and distribution</li> <li>System monitoring and alerting</li> <li>Data quality checks</li> <li>Cross-system data synchronization</li> </ul>"},{"location":"modules/01-foundations/#2-architecture-overview","title":"2. Architecture Overview","text":""},{"location":"modules/01-foundations/#airflow-3-component-model","title":"Airflow 3 Component Model","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                       USER INTERFACE                         \u2502\n\u2502                    (React-based Web UI)                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                        API SERVER                            \u2502\n\u2502              (FastAPI - serves UI + REST API)                \u2502\n\u2502                    airflow api-server                        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n              \u25bc               \u25bc               \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502    SCHEDULER    \u2502 \u2502  DAG PROCESSOR  \u2502 \u2502    TRIGGERER    \u2502\n\u2502 Decides when    \u2502 \u2502 Parses DAG      \u2502 \u2502 Handles async   \u2502\n\u2502 tasks run       \u2502 \u2502 files           \u2502 \u2502 sensors/deferra \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502\n         \u2502 Task Execution Interface (REST API)\n         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                         EXECUTOR                             \u2502\n\u2502   (KubernetesExecutor / CeleryExecutor / LocalExecutor)     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                         WORKERS                              \u2502\n\u2502              (Execute actual task code)                      \u2502\n\u2502              Uses Task SDK for communication                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    METADATA DATABASE                         \u2502\n\u2502              (PostgreSQL recommended)                        \u2502\n\u2502    Stores DAG definitions, run history, task states          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"modules/01-foundations/#what-changed-in-airflow-3","title":"\ud83c\udd95 What Changed in Airflow 3","text":"Component Airflow 2.x Airflow 3 Web Server <code>airflow webserver</code> <code>airflow api-server</code> DAG Parsing Part of scheduler Separate <code>airflow dag-processor</code> Worker DB Access Direct connection Via Task Execution API only API <code>/api/v1</code> (Flask) <code>/api/v2</code> (FastAPI) <p>Why this matters: Workers no longer need database credentials, improving security. The Task SDK handles all communication with the scheduler.</p>"},{"location":"modules/01-foundations/#3-core-concepts","title":"3. Core Concepts","text":""},{"location":"modules/01-foundations/#dag-directed-acyclic-graph","title":"DAG (Directed Acyclic Graph)","text":"<p>A DAG defines: - What tasks exist - Dependencies between tasks (which runs before which) - Schedule (when to run) - Configuration (retries, timeouts, etc.)</p> <pre><code>from airflow.sdk import DAG\nfrom datetime import datetime\n\nwith DAG(\n    dag_id=\"my_first_dag\",\n    start_date=datetime(2024, 1, 1),\n    schedule=\"@daily\",  # or cron: \"0 0 * * *\"\n    catchup=False,\n    tags=[\"example\", \"learning\"]\n):\n    # Tasks defined here\n    pass\n</code></pre> <p>Key Properties: - <code>dag_id</code>: Unique identifier (must be unique across all DAGs) - <code>start_date</code>: When the DAG becomes active - <code>schedule</code>: How often to run (None = manual only) - <code>catchup</code>: Whether to backfill missed runs</p>"},{"location":"modules/01-foundations/#tasks","title":"Tasks","text":"<p>A task is a single unit of work. In Airflow 3, tasks are defined using:</p> <ol> <li>TaskFlow API (recommended) \u2014 Python functions with <code>@task</code> decorator</li> <li>Operators \u2014 Pre-built task templates</li> </ol> <pre><code>from airflow.sdk import task\n\n@task\ndef extract_data():\n    \"\"\"This is a task defined with TaskFlow API\"\"\"\n    return {\"users\": 100, \"events\": 5000}\n\n@task\ndef transform_data(raw_data: dict):\n    \"\"\"Tasks can receive data from upstream tasks\"\"\"\n    return {\"processed_users\": raw_data[\"users\"] * 2}\n</code></pre>"},{"location":"modules/01-foundations/#dependencies","title":"Dependencies","text":"<p>Dependencies define execution order:</p> <pre><code># TaskFlow: Implicit dependencies via function calls\nresult = extract_data()\ntransformed = transform_data(result)\n\n# Explicit operators: Use &gt;&gt; or &lt;&lt;\ntask_a &gt;&gt; task_b &gt;&gt; task_c      # A then B then C\ntask_a &gt;&gt; [task_b, task_c]      # A then B and C in parallel\n[task_a, task_b] &gt;&gt; task_c      # A and B complete, then C\n</code></pre>"},{"location":"modules/01-foundations/#xcom-cross-communication","title":"XCom (Cross-Communication)","text":"<p>XComs allow tasks to share small amounts of data: - TaskFlow handles this automatically via return values - Operators use <code>xcom_push()</code> and <code>xcom_pull()</code></p> <p>\u26a0\ufe0f Warning: XComs are stored in the database. Keep them small (&lt; 48KB recommended).</p>"},{"location":"modules/01-foundations/#4-the-execution-model","title":"4. The Execution Model","text":""},{"location":"modules/01-foundations/#key-terms","title":"Key Terms","text":"Term Definition DAG Run A single execution of a DAG Task Instance A single execution of a task within a DAG Run Logical Date The scheduled time for a run (formerly <code>execution_date</code>) Data Interval The time period the run covers <code>[start, end)</code>"},{"location":"modules/01-foundations/#task-lifecycle","title":"Task Lifecycle","text":"<pre><code>none \u2192 scheduled \u2192 queued \u2192 running \u2192 success/failed/skipped\n                      \u2193\n              up_for_retry (if retries configured)\n</code></pre>"},{"location":"modules/01-foundations/#understanding-data-intervals","title":"Understanding Data Intervals","text":"<p>For a daily DAG with <code>start_date=2024-01-01</code>:</p> Run # Logical Date Data Interval Start Data Interval End 1 2024-01-01 2024-01-01 00:00 2024-01-02 00:00 2 2024-01-02 2024-01-02 00:00 2024-01-03 00:00 <p>Key Insight: The run with <code>logical_date=2024-01-01</code> actually runs at the END of that day (2024-01-02 00:00), processing data FROM 2024-01-01.</p>"},{"location":"modules/01-foundations/#5-your-first-dag","title":"5. Your First DAG","text":"<p>Let's create a complete, working DAG:</p> <pre><code>\"\"\"\nMy First Airflow 3 DAG\n\nThis DAG demonstrates:\n- Basic DAG structure\n- TaskFlow API\n- Task dependencies\n- XCom data passing\n\"\"\"\n\nfrom airflow.sdk import DAG, task\nfrom datetime import datetime, timedelta\n\n# Default arguments applied to all tasks\ndefault_args = {\n    \"owner\": \"learner\",\n    \"retries\": 1,\n    \"retry_delay\": timedelta(minutes=5),\n}\n\nwith DAG(\n    dag_id=\"01_hello_airflow\",\n    default_args=default_args,\n    description=\"My first Airflow 3 DAG\",\n    start_date=datetime(2024, 1, 1),\n    schedule=None,  # Manual trigger only\n    catchup=False,\n    tags=[\"module-01\", \"foundations\"],\n):\n\n    @task\n    def greet():\n        \"\"\"First task: Print a greeting\"\"\"\n        print(\"Hello from Airflow 3!\")\n        return \"Hello\"\n\n    @task\n    def get_current_time():\n        \"\"\"Second task: Get current timestamp\"\"\"\n        from datetime import datetime\n        now = datetime.now().isoformat()\n        print(f\"Current time: {now}\")\n        return now\n\n    @task\n    def combine_messages(greeting: str, timestamp: str):\n        \"\"\"Third task: Combine data from previous tasks\"\"\"\n        message = f\"{greeting}! The time is {timestamp}\"\n        print(f\"Combined message: {message}\")\n        return message\n\n    @task\n    def final_log(message: str):\n        \"\"\"Fourth task: Final logging\"\"\"\n        print(f\"Workflow complete. Final message: {message}\")\n\n    # Define the DAG structure\n    greeting = greet()\n    current_time = get_current_time()\n    combined = combine_messages(greeting, current_time)\n    final_log(combined)\n</code></pre>"},{"location":"modules/01-foundations/#running-your-dag","title":"Running Your DAG","text":"<ol> <li> <p>Copy to playground:    <pre><code>cp exercises/01_hello_airflow.py ../../dags/playground/\n</code></pre></p> </li> <li> <p>Verify it loads:    <pre><code>airflow dags list | grep 01_hello\n</code></pre></p> </li> <li> <p>Test without scheduling:    <pre><code>airflow dags test 01_hello_airflow 2024-01-01\n</code></pre></p> </li> <li> <p>Trigger via UI:</p> </li> <li>Open http://localhost:8080</li> <li>Find <code>01_hello_airflow</code></li> <li>Click the play button (\u25b6)</li> </ol>"},{"location":"modules/01-foundations/#6-airflow-3-import-patterns","title":"6. Airflow 3 Import Patterns","text":""},{"location":"modules/01-foundations/#critical-new-import-paths","title":"\u26a0\ufe0f Critical: New Import Paths","text":"<p>Airflow 3 uses the <code>airflow.sdk</code> namespace:</p> <pre><code># \u2705 Airflow 3 way\nfrom airflow.sdk import DAG, task\nfrom airflow.sdk import Asset\n\n# \u274c Old Airflow 2.x (still works but deprecated)\nfrom airflow import DAG\nfrom airflow.decorators import task\n</code></pre>"},{"location":"modules/01-foundations/#standard-provider-package","title":"Standard Provider Package","text":"<p>Common operators moved to <code>apache-airflow-providers-standard</code>:</p> <pre><code># Install first: pip install apache-airflow-providers-standard\n\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.providers.standard.operators.python import PythonOperator\nfrom airflow.providers.standard.sensors.filesystem import FileSensor\n</code></pre>"},{"location":"modules/01-foundations/#exercises","title":"\ud83d\udcdd Exercises","text":""},{"location":"modules/01-foundations/#exercise-11-conceptual-understanding","title":"Exercise 1.1: Conceptual Understanding","text":"<p>Answer these questions in your own words: 1. What's the difference between a DAG and a Task? 2. Why do workers in Airflow 3 not connect directly to the database? 3. What happens if you set <code>catchup=True</code> on a DAG with <code>start_date</code> from 6 months ago?</p>"},{"location":"modules/01-foundations/#exercise-12-build-a-simple-dag","title":"Exercise 1.2: Build a Simple DAG","text":"<p>Create a DAG that: - Has 4 tasks - Task A and Task B run in parallel - Task C depends on both A and B - Task D depends on C - Uses the TaskFlow API</p> <p>Save as <code>exercises/solutions/ex_1_2_parallel.py</code></p>"},{"location":"modules/01-foundations/#exercise-13-explore-the-ui","title":"Exercise 1.3: Explore the UI","text":"<p>With your local environment running: 1. Navigate to DAGs view 2. Find your DAG and trigger it manually 3. View the Graph view \u2014 understand the visual representation 4. Click on a task instance and view its logs 5. Explore the Grid view (new in Airflow 3)</p>"},{"location":"modules/01-foundations/#checkpoint","title":"\u2705 Checkpoint","text":"<p>Before moving to Module 02, ensure you can:</p> <ul> <li> Explain the role of each Airflow component</li> <li> Describe why Airflow 3's architecture is more secure</li> <li> Write a basic DAG with multiple tasks</li> <li> Understand task dependencies using <code>&gt;&gt;</code></li> <li> Run a DAG using <code>airflow dags test</code></li> <li> Navigate the Airflow UI</li> </ul>"},{"location":"modules/01-foundations/#further-reading","title":"\ud83d\udcda Further Reading","text":"<ul> <li>Airflow Concepts Documentation</li> <li>Upgrading to Airflow 3</li> <li>Architecture Overview</li> </ul> <p>Next: Module 02: TaskFlow API \u2192</p> <p>_[DAG]: Directed Acyclic Graph - Airflow's core workflow abstraction _[XCom]: Cross-Communication - Airflow's mechanism for task-to-task data passing _[Executor]: The component that runs tasks (Local, Celery, Kubernetes, etc.) _[Scheduler]: The component that triggers DAGs and determines task execution order _[Webserver]: The component that serves the Airflow UI _[Provider]: An Airflow package that integrates with external systems _[TaskFlow]: The modern Python-native API for defining Airflow tasks using decorators _[Asset]: A logical representation of data that DAGs can produce or consume _[SDK]: Software Development Kit - the airflow.sdk module in Airflow 3.x _[K8s]: Kubernetes - container orchestration platform _[uv]: A fast Python package manager by Astral _[ruff]: A fast Python linter and formatter by Astral _[CI]: Continuous Integration _[CD]: Continuous Deployment *[PR]: Pull Request</p>"},{"location":"modules/01-foundations/exercises/exercise_1_1/","title":"Exercise 1.1: Hello World DAG","text":""},{"location":"modules/01-foundations/exercises/exercise_1_1/#objective","title":"Objective","text":"<p>Create your first Airflow DAG that prints \"Hello, Airflow 3!\" to the logs.</p>"},{"location":"modules/01-foundations/exercises/exercise_1_1/#requirements","title":"Requirements","text":"<p>Your DAG should: 1. Be named <code>hello_airflow</code> 2. Have a start date of January 1, 2024 3. Not be scheduled (manual trigger only) 4. Contain a single task that prints the message</p>"},{"location":"modules/01-foundations/exercises/exercise_1_1/#starter-code","title":"Starter Code","text":"<p>Create a file <code>dags/playground/hello_airflow.py</code>:</p> <pre><code># TODO: Import DAG and task from airflow.sdk\n# Hint: from airflow.sdk import ...\n\n# TODO: Define your DAG using the context manager\n# Hint: with DAG(...) as dag:\n\n# TODO: Create a task using the @task decorator that prints \"Hello, Airflow 3!\"\n</code></pre>"},{"location":"modules/01-foundations/exercises/exercise_1_1/#verification","title":"Verification","text":"<ol> <li>After saving, wait ~30 seconds for the DAG to appear in the UI</li> <li>Toggle the DAG \"ON\" </li> <li>Trigger a manual run (play button)</li> <li>Check the task logs - you should see your message</li> </ol>"},{"location":"modules/01-foundations/exercises/exercise_1_1/#hints","title":"Hints","text":"Hint 1: Import statement <pre><code>from airflow.sdk import DAG, task\n</code></pre> Hint 2: DAG definition <pre><code>with DAG(\n    dag_id=\"hello_airflow\",\n    start_date=datetime(2024, 1, 1),\n    schedule=None,  # Manual trigger only\n):\n    # tasks go here\n</code></pre> Hint 3: Task decorator <pre><code>@task\ndef say_hello():\n    print(\"Hello, Airflow 3!\")\n\nsay_hello()  # Don't forget to call the task!\n</code></pre>"},{"location":"modules/01-foundations/exercises/exercise_1_1/#success-criteria","title":"Success Criteria","text":"<ul> <li> DAG appears in Airflow UI</li> <li> DAG runs successfully (green status)</li> <li> Log output shows \"Hello, Airflow 3!\"</li> </ul> <p>_[DAG]: Directed Acyclic Graph - Airflow's core workflow abstraction _[XCom]: Cross-Communication - Airflow's mechanism for task-to-task data passing _[Executor]: The component that runs tasks (Local, Celery, Kubernetes, etc.) _[Scheduler]: The component that triggers DAGs and determines task execution order _[Webserver]: The component that serves the Airflow UI _[Provider]: An Airflow package that integrates with external systems _[TaskFlow]: The modern Python-native API for defining Airflow tasks using decorators _[Asset]: A logical representation of data that DAGs can produce or consume _[SDK]: Software Development Kit - the airflow.sdk module in Airflow 3.x _[K8s]: Kubernetes - container orchestration platform _[uv]: A fast Python package manager by Astral _[ruff]: A fast Python linter and formatter by Astral _[CI]: Continuous Integration _[CD]: Continuous Deployment *[PR]: Pull Request</p>"},{"location":"modules/01-foundations/exercises/exercise_1_3/","title":"Exercise 1.3: Sequential Task Dependencies","text":""},{"location":"modules/01-foundations/exercises/exercise_1_3/#objective","title":"Objective","text":"<p>Create a DAG with three tasks that execute in sequence, simulating a simple ETL pipeline.</p>"},{"location":"modules/01-foundations/exercises/exercise_1_3/#requirements","title":"Requirements","text":"<p>Your DAG should: 1. Be named <code>simple_etl_pipeline</code> 2. Have three tasks: <code>extract</code>, <code>transform</code>, <code>load</code> 3. Tasks must run in order: extract \u2192 transform \u2192 load 4. Each task should print what it's doing and return some data</p>"},{"location":"modules/01-foundations/exercises/exercise_1_3/#starter-code","title":"Starter Code","text":"<pre><code>from datetime import datetime\nfrom airflow.sdk import DAG, task\n\n# TODO: Create three tasks using @task decorator\n# Each task should:\n# - Print what stage it's executing\n# - Return some data (extract returns raw data, transform returns processed, etc.)\n\n# TODO: Define task dependencies\n# Hint: Use the &gt;&gt; operator to set dependencies\n# Example: task1 &gt;&gt; task2 &gt;&gt; task3\n\nwith DAG(\n    dag_id=\"simple_etl_pipeline\",\n    start_date=datetime(2024, 1, 1),\n    schedule=None,\n    catchup=False,\n):\n    # Your code here\n    pass\n</code></pre>"},{"location":"modules/01-foundations/exercises/exercise_1_3/#expected-behavior","title":"Expected Behavior","text":"<p>When you run this DAG: 1. The Airflow UI should show 3 tasks in sequence (Graph view) 2. Tasks execute one after another 3. Each task log shows its print statement</p>"},{"location":"modules/01-foundations/exercises/exercise_1_3/#verification-steps","title":"Verification Steps","text":"<ol> <li>Open the DAG in the Airflow UI</li> <li>Go to Graph view - verify the dependencies look correct</li> <li>Trigger the DAG</li> <li>Verify tasks run in order (check timestamps in task details)</li> <li>Check logs for each task</li> </ol>"},{"location":"modules/01-foundations/exercises/exercise_1_3/#hints","title":"Hints","text":"Hint 1: Passing data between tasks  Tasks can return values and receive them as parameters:  <pre><code>@task\ndef extract():\n    data = [1, 2, 3]\n    print(f\"Extracted: {data}\")\n    return data\n\n@task  \ndef transform(raw_data):\n    result = [x * 2 for x in raw_data]\n    print(f\"Transformed: {result}\")\n    return result\n</code></pre> Hint 2: Setting dependencies with return values  When using TaskFlow, dependencies are implicit through data passing:  <pre><code>raw = extract()\ntransformed = transform(raw)  # This creates the dependency automatically!\nload(transformed)\n</code></pre>"},{"location":"modules/01-foundations/exercises/exercise_1_3/#bonus-challenge","title":"Bonus Challenge","text":"<p>After completing the basic exercise, try: 1. Add a fourth task that runs after <code>load</code> 2. Make <code>extract</code> return a dictionary with metadata 3. Add error handling with try/except in one task</p>"},{"location":"modules/01-foundations/exercises/exercise_1_3/#success-criteria","title":"Success Criteria","text":"<ul> <li> DAG has 3 tasks visible in UI</li> <li> Graph view shows correct sequential dependencies</li> <li> All tasks complete successfully</li> <li> Logs show data flowing between tasks</li> </ul> <p>_[DAG]: Directed Acyclic Graph - Airflow's core workflow abstraction _[XCom]: Cross-Communication - Airflow's mechanism for task-to-task data passing _[Executor]: The component that runs tasks (Local, Celery, Kubernetes, etc.) _[Scheduler]: The component that triggers DAGs and determines task execution order _[Webserver]: The component that serves the Airflow UI _[Provider]: An Airflow package that integrates with external systems _[TaskFlow]: The modern Python-native API for defining Airflow tasks using decorators _[Asset]: A logical representation of data that DAGs can produce or consume _[SDK]: Software Development Kit - the airflow.sdk module in Airflow 3.x _[K8s]: Kubernetes - container orchestration platform _[uv]: A fast Python package manager by Astral _[ruff]: A fast Python linter and formatter by Astral _[CI]: Continuous Integration _[CD]: Continuous Deployment *[PR]: Pull Request</p>"},{"location":"modules/02-taskflow-api/","title":"Module 02: TaskFlow API","text":""},{"location":"modules/02-taskflow-api/#learning-objectives","title":"\ud83c\udfaf Learning Objectives","text":"<p>By the end of this module, you will:</p> <ul> <li>Master the <code>@task</code> decorator and its parameters</li> <li>Understand automatic XCom handling with TaskFlow</li> <li>Use multiple outputs and complex return types</li> <li>Handle task dependencies elegantly</li> <li>Know when to use TaskFlow vs traditional Operators</li> </ul>"},{"location":"modules/02-taskflow-api/#estimated-time-4-5-hours","title":"\u23f1\ufe0f Estimated Time: 4-5 hours","text":""},{"location":"modules/02-taskflow-api/#1-the-taskflow-philosophy","title":"1. The TaskFlow Philosophy","text":"<p>TaskFlow is Airflow's Pythonic way of writing tasks. Instead of writing operators and managing XCom manually, you write Python functions that just work.</p>"},{"location":"modules/02-taskflow-api/#traditional-operator-approach-vs-taskflow","title":"Traditional Operator Approach vs TaskFlow","text":"<pre><code># \u274c Traditional (verbose, explicit XCom management)\ndef extract_func(**context):\n    data = {\"value\": 42}\n    context[\"ti\"].xcom_push(key=\"extracted_data\", value=data)\n\n\ndef transform_func(**context):\n    data = context[\"ti\"].xcom_pull(key=\"extracted_data\", task_ids=\"extract_task\")\n    result = data[\"value\"] * 2\n    context[\"ti\"].xcom_push(key=\"transformed_data\", value=result)\n\n\nextract_task = PythonOperator(task_id=\"extract_task\", python_callable=extract_func, provide_context=True)\ntransform_task = PythonOperator(task_id=\"transform_task\", python_callable=transform_func, provide_context=True)\nextract_task &gt;&gt; transform_task\n\n\n# \u2705 TaskFlow (clean, automatic XCom)\n@task\ndef extract():\n    return {\"value\": 42}\n\n\n@task\ndef transform(data: dict):\n    return data[\"value\"] * 2\n\n\n# Dependencies created by function calls\nresult = extract()\ntransformed = transform(result)  # Automatic dependency + XCom passing\n</code></pre>"},{"location":"modules/02-taskflow-api/#2-the-task-decorator-deep-dive","title":"2. The @task Decorator Deep Dive","text":""},{"location":"modules/02-taskflow-api/#basic-usage","title":"Basic Usage","text":"<pre><code>from datetime import datetime\n\nfrom airflow.sdk import DAG, task\n\nwith DAG(dag_id=\"taskflow_demo\", start_date=datetime(2024, 1, 1), schedule=None):\n\n    @task\n    def simple_task():\n        \"\"\"A basic task with no inputs or outputs\"\"\"\n        print(\"Executing simple task\")\n\n    @task\n    def task_with_return():\n        \"\"\"Return value automatically becomes XCom\"\"\"\n        return {\"status\": \"complete\", \"count\": 42}\n\n    @task\n    def task_with_input(data: dict):\n        \"\"\"Receives XCom from upstream task\"\"\"\n        print(f\"Received: {data}\")\n        return data[\"count\"] * 2\n\n    # Wire up\n    data = task_with_return()\n    result = task_with_input(data)\n</code></pre>"},{"location":"modules/02-taskflow-api/#task-decorator-parameters","title":"Task Decorator Parameters","text":"<pre><code>@task(\n    task_id=\"custom_task_id\",  # Override auto-generated ID\n    multiple_outputs=True,  # Each dict key becomes separate XCom\n    retries=3,  # Number of retry attempts\n    retry_delay=timedelta(minutes=5),  # Delay between retries\n    trigger_rule=\"all_success\",  # When to run (see Module 04)\n    pool=\"default_pool\",  # Resource pool\n    queue=\"default\",  # Queue for CeleryExecutor\n    execution_timeout=timedelta(hours=1),  # Max execution time\n    do_xcom_push=True,  # Push return value to XCom (default True)\n)\ndef my_configured_task():\n    pass\n</code></pre>"},{"location":"modules/02-taskflow-api/#3-xcom-patterns-with-taskflow","title":"3. XCom Patterns with TaskFlow","text":""},{"location":"modules/02-taskflow-api/#automatic-xcom-default-behavior","title":"Automatic XCom (Default Behavior)","text":"<pre><code>@task\ndef extract():\n    # This entire dict is stored as a single XCom value\n    return {\"users\": [1, 2, 3], \"metadata\": {\"source\": \"api\"}}\n\n\n@task\ndef process(data: dict):\n    # Access the full dict\n    users = data[\"users\"]\n    source = data[\"metadata\"][\"source\"]\n    return len(users)\n</code></pre>"},{"location":"modules/02-taskflow-api/#multiple-outputs","title":"Multiple Outputs","text":"<p>When you return a dictionary and want each key as a separate XCom:</p> <pre><code>@task(multiple_outputs=True)\ndef extract_multiple():\n    # Each key becomes a separate XCom entry\n    return {\"user_count\": 100, \"event_count\": 5000, \"timestamp\": \"2024-01-01T00:00:00\"}\n\n\n@task\ndef process_users(count: int):\n    print(f\"Processing {count} users\")\n\n\n@task\ndef process_events(count: int):\n    print(f\"Processing {count} events\")\n\n\n# Access individual outputs\nresult = extract_multiple()\nprocess_users(result[\"user_count\"])\nprocess_events(result[\"event_count\"])\n</code></pre>"},{"location":"modules/02-taskflow-api/#passing-large-data-anti-pattern-warning","title":"Passing Large Data (Anti-Pattern Warning)","text":"<pre><code># \u274c DON'T: Pass large data through XCom\n@task\ndef bad_practice():\n    large_dataframe = pd.read_csv(\"huge_file.csv\")  # 10GB file\n    return large_dataframe.to_dict()  # Will crash or slow down\n\n\n# \u2705 DO: Pass references, not data\n@task\ndef good_practice():\n    # Process and save to external storage\n    df = pd.read_csv(\"huge_file.csv\")\n    output_path = \"/data/processed/output.parquet\"\n    df.to_parquet(output_path)\n    # Only return the reference\n    return {\"path\": output_path, \"row_count\": len(df)}\n\n\n@task\ndef consume(metadata: dict):\n    # Load from path when needed\n    df = pd.read_parquet(metadata[\"path\"])\n</code></pre>"},{"location":"modules/02-taskflow-api/#4-context-access-in-taskflow","title":"4. Context Access in TaskFlow","text":"<p>Access Airflow runtime context within TaskFlow tasks:</p> <pre><code>from airflow.sdk import task\nfrom airflow.sdk.types import Context\n\n\n@task\ndef context_aware_task(**context: Context):\n    \"\"\"Access full Airflow context\"\"\"\n    # Common context variables\n    logical_date = context[\"logical_date\"]\n    dag_run = context[\"dag_run\"]\n    task_instance = context[\"ti\"]\n    params = context[\"params\"]\n\n    # Data interval (Airflow 3)\n    data_interval_start = context[\"data_interval_start\"]\n    data_interval_end = context[\"data_interval_end\"]\n\n    print(f\"Running for: {logical_date}\")\n    print(f\"Data interval: {data_interval_start} to {data_interval_end}\")\n\n    return {\"processed_date\": str(logical_date)}\n</code></pre>"},{"location":"modules/02-taskflow-api/#get-current-context-programmatically","title":"Get Current Context Programmatically","text":"<pre><code>from airflow.sdk import get_current_context, task\n\n\n@task\ndef task_with_context():\n    context = get_current_context()\n    ti = context[\"ti\"]\n\n    # You can also pull XCom explicitly if needed\n    upstream_data = ti.xcom_pull(task_ids=\"other_task\", key=\"custom_key\")\n\n    return {\"dag_run_id\": context[\"dag_run\"].run_id}\n</code></pre>"},{"location":"modules/02-taskflow-api/#5-task-dependencies-and-control-flow","title":"5. Task Dependencies and Control Flow","text":""},{"location":"modules/02-taskflow-api/#chaining-tasks","title":"Chaining Tasks","text":"<pre><code>@task\ndef a():\n    return 1\n\n\n@task\ndef b(x):\n    return x + 1\n\n\n@task\ndef c(x):\n    return x + 1\n\n\n@task\ndef d(x):\n    return x + 1\n\n\n# Method 1: Nested calls (creates linear chain)\nresult = d(c(b(a())))\n\n# Method 2: Intermediate variables (more readable)\na_result = a()\nb_result = b(a_result)\nc_result = c(b_result)\nd_result = d(c_result)\n</code></pre>"},{"location":"modules/02-taskflow-api/#parallel-execution","title":"Parallel Execution","text":"<pre><code>@task\ndef source_a():\n    return \"A\"\n\n\n@task\ndef source_b():\n    return \"B\"\n\n\n@task\ndef source_c():\n    return \"C\"\n\n\n@task\ndef combine(a, b, c):\n    return f\"{a}+{b}+{c}\"\n\n\n# a, b, c run in parallel; combine waits for all\nresult = combine(source_a(), source_b(), source_c())\n</code></pre>"},{"location":"modules/02-taskflow-api/#mixing-taskflow-and-operators","title":"Mixing TaskFlow and Operators","text":"<pre><code>from airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.sdk import DAG, task\n\nwith DAG(...):\n\n    @task\n    def prepare_config():\n        return {\"file\": \"/tmp/data.csv\"}\n\n    # Traditional operator\n    download = BashOperator(task_id=\"download_data\", bash_command=\"curl -o /tmp/data.csv https://example.com/data.csv\")\n\n    @task\n    def process_file(config: dict):\n        print(f\"Processing {config['file']}\")\n\n    # Mix dependencies\n    config = prepare_config()\n    config &gt;&gt; download  # TaskFlow output can set dependency on operator\n    download &gt;&gt; process_file(config)  # Or use explicit\n</code></pre>"},{"location":"modules/02-taskflow-api/#using-output-for-operator-xcom","title":"Using output() for Operator XCom","text":"<pre><code>from airflow.providers.standard.operators.bash import BashOperator\nfrom airflow.sdk import DAG, task\n\nwith DAG(...):\n    bash_task = BashOperator(task_id=\"generate_value\", bash_command=\"echo 'hello world'\")\n\n    @task\n    def consume_bash_output(value: str):\n        print(f\"Bash said: {value}\")\n\n    # Use .output to get the XCom reference\n    consume_bash_output(bash_task.output)\n</code></pre>"},{"location":"modules/02-taskflow-api/#6-best-practices","title":"6. Best Practices","text":""},{"location":"modules/02-taskflow-api/#do","title":"DO \u2705","text":"<pre><code># Keep tasks focused and single-purpose\n@task\ndef extract_users():\n    return fetch_from_api(\"/users\")\n\n\n@task\ndef extract_orders():\n    return fetch_from_api(\"/orders\")\n\n\n# Use type hints for clarity\n@task\ndef process(data: list[dict]) -&gt; dict:\n    return {\"count\": len(data)}\n\n\n# Use meaningful task IDs\n@task(task_id=\"validate_user_permissions\")\ndef validate():\n    pass\n</code></pre>"},{"location":"modules/02-taskflow-api/#dont","title":"DON'T \u274c","text":"<pre><code># Don't do everything in one task\n@task\ndef do_everything():\n    data = extract()\n    cleaned = transform(data)\n    load(cleaned)\n    send_notification()\n    # If any step fails, you restart from scratch\n\n\n# Don't pass large data through XCom\n@task\ndef bad_return():\n    return load_giant_dataframe().to_dict()\n\n\n# Don't use side effects for dependencies\n@task\ndef task_a():\n    write_to_file(\"/tmp/done.txt\")\n\n\n@task\ndef task_b():\n    # DON'T: Check file existence for dependency\n    while not os.path.exists(\"/tmp/done.txt\"):\n        time.sleep(1)\n</code></pre>"},{"location":"modules/02-taskflow-api/#exercises","title":"\ud83d\udcdd Exercises","text":""},{"location":"modules/02-taskflow-api/#exercise-21-etl-pipeline","title":"Exercise 2.1: ETL Pipeline","text":"<p>Build a TaskFlow ETL pipeline that:</p> <ol> <li>Extracts data from two mock sources (return dictionaries)</li> <li>Transforms each source separately</li> <li>Combines the transformed data</li> <li>Loads to a \"destination\" (just print)</li> </ol> <p>Requirements:</p> <ul> <li>Use <code>multiple_outputs=True</code> where appropriate</li> <li>Include proper type hints</li> <li>Add retry configuration to the extract tasks</li> </ul>"},{"location":"modules/02-taskflow-api/#exercise-22-context-usage","title":"Exercise 2.2: Context Usage","text":"<p>Create a DAG that:</p> <ol> <li>Accesses the <code>logical_date</code> and formats it</li> <li>Creates a filename based on the date</li> <li>Simulates writing to that file</li> <li>Returns metadata about the operation</li> </ol>"},{"location":"modules/02-taskflow-api/#exercise-23-mixed-operators-and-taskflow","title":"Exercise 2.3: Mixed Operators and TaskFlow","text":"<p>Create a DAG that:</p> <ol> <li>Uses a BashOperator to create a temp file</li> <li>Uses a TaskFlow task to read the file path from XCom</li> <li>Uses another TaskFlow task to \"process\" the file</li> </ol>"},{"location":"modules/02-taskflow-api/#checkpoint","title":"\u2705 Checkpoint","text":"<p>Before moving to Module 03, ensure you can:</p> <ul> <li> Write tasks using the <code>@task</code> decorator</li> <li> Return and consume data between tasks automatically</li> <li> Use <code>multiple_outputs=True</code> correctly</li> <li> Access Airflow context within TaskFlow tasks</li> <li> Mix TaskFlow tasks with traditional Operators</li> <li> Explain why large data shouldn't go through XCom</li> </ul>"},{"location":"modules/02-taskflow-api/#industry-spotlight-stripe","title":"\ud83c\udfed Industry Spotlight: Stripe","text":"<p>How Stripe Uses TaskFlow for Payment Processing</p> <p>Stripe processes billions of dollars in payments, requiring reliable, testable pipelines that handle complex financial data transformations. TaskFlow's Python-native approach provides significant advantages:</p> Challenge TaskFlow Solution Financial accuracy Type hints + return values make data contracts explicit Auditability XCom history provides complete data lineage Testing Pure Python functions are easily unit-tested Retry handling Built-in retry decorators handle transient failures <p>Pattern in Use: Stripe-style payment reconciliation with TaskFlow:</p> <pre><code>@task(retries=3, retry_delay=timedelta(minutes=1))\ndef extract_transactions(date: str) -&gt; list[dict]:\n    \"\"\"Extract with automatic retry on API failures.\"\"\"\n    return payment_api.get_transactions(date)\n\n\n@task\ndef validate_amounts(transactions: list[dict]) -&gt; dict:\n    \"\"\"Type-safe validation with clear data contracts.\"\"\"\n    total = sum(t[\"amount\"] for t in transactions)\n    return {\"count\": len(transactions), \"total\": total}\n</code></pre> <p>Key Insight: TaskFlow's automatic XCom handling eliminates manual serialization bugs that caused reconciliation issues in legacy systems.</p> <p>\ud83d\udcd6 Related Exercise: Exercise 2.4: LLM ETL Pipeline - Apply TaskFlow patterns to AI/ML workloads</p>"},{"location":"modules/02-taskflow-api/#further-reading","title":"\ud83d\udcda Further Reading","text":"<ul> <li>TaskFlow Tutorial</li> <li>XCom Documentation</li> <li>Best Practices</li> <li>Case Study: Stripe Fraud Detection</li> </ul> <p>Next: Module 03: Operators &amp; Hooks \u2192</p> <p>_[DAG]: Directed Acyclic Graph - Airflow's core workflow abstraction _[XCom]: Cross-Communication - Airflow's mechanism for task-to-task data passing _[Executor]: The component that runs tasks (Local, Celery, Kubernetes, etc.) _[Scheduler]: The component that triggers DAGs and determines task execution order _[Webserver]: The component that serves the Airflow UI _[Provider]: An Airflow package that integrates with external systems _[TaskFlow]: The modern Python-native API for defining Airflow tasks using decorators _[Asset]: A logical representation of data that DAGs can produce or consume _[SDK]: Software Development Kit - the airflow.sdk module in Airflow 3.x _[K8s]: Kubernetes - container orchestration platform _[uv]: A fast Python package manager by Astral _[ruff]: A fast Python linter and formatter by Astral _[CI]: Continuous Integration _[CD]: Continuous Deployment *[PR]: Pull Request</p>"},{"location":"modules/02-taskflow-api/exercises/exercise_2_1_etl_pipeline/","title":"Exercise 2.1: ETL Pipeline with TaskFlow","text":""},{"location":"modules/02-taskflow-api/exercises/exercise_2_1_etl_pipeline/#objective","title":"Objective","text":"<p>Build a complete ETL (Extract, Transform, Load) pipeline using the TaskFlow API that processes data from two different sources, transforms them separately, combines the results, and loads to a destination.</p>"},{"location":"modules/02-taskflow-api/exercises/exercise_2_1_etl_pipeline/#requirements","title":"Requirements","text":"<p>Your DAG should: 1. Be named <code>exercise_2_1_etl_pipeline</code> 2. Have a start date of January 1, 2024 3. Not be scheduled (manual trigger only) 4. Include proper tags: <code>[\"exercise\", \"module-02\", \"etl\"]</code></p>"},{"location":"modules/02-taskflow-api/exercises/exercise_2_1_etl_pipeline/#task-requirements","title":"Task Requirements","text":"<ol> <li>Extract Tasks (2 tasks):</li> <li><code>extract_users</code>: Returns mock user data as a dictionary</li> <li><code>extract_orders</code>: Returns mock order data as a dictionary</li> <li> <p>Both extract tasks should have <code>retries=2</code> and <code>retry_delay=timedelta(seconds=30)</code></p> </li> <li> <p>Transform Tasks (2 tasks):</p> </li> <li><code>transform_users</code>: Takes user data, adds a <code>processed</code> field</li> <li><code>transform_orders</code>: Takes order data, calculates order totals</li> <li> <p>Use <code>multiple_outputs=True</code> where appropriate</p> </li> <li> <p>Combine Task:</p> </li> <li><code>combine_data</code>: Merges the transformed user and order data</li> <li> <p>Returns a combined summary dictionary</p> </li> <li> <p>Load Task:</p> </li> <li><code>load_to_destination</code>: Receives combined data and prints it</li> <li>Returns metadata about the load operation</li> </ol>"},{"location":"modules/02-taskflow-api/exercises/exercise_2_1_etl_pipeline/#type-hints","title":"Type Hints","text":"<p>All tasks must include proper type hints for parameters and return values.</p>"},{"location":"modules/02-taskflow-api/exercises/exercise_2_1_etl_pipeline/#starter-code","title":"Starter Code","text":"<p>Create a file <code>dags/playground/exercise_2_1_etl_pipeline.py</code>:</p> <pre><code>\"\"\"\nExercise 2.1: ETL Pipeline with TaskFlow API\n============================================\nBuild a multi-source ETL pipeline using TaskFlow patterns.\n\"\"\"\n\nfrom datetime import datetime, timedelta\n# TODO: Import DAG and task from airflow.sdk\n\n\n# TODO: Define the DAG with the specified configuration\n# Hint: Use the @dag decorator or context manager\n\n\n    # TODO: Define extract_users task\n    # - Returns: {\"users\": [{\"id\": 1, \"name\": \"Alice\"}, {\"id\": 2, \"name\": \"Bob\"}]}\n    # - Add retries=2, retry_delay=timedelta(seconds=30)\n\n\n    # TODO: Define extract_orders task\n    # - Returns: {\"orders\": [{\"id\": 101, \"user_id\": 1, \"amount\": 99.99}, ...]}\n    # - Add retries=2, retry_delay=timedelta(seconds=30)\n\n\n    # TODO: Define transform_users task\n    # - Takes user data dict as input\n    # - Adds \"processed\": True to each user\n    # - Consider using multiple_outputs=True\n\n\n    # TODO: Define transform_orders task\n    # - Takes order data dict as input\n    # - Calculates total of all orders\n    # - Returns dict with orders and total\n\n\n    # TODO: Define combine_data task\n    # - Takes transformed users and orders\n    # - Creates a summary combining both\n\n\n    # TODO: Define load_to_destination task\n    # - Prints the combined data\n    # - Returns metadata about the operation\n\n\n    # TODO: Wire up the tasks\n    # users_raw = extract_users()\n    # orders_raw = extract_orders()\n    # ...\n\n\n# Don't forget to instantiate the DAG!\n</code></pre>"},{"location":"modules/02-taskflow-api/exercises/exercise_2_1_etl_pipeline/#verification","title":"Verification","text":"<ol> <li>Save the file and wait for Airflow to parse it (~30 seconds)</li> <li>Check the DAG appears in the UI without errors</li> <li>Trigger a manual run</li> <li>Verify in the Graph view:</li> <li><code>extract_users</code> and <code>extract_orders</code> run in parallel</li> <li>Both feed into their respective transform tasks</li> <li><code>combine_data</code> waits for both transforms</li> <li><code>load_to_destination</code> runs last</li> <li>Check the logs for each task to see the data flow</li> </ol>"},{"location":"modules/02-taskflow-api/exercises/exercise_2_1_etl_pipeline/#hints","title":"Hints","text":"Hint 1: Import statement <pre><code>from airflow.sdk import dag, task\n</code></pre> Hint 2: Using @dag decorator <pre><code>@dag(\n    dag_id=\"exercise_2_1_etl_pipeline\",\n    start_date=datetime(2024, 1, 1),\n    schedule=None,\n    catchup=False,\n    tags=[\"exercise\", \"module-02\", \"etl\"],\n)\ndef etl_pipeline():\n    # Define tasks inside\n    pass\n\netl_pipeline()  # Instantiate!\n</code></pre> Hint 3: Task with retries <pre><code>@task(retries=2, retry_delay=timedelta(seconds=30))\ndef extract_users() -&gt; dict:\n    return {\"users\": [...]}\n</code></pre> Hint 4: Multiple outputs pattern <pre><code>@task(multiple_outputs=True)\ndef transform_orders(data: dict) -&gt; dict:\n    orders = data[\"orders\"]\n    total = sum(o[\"amount\"] for o in orders)\n    return {\n        \"orders\": orders,\n        \"total\": total\n    }\n\n# Access individual outputs:\nresult = transform_orders(raw_data)\nresult[\"orders\"]  # Just orders\nresult[\"total\"]   # Just total\n</code></pre> Hint 5: Wiring parallel tasks <pre><code># These run in parallel (no dependency between them)\nusers_raw = extract_users()\norders_raw = extract_orders()\n\n# These wait for their respective upstream tasks\nusers_transformed = transform_users(users_raw)\norders_transformed = transform_orders(orders_raw)\n\n# This waits for both transforms\ncombined = combine_data(users_transformed, orders_transformed)\n</code></pre>"},{"location":"modules/02-taskflow-api/exercises/exercise_2_1_etl_pipeline/#success-criteria","title":"Success Criteria","text":"<ul> <li> DAG appears in UI without import errors</li> <li> DAG has correct tags and description</li> <li> Extract tasks have retry configuration</li> <li> Extract tasks run in parallel (check Graph view)</li> <li> Transform tasks use type hints</li> <li> Multiple outputs used where appropriate</li> <li> Load task prints the combined summary</li> <li> All tasks complete successfully (green)</li> </ul>"},{"location":"modules/02-taskflow-api/exercises/exercise_2_1_etl_pipeline/#expected-output","title":"Expected Output","text":"<p>In the <code>load_to_destination</code> task logs, you should see something like:</p> <pre><code>Loading data summary:\n{\n    \"user_count\": 2,\n    \"order_count\": 3,\n    \"total_revenue\": 299.97,\n    \"processed_at\": \"2024-01-01T00:00:00\"\n}\n</code></pre> <p>_[DAG]: Directed Acyclic Graph - Airflow's core workflow abstraction _[XCom]: Cross-Communication - Airflow's mechanism for task-to-task data passing _[Executor]: The component that runs tasks (Local, Celery, Kubernetes, etc.) _[Scheduler]: The component that triggers DAGs and determines task execution order _[Webserver]: The component that serves the Airflow UI _[Provider]: An Airflow package that integrates with external systems _[TaskFlow]: The modern Python-native API for defining Airflow tasks using decorators _[Asset]: A logical representation of data that DAGs can produce or consume _[SDK]: Software Development Kit - the airflow.sdk module in Airflow 3.x _[K8s]: Kubernetes - container orchestration platform _[uv]: A fast Python package manager by Astral _[ruff]: A fast Python linter and formatter by Astral _[CI]: Continuous Integration _[CD]: Continuous Deployment *[PR]: Pull Request</p>"},{"location":"modules/02-taskflow-api/exercises/exercise_2_2_context_usage/","title":"Exercise 2.2: Context Usage in TaskFlow","text":""},{"location":"modules/02-taskflow-api/exercises/exercise_2_2_context_usage/#objective","title":"Objective","text":"<p>Create a DAG that demonstrates how to access Airflow's runtime context within TaskFlow tasks, including logical dates, data intervals, and task instance information.</p>"},{"location":"modules/02-taskflow-api/exercises/exercise_2_2_context_usage/#requirements","title":"Requirements","text":"<p>Your DAG should: 1. Be named <code>exercise_2_2_context_usage</code> 2. Have a start date of January 1, 2024 3. Not be scheduled (manual trigger only) 4. Include tags: <code>[\"exercise\", \"module-02\", \"context\"]</code></p>"},{"location":"modules/02-taskflow-api/exercises/exercise_2_2_context_usage/#task-requirements","title":"Task Requirements","text":"<ol> <li>get_execution_info task:</li> <li>Access <code>logical_date</code> from context</li> <li>Access <code>data_interval_start</code> and <code>data_interval_end</code></li> <li>Format the logical_date as \"YYYY-MM-DD\"</li> <li> <p>Return execution metadata as a dictionary</p> </li> <li> <p>generate_filename task:</p> </li> <li>Takes the execution info from upstream</li> <li>Creates a filename pattern: <code>data_YYYY-MM-DD_HHmmss.csv</code></li> <li> <p>Returns the filename and full path</p> </li> <li> <p>simulate_file_write task:</p> </li> <li>Takes the file info from upstream</li> <li>Simulates writing data to the file (just print)</li> <li> <p>Returns metadata about the operation (bytes written, etc.)</p> </li> <li> <p>log_summary task:</p> </li> <li>Takes the write metadata</li> <li>Logs a summary of the entire operation</li> <li>Returns final status</li> </ol>"},{"location":"modules/02-taskflow-api/exercises/exercise_2_2_context_usage/#starter-code","title":"Starter Code","text":"<p>Create a file <code>dags/playground/exercise_2_2_context_usage.py</code>:</p> <pre><code>\"\"\"\nExercise 2.2: Context Usage in TaskFlow\n=======================================\nLearn to access Airflow context within TaskFlow tasks.\n\"\"\"\n\nfrom datetime import datetime\n# TODO: Import dag, task from airflow.sdk\n# TODO: Import get_current_context if needed\n\n\n# TODO: Define the DAG\ndef context_usage_dag():\n\n    # TODO: Create get_execution_info task\n    # Access context using **context parameter or get_current_context()\n    # Return: {\"logical_date\": \"...\", \"interval_start\": \"...\", \"interval_end\": \"...\"}\n\n\n    # TODO: Create generate_filename task\n    # Take execution_info dict as parameter\n    # Return: {\"filename\": \"data_2024-01-01_120000.csv\", \"path\": \"/data/output/...\"}\n\n\n    # TODO: Create simulate_file_write task\n    # Take file_info dict as parameter\n    # Print simulation message\n    # Return: {\"bytes_written\": 1024, \"rows\": 100, \"status\": \"simulated\"}\n\n\n    # TODO: Create log_summary task\n    # Take write_result dict as parameter\n    # Print summary\n    # Return: {\"overall_status\": \"success\", \"completed_at\": \"...\"}\n\n\n    # TODO: Wire up the tasks\n    pass\n\n\n# TODO: Instantiate the DAG\n</code></pre>"},{"location":"modules/02-taskflow-api/exercises/exercise_2_2_context_usage/#verification","title":"Verification","text":"<ol> <li>Save and wait for DAG to appear</li> <li>Trigger a manual run</li> <li>Check each task's logs:</li> <li><code>get_execution_info</code>: Should show the logical_date and intervals</li> <li><code>generate_filename</code>: Should show generated filename with date</li> <li><code>simulate_file_write</code>: Should show file write simulation</li> <li><code>log_summary</code>: Should show complete summary</li> </ol>"},{"location":"modules/02-taskflow-api/exercises/exercise_2_2_context_usage/#hints","title":"Hints","text":"Hint 1: Accessing context with **context <pre><code>from airflow.sdk import task\n\n@task\ndef my_task(**context):\n    logical_date = context[\"logical_date\"]\n    ti = context[\"ti\"]\n    dag_run = context[\"dag_run\"]\n    return {\"date\": str(logical_date)}\n</code></pre> Hint 2: Using get_current_context() <pre><code>from airflow.sdk import task, get_current_context\n\n@task\ndef my_task():\n    context = get_current_context()\n    logical_date = context[\"logical_date\"]\n    return {\"date\": str(logical_date)}\n</code></pre> Hint 3: Formatting dates <pre><code># logical_date is a pendulum DateTime object\nformatted = logical_date.strftime(\"%Y-%m-%d\")\nfilename_date = logical_date.strftime(\"%Y-%m-%d_%H%M%S\")\n</code></pre> Hint 4: Data intervals <pre><code>@task\ndef show_intervals(**context):\n    # Data intervals define the period this run covers\n    start = context[\"data_interval_start\"]\n    end = context[\"data_interval_end\"]\n\n    print(f\"This run covers: {start} to {end}\")\n    return {\n        \"interval_start\": str(start),\n        \"interval_end\": str(end),\n    }\n</code></pre>"},{"location":"modules/02-taskflow-api/exercises/exercise_2_2_context_usage/#success-criteria","title":"Success Criteria","text":"<ul> <li> DAG runs without errors</li> <li> get_execution_info correctly extracts context values</li> <li> Filename includes the logical_date formatted correctly</li> <li> All tasks pass data through XCom properly</li> <li> Final summary includes all operation metadata</li> </ul>"},{"location":"modules/02-taskflow-api/exercises/exercise_2_2_context_usage/#context-variables-reference","title":"Context Variables Reference","text":"Variable Description <code>logical_date</code> The logical date/time for this DAG run <code>data_interval_start</code> Start of the data interval <code>data_interval_end</code> End of the data interval <code>dag_run</code> The DagRun object <code>ti</code> TaskInstance object <code>params</code> DAG parameters <code>run_id</code> Unique run identifier <p>_[DAG]: Directed Acyclic Graph - Airflow's core workflow abstraction _[XCom]: Cross-Communication - Airflow's mechanism for task-to-task data passing _[Executor]: The component that runs tasks (Local, Celery, Kubernetes, etc.) _[Scheduler]: The component that triggers DAGs and determines task execution order _[Webserver]: The component that serves the Airflow UI _[Provider]: An Airflow package that integrates with external systems _[TaskFlow]: The modern Python-native API for defining Airflow tasks using decorators _[Asset]: A logical representation of data that DAGs can produce or consume _[SDK]: Software Development Kit - the airflow.sdk module in Airflow 3.x _[K8s]: Kubernetes - container orchestration platform _[uv]: A fast Python package manager by Astral _[ruff]: A fast Python linter and formatter by Astral _[CI]: Continuous Integration _[CD]: Continuous Deployment *[PR]: Pull Request</p>"},{"location":"modules/02-taskflow-api/exercises/exercise_2_3_mixed_operators/","title":"Exercise 2.3: Mixed Operators and TaskFlow","text":""},{"location":"modules/02-taskflow-api/exercises/exercise_2_3_mixed_operators/#objective","title":"Objective","text":"<p>Create a DAG that demonstrates how to seamlessly mix traditional Airflow operators with TaskFlow tasks, using XCom to pass data between them.</p>"},{"location":"modules/02-taskflow-api/exercises/exercise_2_3_mixed_operators/#requirements","title":"Requirements","text":"<p>Your DAG should: 1. Be named <code>exercise_2_3_mixed_operators</code> 2. Have a start date of January 1, 2024 3. Not be scheduled (manual trigger only) 4. Include tags: <code>[\"exercise\", \"module-02\", \"mixed\"]</code></p>"},{"location":"modules/02-taskflow-api/exercises/exercise_2_3_mixed_operators/#task-requirements","title":"Task Requirements","text":"<ol> <li>BashOperator Task (<code>create_temp_file</code>):</li> <li>Creates a temporary file with some content</li> <li>Echoes the file path (this becomes the XCom return value)</li> <li> <p>Use <code>do_xcom_push=True</code> (default for BashOperator with echo)</p> </li> <li> <p>TaskFlow Task (<code>read_file_info</code>):</p> </li> <li>Receives the file path from the BashOperator via XCom</li> <li>Uses <code>.output</code> to get the XCom reference</li> <li> <p>Returns file metadata dictionary</p> </li> <li> <p>TaskFlow Task (<code>process_file</code>):</p> </li> <li>Receives the file metadata</li> <li>Simulates processing the file</li> <li> <p>Returns processing results</p> </li> <li> <p>PythonOperator Task (<code>cleanup</code>):</p> </li> <li>Uses traditional PythonOperator (not TaskFlow)</li> <li>Shows how to access XCom from previous tasks</li> <li>Simulates cleanup operations</li> </ol>"},{"location":"modules/02-taskflow-api/exercises/exercise_2_3_mixed_operators/#starter-code","title":"Starter Code","text":"<p>Create a file <code>dags/playground/exercise_2_3_mixed_operators.py</code>:</p> <pre><code>\"\"\"\nExercise 2.3: Mixed Operators and TaskFlow\n==========================================\nLearn to combine traditional operators with TaskFlow tasks.\n\"\"\"\n\nfrom datetime import datetime\n# TODO: Import necessary components\n# from airflow.sdk import DAG, task\n# from airflow.providers.standard.operators.bash import BashOperator\n# from airflow.providers.standard.operators.python import PythonOperator\n\n\n# TODO: Define a function for the PythonOperator (cleanup)\n# def cleanup_function(**context):\n#     # Access XCom from previous task\n#     ti = context['ti']\n#     ...\n\n\n# TODO: Define the DAG using context manager\n# with DAG(...) as dag:\n\n    # TODO: Create BashOperator to create temp file\n    # Bash command should:\n    # 1. Create a temp file: /tmp/airflow_exercise_XXXXX\n    # 2. Write some data to it\n    # 3. Echo the file path (last echo is the XCom value)\n\n    # TODO: Create TaskFlow task to read file info\n    # Use the .output attribute to get XCom from BashOperator\n\n    # TODO: Create TaskFlow task to process file\n\n    # TODO: Create PythonOperator for cleanup\n\n    # TODO: Set up dependencies\n</code></pre>"},{"location":"modules/02-taskflow-api/exercises/exercise_2_3_mixed_operators/#verification","title":"Verification","text":"<ol> <li>Run the DAG and check each task's logs</li> <li>Verify the file path passes from Bash \u2192 TaskFlow correctly</li> <li>Check that the PythonOperator can access upstream XCom</li> <li>Confirm cleanup task runs last</li> </ol>"},{"location":"modules/02-taskflow-api/exercises/exercise_2_3_mixed_operators/#hints","title":"Hints","text":"Hint 1: BashOperator XCom <pre><code># BashOperator returns the last line of stdout as XCom\ncreate_file = BashOperator(\n    task_id=\"create_temp_file\",\n    bash_command=\"\"\"\n        TEMP_FILE=$(mktemp /tmp/airflow_exercise_XXXXXX)\n        echo \"Sample data: $(date)\" &gt; $TEMP_FILE\n        echo $TEMP_FILE\n    \"\"\",\n)\n\n# The file path is now in create_file.output\n</code></pre> Hint 2: TaskFlow consuming operator output <pre><code>@task\ndef read_file_info(file_path: str) -&gt; dict:\n    print(f\"Received file path: {file_path}\")\n    return {\"path\": file_path, \"exists\": True}\n\n# Use .output to get the XCom reference\nfile_info = read_file_info(create_file.output)\n</code></pre> Hint 3: PythonOperator accessing XCom <pre><code>def cleanup_function(**context):\n    ti = context['ti']\n    # Pull XCom from specific task\n    file_path = ti.xcom_pull(task_ids='create_temp_file')\n    print(f\"Cleaning up: {file_path}\")\n\ncleanup = PythonOperator(\n    task_id=\"cleanup\",\n    python_callable=cleanup_function,\n)\n</code></pre> Hint 4: Setting dependencies <pre><code># With TaskFlow, dependencies are set by data flow\n# But you can also use &gt;&gt; for explicit ordering\n\n# TaskFlow creates deps automatically:\nfile_info = read_file_info(create_file.output)  # create_file &gt;&gt; read_file_info\nresult = process_file(file_info)                 # read_file_info &gt;&gt; process_file\n\n# For PythonOperator, use explicit dep:\nresult &gt;&gt; cleanup  # or: cleanup.set_upstream(result)\n</code></pre>"},{"location":"modules/02-taskflow-api/exercises/exercise_2_3_mixed_operators/#success-criteria","title":"Success Criteria","text":"<ul> <li> BashOperator creates a temp file and outputs its path</li> <li> TaskFlow task receives the file path via XCom</li> <li> Data flows correctly through all tasks</li> <li> PythonOperator successfully accesses XCom</li> <li> All tasks show green (success) status</li> </ul>"},{"location":"modules/02-taskflow-api/exercises/exercise_2_3_mixed_operators/#key-concepts","title":"Key Concepts","text":"Concept Description <code>.output</code> Property on operators that provides XCom reference <code>ti.xcom_pull()</code> Method to explicitly pull XCom in traditional operators Mixed deps TaskFlow creates deps from function calls; operators use <code>&gt;&gt;</code> XCom bridge <code>.output</code> bridges traditional operators to TaskFlow <p>_[DAG]: Directed Acyclic Graph - Airflow's core workflow abstraction _[XCom]: Cross-Communication - Airflow's mechanism for task-to-task data passing _[Executor]: The component that runs tasks (Local, Celery, Kubernetes, etc.) _[Scheduler]: The component that triggers DAGs and determines task execution order _[Webserver]: The component that serves the Airflow UI _[Provider]: An Airflow package that integrates with external systems _[TaskFlow]: The modern Python-native API for defining Airflow tasks using decorators _[Asset]: A logical representation of data that DAGs can produce or consume _[SDK]: Software Development Kit - the airflow.sdk module in Airflow 3.x _[K8s]: Kubernetes - container orchestration platform _[uv]: A fast Python package manager by Astral _[ruff]: A fast Python linter and formatter by Astral _[CI]: Continuous Integration _[CD]: Continuous Deployment *[PR]: Pull Request</p>"},{"location":"modules/02-taskflow-api/exercises/exercise_2_4_llm_etl/","title":"Exercise 2.4: LLM-Powered Text Extraction Pipeline","text":"<p>Build an ETL pipeline that uses LLM APIs to extract structured information from unstructured text, demonstrating XCom for passing complex data and context for API key management.</p>"},{"location":"modules/02-taskflow-api/exercises/exercise_2_4_llm_etl/#learning-goals","title":"Learning Goals","text":"<ul> <li>Pass complex data structures (embeddings, extracted entities) via XCom</li> <li>Use Airflow context to securely access API credentials</li> <li>Implement TaskFlow patterns for LLM-powered workflows</li> <li>Handle LLM response parsing and error recovery</li> </ul>"},{"location":"modules/02-taskflow-api/exercises/exercise_2_4_llm_etl/#scenario","title":"Scenario","text":"<p>You're building a pipeline that processes customer feedback documents. The pipeline should:</p> <ol> <li>Extract: Load raw feedback text from a source</li> <li>Transform: Use an LLM to extract structured entities (sentiment, topics, action items)</li> <li>Load: Store the extracted data in a structured format</li> </ol> <p>This exercise reinforces TaskFlow concepts while introducing AI/ML patterns.</p>"},{"location":"modules/02-taskflow-api/exercises/exercise_2_4_llm_etl/#requirements","title":"Requirements","text":""},{"location":"modules/02-taskflow-api/exercises/exercise_2_4_llm_etl/#task-1-secure-api-key-access","title":"Task 1: Secure API Key Access","text":"<p>Create a task that demonstrates secure credential access:</p> <ul> <li>Use <code>Variable.get()</code> for API keys (mock for testing)</li> <li>Access connection info from Airflow context</li> <li>Never log or expose sensitive credentials</li> </ul>"},{"location":"modules/02-taskflow-api/exercises/exercise_2_4_llm_etl/#task-2-llm-text-extraction","title":"Task 2: LLM Text Extraction","text":"<p>Create a TaskFlow task that:</p> <ul> <li>Accepts raw text as input (passed via XCom)</li> <li>Calls an LLM API (mock implementation provided)</li> <li>Returns structured extraction results</li> <li>Handles parsing errors gracefully</li> </ul>"},{"location":"modules/02-taskflow-api/exercises/exercise_2_4_llm_etl/#task-3-complex-xcom-data","title":"Task 3: Complex XCom Data","text":"<p>Demonstrate passing complex structures:</p> <ul> <li>Pass extraction results (dicts with nested data)</li> <li>Handle list outputs from dynamic operations</li> <li>Validate data structure between tasks</li> </ul>"},{"location":"modules/02-taskflow-api/exercises/exercise_2_4_llm_etl/#task-4-context-aware-processing","title":"Task 4: Context-Aware Processing","text":"<p>Use Airflow context effectively:</p> <ul> <li>Access <code>task_instance</code> for XCom operations</li> <li>Use <code>dag_run</code> configuration for runtime parameters</li> <li>Log processing metadata with context info</li> </ul>"},{"location":"modules/02-taskflow-api/exercises/exercise_2_4_llm_etl/#success-criteria","title":"Success Criteria","text":"<ul> <li> API keys are never exposed in logs</li> <li> Complex data structures pass correctly between tasks</li> <li> LLM extraction returns structured results</li> <li> Pipeline handles extraction failures gracefully</li> <li> Context is used appropriately for metadata</li> </ul>"},{"location":"modules/02-taskflow-api/exercises/exercise_2_4_llm_etl/#hints","title":"Hints","text":"Hint 1: Secure Variable Access <pre><code>from airflow import Variable\n\n\ndef get_api_key() -&gt; str:\n    \"\"\"Securely retrieve API key.\"\"\"\n    # Use Variable for sensitive data - never hardcode\n    api_key = Variable.get(\"OPENAI_API_KEY\", default_var=None)\n\n    if api_key is None:\n        # Fallback for testing without real API\n        return \"mock-api-key-for-testing\"\n\n    return api_key\n</code></pre> Hint 2: Complex XCom Data <pre><code>@task\ndef extract_entities(text: str) -&gt; dict[str, Any]:\n    \"\"\"Extract entities - returns complex nested dict.\"\"\"\n    result = {\n        \"sentiment\": {\"label\": \"positive\", \"score\": 0.85},\n        \"topics\": [\"product\", \"service\"],\n        \"entities\": [\n            {\"type\": \"PRODUCT\", \"text\": \"Widget Pro\", \"confidence\": 0.92},\n            {\"type\": \"PERSON\", \"text\": \"John\", \"confidence\": 0.88},\n        ],\n        \"action_items\": [\n            {\"priority\": \"high\", \"description\": \"Follow up with customer\"},\n        ],\n    }\n    return result  # Automatically serialized to XCom\n\n\n@task\ndef process_extraction(extraction: dict[str, Any]) -&gt; dict[str, Any]:\n    \"\"\"Process extraction result passed via XCom.\"\"\"\n    # XCom automatically deserializes the dict\n    sentiment = extraction[\"sentiment\"][\"label\"]\n    topic_count = len(extraction[\"topics\"])\n\n    return {\n        \"summary\": f\"Found {topic_count} topics with {sentiment} sentiment\",\n        \"requires_followup\": len(extraction[\"action_items\"]) &gt; 0,\n    }\n</code></pre> Hint 3: Using Airflow Context <pre><code>from airflow.sdk import task\n\n\n@task\ndef process_with_context(**context) -&gt; dict[str, Any]:\n    \"\"\"Access Airflow context in TaskFlow.\"\"\"\n    # Access task instance\n    ti = context[\"task_instance\"]\n\n    # Get DAG run configuration\n    dag_conf = context[\"dag_run\"].conf or {}\n    custom_param = dag_conf.get(\"extraction_model\", \"default\")\n\n    # Access execution date\n    execution_date = context[\"logical_date\"]\n\n    # Log with context (don't log sensitive data!)\n    logger.info(f\"Processing run from {execution_date}\")\n\n    return {\n        \"model_used\": custom_param,\n        \"run_id\": context[\"run_id\"],\n    }\n</code></pre> Hint 4: Error Handling in LLM Tasks <pre><code>import json\n\n\n@task\ndef safe_llm_extraction(text: str) -&gt; dict[str, Any]:\n    \"\"\"LLM extraction with error handling.\"\"\"\n    try:\n        # Call LLM (mock or real)\n        response = call_llm(text)\n\n        # Parse JSON response\n        try:\n            result = json.loads(response)\n        except json.JSONDecodeError:\n            # Fallback for non-JSON responses\n            result = {\"raw_response\": response, \"parse_error\": True}\n\n        return result\n\n    except Exception as e:\n        # Return error structure instead of failing\n        return {\n            \"error\": str(e),\n            \"input_text\": text[:100],  # Truncate for safety\n            \"success\": False,\n        }\n</code></pre>"},{"location":"modules/02-taskflow-api/exercises/exercise_2_4_llm_etl/#files","title":"Files","text":"<ul> <li>Starter: <code>exercise_2_4_llm_etl_starter.py</code></li> <li>Solution: <code>../solutions/solution_2_4_llm_etl.py</code></li> </ul>"},{"location":"modules/02-taskflow-api/exercises/exercise_2_4_llm_etl/#estimated-time","title":"Estimated Time","text":"<p>45-60 minutes</p>"},{"location":"modules/02-taskflow-api/exercises/exercise_2_4_llm_etl/#next-steps","title":"Next Steps","text":"<p>After completing this exercise:</p> <ol> <li>Try with a real LLM API (OpenAI, Anthropic)</li> <li>Add retry logic for API failures</li> <li>Implement batching for multiple documents</li> <li>See Module 15 for advanced LLM orchestration patterns</li> </ol> <p>_[DAG]: Directed Acyclic Graph - Airflow's core workflow abstraction _[XCom]: Cross-Communication - Airflow's mechanism for task-to-task data passing _[Executor]: The component that runs tasks (Local, Celery, Kubernetes, etc.) _[Scheduler]: The component that triggers DAGs and determines task execution order _[Webserver]: The component that serves the Airflow UI _[Provider]: An Airflow package that integrates with external systems _[TaskFlow]: The modern Python-native API for defining Airflow tasks using decorators _[Asset]: A logical representation of data that DAGs can produce or consume _[SDK]: Software Development Kit - the airflow.sdk module in Airflow 3.x _[K8s]: Kubernetes - container orchestration platform _[uv]: A fast Python package manager by Astral _[ruff]: A fast Python linter and formatter by Astral _[CI]: Continuous Integration _[CD]: Continuous Deployment *[PR]: Pull Request</p>"},{"location":"modules/03-operators-hooks/","title":"Module 03: Operators &amp; Hooks","text":""},{"location":"modules/03-operators-hooks/#learning-objectives","title":"\ud83c\udfaf Learning Objectives","text":"<p>By the end of this module, you will: - Understand the Operator/Hook architecture - Use built-in operators from <code>apache-airflow-providers-standard</code> - Know when to use operators vs TaskFlow - Create custom operators for reusable logic - Understand Hooks and connection management</p>"},{"location":"modules/03-operators-hooks/#estimated-time-4-5-hours","title":"\u23f1\ufe0f Estimated Time: 4-5 hours","text":""},{"location":"modules/03-operators-hooks/#1-operators-vs-taskflow-when-to-use-which","title":"1. Operators vs TaskFlow: When to Use Which","text":"Use Case Recommended Approach Custom Python logic TaskFlow <code>@task</code> Bash commands <code>BashOperator</code> SQL queries Database-specific operators Cloud services (S3, GCS, BigQuery) Provider operators Reusable logic across DAGs Custom Operator Simple data transformation TaskFlow External API calls TaskFlow or HTTP operators <p>Rule of thumb: Use TaskFlow for Python logic, operators for external system interactions.</p>"},{"location":"modules/03-operators-hooks/#2-standard-operators-apache-airflow-providers-standard","title":"2. Standard Operators (apache-airflow-providers-standard)","text":""},{"location":"modules/03-operators-hooks/#installation","title":"Installation","text":"<pre><code>pip install apache-airflow-providers-standard\n</code></pre>"},{"location":"modules/03-operators-hooks/#bashoperator","title":"BashOperator","text":"<pre><code>from airflow.providers.standard.operators.bash import BashOperator\n\n# Simple command\nprint_date = BashOperator(\n    task_id=\"print_date\",\n    bash_command=\"date\",\n)\n\n# With environment variables\nrun_script = BashOperator(\n    task_id=\"run_script\",\n    bash_command=\"python /scripts/process.py \",\n    env={\"DATA_PATH\": \"/data\", \"MODE\": \"production\"},\n)\n\n# With templating (Jinja2)\ntemplated_command = BashOperator(\n    task_id=\"templated\",\n    bash_command=\"\"\"\n        echo \"Processing for {{ ds }}\"\n        echo \"Logical date: {{ logical_date }}\"\n    \"\"\",\n)\n\n# Capture output via XCom\ncapture_output = BashOperator(\n    task_id=\"capture_output\",\n    bash_command=\"echo 'Hello World'\",\n    do_xcom_push=True,  # Last line of stdout becomes XCom\n)\n</code></pre>"},{"location":"modules/03-operators-hooks/#pythonoperator","title":"PythonOperator","text":"<p>When you can't use TaskFlow (rare cases):</p> <pre><code>from airflow.providers.standard.operators.python import PythonOperator\n\ndef my_function(param1, param2, **context):\n    print(f\"Params: {param1}, {param2}\")\n    print(f\"Logical date: {context['logical_date']}\")\n    return \"result\"\n\npython_task = PythonOperator(\n    task_id=\"python_task\",\n    python_callable=my_function,\n    op_kwargs={\"param1\": \"value1\", \"param2\": \"value2\"},\n)\n</code></pre>"},{"location":"modules/03-operators-hooks/#emptyoperator-formerly-dummyoperator","title":"EmptyOperator (formerly DummyOperator)","text":"<p>For control flow and grouping:</p> <pre><code>from airflow.providers.standard.operators.empty import EmptyOperator\n\nstart = EmptyOperator(task_id=\"start\")\nend = EmptyOperator(task_id=\"end\")\n\n# Use for fan-in/fan-out patterns\nstart &gt;&gt; [task_a, task_b, task_c] &gt;&gt; end\n</code></pre>"},{"location":"modules/03-operators-hooks/#branchpythonoperator","title":"BranchPythonOperator","text":"<p>For conditional execution:</p> <pre><code>from airflow.providers.standard.operators.python import BranchPythonOperator\n\ndef choose_branch(**context):\n    if context[\"logical_date\"].weekday() &lt; 5:\n        return \"weekday_task\"\n    return \"weekend_task\"\n\nbranch = BranchPythonOperator(\n    task_id=\"branch\",\n    python_callable=choose_branch,\n)\n\nbranch &gt;&gt; [weekday_task, weekend_task]\n</code></pre>"},{"location":"modules/03-operators-hooks/#3-common-provider-operators","title":"3. Common Provider Operators","text":""},{"location":"modules/03-operators-hooks/#http-operator","title":"HTTP Operator","text":"<pre><code>from airflow.providers.http.operators.http import HttpOperator\n\ncall_api = HttpOperator(\n    task_id=\"call_api\",\n    http_conn_id=\"my_api\",  # Configured in Admin &gt; Connections\n    endpoint=\"/users\",\n    method=\"GET\",\n    headers={\"Content-Type\": \"application/json\"},\n    response_check=lambda response: response.status_code == 200,\n)\n</code></pre>"},{"location":"modules/03-operators-hooks/#postgres-operator","title":"Postgres Operator","text":"<pre><code>from airflow.providers.postgres.operators.postgres import PostgresOperator\n\ncreate_table = PostgresOperator(\n    task_id=\"create_table\",\n    postgres_conn_id=\"my_postgres\",\n    sql=\"\"\"\n        CREATE TABLE IF NOT EXISTS users (\n            id SERIAL PRIMARY KEY,\n            name VARCHAR(100)\n        );\n    \"\"\",\n)\n\n# Or use SQL files\nrun_query = PostgresOperator(\n    task_id=\"run_query\",\n    postgres_conn_id=\"my_postgres\",\n    sql=\"sql/transform_users.sql\",  # Path relative to DAG folder\n)\n</code></pre>"},{"location":"modules/03-operators-hooks/#s3-operators","title":"S3 Operators","text":"<pre><code>from airflow.providers.amazon.aws.operators.s3 import (\n    S3CreateBucketOperator,\n    S3DeleteObjectsOperator,\n    S3ListOperator,\n)\nfrom airflow.providers.amazon.aws.transfers.local_to_s3 import (\n    LocalFilesystemToS3Operator,\n)\n\nupload_file = LocalFilesystemToS3Operator(\n    task_id=\"upload_to_s3\",\n    filename=\"/local/path/file.csv\",\n    dest_key=\"data/file.csv\",\n    dest_bucket=\"my-bucket\",\n    aws_conn_id=\"aws_default\",\n    replace=True,\n)\n</code></pre>"},{"location":"modules/03-operators-hooks/#4-understanding-hooks","title":"4. Understanding Hooks","text":"<p>Hooks are the interface to external systems. Operators use hooks under the hood.</p> <pre><code>Operator (what to do) \n    \u2514\u2500\u2500 Hook (how to connect)\n            \u2514\u2500\u2500 Connection (credentials)\n</code></pre>"},{"location":"modules/03-operators-hooks/#using-hooks-directly-in-taskflow","title":"Using Hooks Directly in TaskFlow","text":"<pre><code>from airflow.sdk import task\nfrom airflow.providers.postgres.hooks.postgres import PostgresHook\n\n@task\ndef query_database():\n    hook = PostgresHook(postgres_conn_id=\"my_postgres\")\n\n    # Get records\n    records = hook.get_records(\"SELECT * FROM users LIMIT 10\")\n\n    # Get pandas DataFrame\n    df = hook.get_pandas_df(\"SELECT * FROM users\")\n\n    # Execute command\n    hook.run(\"UPDATE users SET active = true WHERE id = 1\")\n\n    return {\"row_count\": len(records)}\n</code></pre>"},{"location":"modules/03-operators-hooks/#common-hooks","title":"Common Hooks","text":"Hook Use Case <code>PostgresHook</code> PostgreSQL database <code>MySqlHook</code> MySQL database <code>S3Hook</code> AWS S3 <code>GCSHook</code> Google Cloud Storage <code>HttpHook</code> REST APIs <code>SlackHook</code> Slack messaging"},{"location":"modules/03-operators-hooks/#5-connections-management","title":"5. Connections Management","text":"<p>Connections store credentials and are referenced by <code>conn_id</code>.</p>"},{"location":"modules/03-operators-hooks/#creating-connections","title":"Creating Connections","text":"<p>Via UI: Admin \u2192 Connections \u2192 Add</p> <p>Via CLI: <pre><code>airflow connections add 'my_postgres' \\\n    --conn-type 'postgres' \\\n    --conn-host 'localhost' \\\n    --conn-schema 'mydb' \\\n    --conn-login 'user' \\\n    --conn-password 'pass' \\\n    --conn-port 5432\n</code></pre></p> <p>Via Environment Variable: <pre><code>export AIRFLOW_CONN_MY_POSTGRES='postgresql://user:pass@localhost:5432/mydb'\n</code></pre></p>"},{"location":"modules/03-operators-hooks/#connection-string-format","title":"Connection String Format","text":"<pre><code>&lt;conn-type&gt;://&lt;login&gt;:&lt;password&gt;@&lt;host&gt;:&lt;port&gt;/&lt;schema&gt;?param1=value1\n</code></pre>"},{"location":"modules/03-operators-hooks/#6-building-custom-operators","title":"6. Building Custom Operators","text":"<p>For reusable, tested logic across DAGs:</p> <pre><code>from airflow.models import BaseOperator\nfrom airflow.providers.http.hooks.http import HttpHook\nfrom typing import Any\n\nclass FetchAndStoreOperator(BaseOperator):\n    \"\"\"\n    Fetches data from an API and stores it in S3.\n\n    :param http_conn_id: Connection ID for the API\n    :param endpoint: API endpoint to call\n    :param s3_bucket: Target S3 bucket\n    :param s3_key: Target S3 key\n    \"\"\"\n\n    # Fields to template (Jinja2)\n    template_fields = (\"endpoint\", \"s3_key\")\n\n    def __init__(\n        self,\n        http_conn_id: str,\n        endpoint: str,\n        s3_bucket: str,\n        s3_key: str,\n        **kwargs\n    ):\n        super().__init__(**kwargs)\n        self.http_conn_id = http_conn_id\n        self.endpoint = endpoint\n        self.s3_bucket = s3_bucket\n        self.s3_key = s3_key\n\n    def execute(self, context: Any):\n        \"\"\"Main execution method\"\"\"\n        # Fetch from API\n        http_hook = HttpHook(http_conn_id=self.http_conn_id)\n        response = http_hook.run(self.endpoint)\n        data = response.json()\n\n        # Store to S3\n        from airflow.providers.amazon.aws.hooks.s3 import S3Hook\n        s3_hook = S3Hook()\n        s3_hook.load_string(\n            string_data=json.dumps(data),\n            key=self.s3_key,\n            bucket_name=self.s3_bucket,\n            replace=True,\n        )\n\n        self.log.info(f\"Stored {len(data)} records to s3://{self.s3_bucket}/{self.s3_key}\")\n\n        return {\"records\": len(data), \"location\": f\"s3://{self.s3_bucket}/{self.s3_key}\"}\n</code></pre>"},{"location":"modules/03-operators-hooks/#using-custom-operators","title":"Using Custom Operators","text":"<pre><code>from my_operators import FetchAndStoreOperator\n\nfetch_users = FetchAndStoreOperator(\n    task_id=\"fetch_users\",\n    http_conn_id=\"my_api\",\n    endpoint=\"/users\",\n    s3_bucket=\"my-data-lake\",\n    s3_key=\"raw/users/{{ ds }}/data.json\",  # Templated!\n)\n</code></pre>"},{"location":"modules/03-operators-hooks/#7-sensors-waiting-for-conditions","title":"7. Sensors: Waiting for Conditions","text":"<p>Sensors are operators that wait for something to happen:</p> <pre><code>from airflow.providers.standard.sensors.filesystem import FileSensor\nfrom airflow.providers.amazon.aws.sensors.s3 import S3KeySensor\n\n# Wait for local file\nwait_for_file = FileSensor(\n    task_id=\"wait_for_file\",\n    filepath=\"/data/incoming/daily_export.csv\",\n    poke_interval=60,  # Check every 60 seconds\n    timeout=3600,      # Give up after 1 hour\n    mode=\"poke\",       # or \"reschedule\" to free up worker\n)\n\n# Wait for S3 object\nwait_for_s3 = S3KeySensor(\n    task_id=\"wait_for_s3\",\n    bucket_name=\"upstream-bucket\",\n    bucket_key=\"exports/{{ ds }}/complete.flag\",\n    aws_conn_id=\"aws_default\",\n    mode=\"reschedule\",  # Recommended for long waits\n)\n</code></pre>"},{"location":"modules/03-operators-hooks/#sensor-modes","title":"Sensor Modes","text":"Mode Behavior Use When <code>poke</code> Keeps worker slot occupied Short waits (&lt; 5 min) <code>reschedule</code> Releases worker, reschedules Long waits (&gt; 5 min)"},{"location":"modules/03-operators-hooks/#exercises","title":"\ud83d\udcdd Exercises","text":""},{"location":"modules/03-operators-hooks/#exercise-31-operator-exploration","title":"Exercise 3.1: Operator Exploration","text":"<p>Create a DAG that uses: - BashOperator to create a temp file with random data - PythonOperator to read and process the file - BranchPythonOperator to branch based on data content - Two EmptyOperators as endpoints for each branch</p>"},{"location":"modules/03-operators-hooks/#exercise-32-custom-operator","title":"Exercise 3.2: Custom Operator","text":"<p>Create a custom <code>DataValidationOperator</code> that: - Takes a file path and validation rules as parameters - Reads the file and validates each rule - Returns validation results as XCom - Logs validation failures</p>"},{"location":"modules/03-operators-hooks/#exercise-33-hook-usage","title":"Exercise 3.3: Hook Usage","text":"<p>Create a TaskFlow DAG that: - Uses HttpHook to fetch data from a public API - Processes the data - Uses a FileSystemHook (or simple file write) to store results</p>"},{"location":"modules/03-operators-hooks/#checkpoint","title":"\u2705 Checkpoint","text":"<p>Before moving to Module 04, ensure you can:</p> <ul> <li> Choose between TaskFlow and Operators appropriately</li> <li> Use BashOperator, PythonOperator, and BranchPythonOperator</li> <li> Configure and use Connections</li> <li> Use Hooks in TaskFlow tasks</li> <li> Create a basic custom Operator</li> <li> Understand Sensor modes (poke vs reschedule)</li> </ul> <p>Next: Module 04: Scheduling &amp; Triggers \u2192</p> <p>_[DAG]: Directed Acyclic Graph - Airflow's core workflow abstraction _[XCom]: Cross-Communication - Airflow's mechanism for task-to-task data passing _[Executor]: The component that runs tasks (Local, Celery, Kubernetes, etc.) _[Scheduler]: The component that triggers DAGs and determines task execution order _[Webserver]: The component that serves the Airflow UI _[Provider]: An Airflow package that integrates with external systems _[TaskFlow]: The modern Python-native API for defining Airflow tasks using decorators _[Asset]: A logical representation of data that DAGs can produce or consume _[SDK]: Software Development Kit - the airflow.sdk module in Airflow 3.x _[K8s]: Kubernetes - container orchestration platform _[uv]: A fast Python package manager by Astral _[ruff]: A fast Python linter and formatter by Astral _[CI]: Continuous Integration _[CD]: Continuous Deployment *[PR]: Pull Request</p>"},{"location":"modules/03-operators-hooks/exercises/exercise_3_1_operator_exploration/","title":"Exercise 3.1: Operator Exploration","text":""},{"location":"modules/03-operators-hooks/exercises/exercise_3_1_operator_exploration/#objective","title":"Objective","text":"<p>Create a DAG that demonstrates multiple operator types working together, including conditional branching based on data content.</p>"},{"location":"modules/03-operators-hooks/exercises/exercise_3_1_operator_exploration/#requirements","title":"Requirements","text":"<p>Your DAG should: 1. Be named <code>exercise_3_1_operator_exploration</code> 2. Have a start date of January 1, 2024 3. Not be scheduled (manual trigger only) 4. Include tags: <code>[\"exercise\", \"module-03\", \"operators\"]</code></p>"},{"location":"modules/03-operators-hooks/exercises/exercise_3_1_operator_exploration/#task-requirements","title":"Task Requirements","text":"<ol> <li>BashOperator (<code>generate_data</code>):</li> <li>Creates a temp file</li> <li>Writes random data (use <code>$RANDOM</code> or similar)</li> <li>Include a \"status\" line that's either \"VALID\" or \"INVALID\" randomly</li> <li> <p>Echo the file path for downstream tasks</p> </li> <li> <p>PythonOperator (<code>analyze_data</code>):</p> </li> <li>Reads the file path from XCom</li> <li>Reads file content (simulated or actual)</li> <li>Determines if status is VALID or INVALID</li> <li> <p>Returns the status string via XCom</p> </li> <li> <p>BranchPythonOperator (<code>decide_branch</code>):</p> </li> <li>Pulls the status from <code>analyze_data</code></li> <li>Returns task_id \"process_valid\" if VALID</li> <li> <p>Returns task_id \"handle_invalid\" if INVALID</p> </li> <li> <p>EmptyOperator (<code>process_valid</code>):</p> </li> <li> <p>Endpoint for valid data path</p> </li> <li> <p>EmptyOperator (<code>handle_invalid</code>):</p> </li> <li> <p>Endpoint for invalid data path</p> </li> <li> <p>EmptyOperator (<code>final_task</code>):</p> </li> <li>Runs after either branch (use trigger_rule)</li> </ol>"},{"location":"modules/03-operators-hooks/exercises/exercise_3_1_operator_exploration/#starter-code","title":"Starter Code","text":"<p>Create a file <code>dags/playground/exercise_3_1_operator_exploration.py</code>:</p> <pre><code>\"\"\"\nExercise 3.1: Operator Exploration\n==================================\nExplore different operator types and branching patterns.\n\"\"\"\n\nfrom datetime import datetime\n# TODO: Import necessary operators\n# from airflow.sdk import DAG\n# from airflow.providers.standard.operators.bash import BashOperator\n# from airflow.providers.standard.operators.python import PythonOperator, BranchPythonOperator\n# from airflow.providers.standard.operators.empty import EmptyOperator\n\n\n# TODO: Define Python callables for PythonOperator and BranchPythonOperator\n# def analyze_data_func(**context):\n#     pass\n\n# def decide_branch_func(**context):\n#     pass\n\n\n# TODO: Create the DAG\n# with DAG(...) as dag:\n    # Create tasks and set dependencies\n</code></pre>"},{"location":"modules/03-operators-hooks/exercises/exercise_3_1_operator_exploration/#hints","title":"Hints","text":"Hint 1: Bash random value <pre><code># Generate random status in bash\nif [ $((RANDOM % 2)) -eq 0 ]; then\n    STATUS=\"VALID\"\nelse\n    STATUS=\"INVALID\"\nfi\necho \"STATUS=$STATUS\" &gt; $TEMP_FILE\n</code></pre> Hint 2: BranchPythonOperator return <pre><code>def decide_branch_func(**context):\n    ti = context['ti']\n    status = ti.xcom_pull(task_ids='analyze_data')\n\n    if status == \"VALID\":\n        return \"process_valid\"  # Return task_id as string\n    else:\n        return \"handle_invalid\"\n</code></pre> Hint 3: Final task trigger rule <pre><code># Use trigger_rule to run after ANY branch completes\nfinal_task = EmptyOperator(\n    task_id=\"final_task\",\n    trigger_rule=\"none_failed_min_one_success\",\n)\n</code></pre>"},{"location":"modules/03-operators-hooks/exercises/exercise_3_1_operator_exploration/#success-criteria","title":"Success Criteria","text":"<ul> <li> BashOperator creates file with random status</li> <li> PythonOperator correctly reads and analyzes data</li> <li> Branch correctly routes based on status</li> <li> Only one branch executes per run</li> <li> Final task runs regardless of which branch was taken</li> </ul> <p>_[DAG]: Directed Acyclic Graph - Airflow's core workflow abstraction _[XCom]: Cross-Communication - Airflow's mechanism for task-to-task data passing _[Executor]: The component that runs tasks (Local, Celery, Kubernetes, etc.) _[Scheduler]: The component that triggers DAGs and determines task execution order _[Webserver]: The component that serves the Airflow UI _[Provider]: An Airflow package that integrates with external systems _[TaskFlow]: The modern Python-native API for defining Airflow tasks using decorators _[Asset]: A logical representation of data that DAGs can produce or consume _[SDK]: Software Development Kit - the airflow.sdk module in Airflow 3.x _[K8s]: Kubernetes - container orchestration platform _[uv]: A fast Python package manager by Astral _[ruff]: A fast Python linter and formatter by Astral _[CI]: Continuous Integration _[CD]: Continuous Deployment *[PR]: Pull Request</p>"},{"location":"modules/03-operators-hooks/exercises/exercise_3_2_custom_operator/","title":"Exercise 3.2: Custom Operator","text":""},{"location":"modules/03-operators-hooks/exercises/exercise_3_2_custom_operator/#objective","title":"Objective","text":"<p>Create a custom <code>DataValidationOperator</code> that validates data files against a set of rules, demonstrating how to build reusable operator classes.</p>"},{"location":"modules/03-operators-hooks/exercises/exercise_3_2_custom_operator/#requirements","title":"Requirements","text":"<p>Your custom operator should: 1. Inherit from <code>BaseOperator</code> 2. Accept parameters for file path and validation rules 3. Execute validation logic in the <code>execute()</code> method 4. Return validation results via XCom 5. Log validation failures appropriately</p>"},{"location":"modules/03-operators-hooks/exercises/exercise_3_2_custom_operator/#datavalidationoperator-specifications","title":"DataValidationOperator Specifications","text":"<pre><code>class DataValidationOperator(BaseOperator):\n    \"\"\"\n    Validates data against specified rules.\n\n    Parameters:\n        file_path: Path to the data file (templatable)\n        rules: List of validation rule dictionaries\n        fail_on_error: Whether to raise exception on validation failure\n    \"\"\"\n</code></pre>"},{"location":"modules/03-operators-hooks/exercises/exercise_3_2_custom_operator/#validation-rules-format","title":"Validation Rules Format","text":"<pre><code>rules = [\n    {\"field\": \"email\", \"type\": \"regex\", \"pattern\": r\".*@.*\\..*\"},\n    {\"field\": \"age\", \"type\": \"range\", \"min\": 0, \"max\": 150},\n    {\"field\": \"status\", \"type\": \"allowed_values\", \"values\": [\"active\", \"inactive\"]},\n]\n</code></pre>"},{"location":"modules/03-operators-hooks/exercises/exercise_3_2_custom_operator/#starter-code","title":"Starter Code","text":"<p>Create a file <code>dags/playground/exercise_3_2_custom_operator.py</code>:</p> <pre><code>\"\"\"\nExercise 3.2: Custom Operator\n=============================\nCreate a reusable DataValidationOperator.\n\"\"\"\n\nfrom datetime import datetime\n# TODO: Import necessary modules\n# from airflow.sdk import DAG\n# from airflow.models import BaseOperator\n\n\n# TODO: Create the DataValidationOperator class\n# class DataValidationOperator(BaseOperator):\n#     # Define template_fields for Jinja templating\n#     template_fields = (\"file_path\",)\n#\n#     def __init__(self, file_path, rules, fail_on_error=False, **kwargs):\n#         super().__init__(**kwargs)\n#         # Store parameters\n#\n#     def execute(self, context):\n#         # Implement validation logic\n#         pass\n\n\n# TODO: Create a DAG that uses your custom operator\n</code></pre>"},{"location":"modules/03-operators-hooks/exercises/exercise_3_2_custom_operator/#hints","title":"Hints","text":"Hint 1: BaseOperator structure <pre><code>from airflow.models import BaseOperator\n\nclass DataValidationOperator(BaseOperator):\n    template_fields = (\"file_path\",)  # Enable Jinja templating\n\n    def __init__(self, file_path, rules, fail_on_error=False, **kwargs):\n        super().__init__(**kwargs)\n        self.file_path = file_path\n        self.rules = rules\n        self.fail_on_error = fail_on_error\n\n    def execute(self, context):\n        # Your validation logic here\n        return validation_results\n</code></pre> Hint 2: Validation logic <pre><code>def _validate_rule(self, data, rule):\n    field = rule[\"field\"]\n    value = data.get(field)\n\n    if rule[\"type\"] == \"regex\":\n        import re\n        return bool(re.match(rule[\"pattern\"], str(value)))\n    elif rule[\"type\"] == \"range\":\n        return rule[\"min\"] &lt;= value &lt;= rule[\"max\"]\n    elif rule[\"type\"] == \"allowed_values\":\n        return value in rule[\"values\"]\n</code></pre> Hint 3: Using the operator in a DAG <pre><code>validate_users = DataValidationOperator(\n    task_id=\"validate_users\",\n    file_path=\"/data/users.json\",\n    rules=[\n        {\"field\": \"email\", \"type\": \"regex\", \"pattern\": r\".+@.+\\..+\"},\n        {\"field\": \"age\", \"type\": \"range\", \"min\": 18, \"max\": 120},\n    ],\n    fail_on_error=False,\n)\n</code></pre>"},{"location":"modules/03-operators-hooks/exercises/exercise_3_2_custom_operator/#success-criteria","title":"Success Criteria","text":"<ul> <li> Custom operator inherits from BaseOperator correctly</li> <li> Constructor accepts all required parameters</li> <li> template_fields enables Jinja templating for file_path</li> <li> execute() returns validation results dictionary</li> <li> Validation logic handles multiple rule types</li> <li> Logging shows validation progress and failures</li> <li> Operator can be used in a DAG without errors</li> </ul> <p>_[DAG]: Directed Acyclic Graph - Airflow's core workflow abstraction _[XCom]: Cross-Communication - Airflow's mechanism for task-to-task data passing _[Executor]: The component that runs tasks (Local, Celery, Kubernetes, etc.) _[Scheduler]: The component that triggers DAGs and determines task execution order _[Webserver]: The component that serves the Airflow UI _[Provider]: An Airflow package that integrates with external systems _[TaskFlow]: The modern Python-native API for defining Airflow tasks using decorators _[Asset]: A logical representation of data that DAGs can produce or consume _[SDK]: Software Development Kit - the airflow.sdk module in Airflow 3.x _[K8s]: Kubernetes - container orchestration platform _[uv]: A fast Python package manager by Astral _[ruff]: A fast Python linter and formatter by Astral _[CI]: Continuous Integration _[CD]: Continuous Deployment *[PR]: Pull Request</p>"},{"location":"modules/03-operators-hooks/exercises/exercise_3_3_hook_usage/","title":"Exercise 3.3: Hook Usage","text":""},{"location":"modules/03-operators-hooks/exercises/exercise_3_3_hook_usage/#objective","title":"Objective","text":"<p>Create a TaskFlow DAG that demonstrates using Hooks to interact with external systems, specifically fetching data from a public API and processing the results.</p>"},{"location":"modules/03-operators-hooks/exercises/exercise_3_3_hook_usage/#requirements","title":"Requirements","text":"<p>Your DAG should: 1. Be named <code>exercise_3_3_hook_usage</code> 2. Have a start date of January 1, 2024 3. Not be scheduled (manual trigger only) 4. Include tags: <code>[\"exercise\", \"module-03\", \"hooks\"]</code></p>"},{"location":"modules/03-operators-hooks/exercises/exercise_3_3_hook_usage/#task-requirements","title":"Task Requirements","text":"<ol> <li>fetch_data task:</li> <li>Uses HttpHook (or requests library) to fetch data from a public API</li> <li>Suggested API: JSONPlaceholder (https://jsonplaceholder.typicode.com)</li> <li>Fetch posts or users data</li> <li> <p>Return the JSON response</p> </li> <li> <p>process_data task:</p> </li> <li>Receives the API data</li> <li>Performs some transformation (count, filter, summarize)</li> <li> <p>Returns processed results</p> </li> <li> <p>save_results task:</p> </li> <li>Takes processed results</li> <li>Simulates saving to a file (print the path and summary)</li> <li>Returns metadata about the save operation</li> </ol>"},{"location":"modules/03-operators-hooks/exercises/exercise_3_3_hook_usage/#about-hooks","title":"About Hooks","text":"<p>Hooks are Airflow's interface to external systems. They provide: - Connection management (credentials, endpoints) - Reusable interaction patterns - Consistent error handling</p> <p>Common hooks: - <code>HttpHook</code>: REST API calls - <code>PostgresHook</code>: Database operations - <code>S3Hook</code>: AWS S3 operations - <code>GCSHook</code>: Google Cloud Storage</p>"},{"location":"modules/03-operators-hooks/exercises/exercise_3_3_hook_usage/#starter-code","title":"Starter Code","text":"<p>Create a file <code>dags/playground/exercise_3_3_hook_usage.py</code>:</p> <pre><code>\"\"\"\nExercise 3.3: Hook Usage\n========================\nLearn to use Hooks for external system interaction.\n\"\"\"\n\nfrom datetime import datetime\n# TODO: Import necessary modules\n# from airflow.sdk import dag, task\n# from airflow.providers.http.hooks.http import HttpHook\n# Or use requests directly for simplicity\n\n\n# TODO: Create the DAG with @dag decorator\n\n    # TODO: Create fetch_data task\n    # Option A: Use HttpHook with a configured connection\n    # Option B: Use requests library directly (simpler for exercise)\n\n    # TODO: Create process_data task\n    # Summarize the fetched data\n\n    # TODO: Create save_results task\n    # Simulate saving to file\n\n    # TODO: Wire up the tasks\n</code></pre>"},{"location":"modules/03-operators-hooks/exercises/exercise_3_3_hook_usage/#hints","title":"Hints","text":"Hint 1: Using requests directly (simpler) <pre><code>import requests\n\n@task\ndef fetch_data() -&gt; list:\n    \"\"\"Fetch data from public API.\"\"\"\n    response = requests.get(\"https://jsonplaceholder.typicode.com/posts\")\n    response.raise_for_status()\n    return response.json()\n</code></pre> Hint 2: Using HttpHook (production pattern) <pre><code>from airflow.providers.http.hooks.http import HttpHook\n\n@task\ndef fetch_data() -&gt; list:\n    \"\"\"Fetch data using HttpHook.\"\"\"\n    # Requires connection 'http_default' or custom conn_id\n    hook = HttpHook(method=\"GET\", http_conn_id=\"http_default\")\n\n    # Override the endpoint if connection has base URL\n    response = hook.run(endpoint=\"/posts\")\n    return response.json()\n</code></pre> Hint 3: Processing data <pre><code>@task\ndef process_data(posts: list) -&gt; dict:\n    \"\"\"Process and summarize the posts data.\"\"\"\n    return {\n        \"total_posts\": len(posts),\n        \"unique_users\": len(set(p[\"userId\"] for p in posts)),\n        \"avg_title_length\": sum(len(p[\"title\"]) for p in posts) / len(posts),\n        \"sample_titles\": [p[\"title\"] for p in posts[:3]],\n    }\n</code></pre> Hint 4: Creating a connection (for HttpHook)  If using HttpHook, create a connection in Airflow UI: 1. Admin \u2192 Connections \u2192 Add 2. Conn ID: `jsonplaceholder_api` 3. Conn Type: HTTP 4. Host: `https://jsonplaceholder.typicode.com`  Or use environment variable: <pre><code>AIRFLOW_CONN_JSONPLACEHOLDER_API='http://jsonplaceholder.typicode.com'\n</code></pre>"},{"location":"modules/03-operators-hooks/exercises/exercise_3_3_hook_usage/#success-criteria","title":"Success Criteria","text":"<ul> <li> DAG successfully fetches data from external API</li> <li> Data is properly passed between tasks via XCom</li> <li> Processing task summarizes the data correctly</li> <li> Save task simulates file output with metadata</li> <li> Error handling for API failures (optional but recommended)</li> </ul>"},{"location":"modules/03-operators-hooks/exercises/exercise_3_3_hook_usage/#api-reference","title":"API Reference","text":"<p>JSONPlaceholder endpoints: - <code>/posts</code> - 100 posts - <code>/users</code> - 10 users - <code>/comments</code> - 500 comments - <code>/todos</code> - 200 todos</p> <p>Each returns JSON arrays with predictable structure.</p> <p>_[DAG]: Directed Acyclic Graph - Airflow's core workflow abstraction _[XCom]: Cross-Communication - Airflow's mechanism for task-to-task data passing _[Executor]: The component that runs tasks (Local, Celery, Kubernetes, etc.) _[Scheduler]: The component that triggers DAGs and determines task execution order _[Webserver]: The component that serves the Airflow UI _[Provider]: An Airflow package that integrates with external systems _[TaskFlow]: The modern Python-native API for defining Airflow tasks using decorators _[Asset]: A logical representation of data that DAGs can produce or consume _[SDK]: Software Development Kit - the airflow.sdk module in Airflow 3.x _[K8s]: Kubernetes - container orchestration platform _[uv]: A fast Python package manager by Astral _[ruff]: A fast Python linter and formatter by Astral _[CI]: Continuous Integration _[CD]: Continuous Deployment *[PR]: Pull Request</p>"},{"location":"modules/04-scheduling-triggers/","title":"Module 04: Scheduling &amp; Triggers","text":""},{"location":"modules/04-scheduling-triggers/#learning-objectives","title":"\ud83c\udfaf Learning Objectives","text":"<p>By the end of this module, you will: - Master cron expressions and Airflow schedule syntax - Understand Data Intervals and Logical Dates - Use timetables for complex scheduling - Configure trigger rules for conditional task execution - Handle timezones correctly - Use manual triggers and trigger parameters</p>"},{"location":"modules/04-scheduling-triggers/#estimated-time-3-4-hours","title":"\u23f1\ufe0f Estimated Time: 3-4 hours","text":""},{"location":"modules/04-scheduling-triggers/#1-schedule-syntax-options","title":"1. Schedule Syntax Options","text":""},{"location":"modules/04-scheduling-triggers/#preset-schedules","title":"Preset Schedules","text":"<pre><code>with DAG(\n    dag_id=\"scheduled_dag\",\n    schedule=\"@daily\",  # Preset schedule\n    start_date=datetime(2024, 1, 1),\n):\n    ...\n</code></pre> Preset Equivalent Cron Runs At <code>@once</code> - Once only <code>@hourly</code> <code>0 * * * *</code> Every hour at :00 <code>@daily</code> <code>0 0 * * *</code> Midnight daily <code>@weekly</code> <code>0 0 * * 0</code> Midnight Sunday <code>@monthly</code> <code>0 0 1 * *</code> 1<sup>st</sup> of month midnight <code>@yearly</code> <code>0 0 1 1 *</code> Jan 1<sup>st</sup> midnight"},{"location":"modules/04-scheduling-triggers/#cron-expressions","title":"Cron Expressions","text":"<pre><code># Format: minute hour day-of-month month day-of-week\nwith DAG(\n    dag_id=\"cron_dag\",\n    schedule=\"30 6 * * 1-5\",  # 6:30 AM on weekdays\n    ...\n):\n</code></pre> <p>Cron Fields: <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 minute (0 - 59)\n\u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 hour (0 - 23)\n\u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 day of the month (1 - 31)\n\u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 month (1 - 12)\n\u2502 \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 day of the week (0 - 6) (Sunday = 0)\n\u2502 \u2502 \u2502 \u2502 \u2502\n* * * * *\n</code></pre></p> <p>Common Patterns: <pre><code>\"0 */2 * * *\"     # Every 2 hours\n\"0 9-17 * * 1-5\"  # Every hour 9 AM - 5 PM, weekdays\n\"*/15 * * * *\"    # Every 15 minutes\n\"0 0 L * *\"       # Last day of month (if supported)\n</code></pre></p>"},{"location":"modules/04-scheduling-triggers/#no-schedule-manual-only","title":"No Schedule (Manual Only)","text":"<pre><code>with DAG(\n    dag_id=\"manual_only\",\n    schedule=None,  # Only triggered manually\n    ...\n):\n</code></pre>"},{"location":"modules/04-scheduling-triggers/#2-understanding-data-intervals","title":"2. Understanding Data Intervals","text":"<p>This is often confusing for newcomers. Let's clarify:</p>"},{"location":"modules/04-scheduling-triggers/#the-timeline","title":"The Timeline","text":"<p>For a daily DAG starting 2024-01-01:</p> <pre><code>Time:        00:00          00:00          00:00          00:00\nDate:        Jan 1          Jan 2          Jan 3          Jan 4\n             \u2502              \u2502              \u2502              \u2502\n             \u25bc              \u25bc              \u25bc              \u25bc\nRun 1:       \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524              \u2502              \u2502\n             \u2502   Interval 1 \u2502              \u2502              \u2502\n             \u2502              \u2502              \u2502              \u2502\n             \u2502              \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524              \u2502\nRun 2:       \u2502              \u2502   Interval 2 \u2502              \u2502\n             \u2502              \u2502              \u2502              \u2502\n             \u2502              \u2502              \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\nRun 3:       \u2502              \u2502              \u2502   Interval 3 \u2502\n</code></pre> <p>Key Insight: Each run processes data FROM its interval. The run executes at the END of its interval.</p>"},{"location":"modules/04-scheduling-triggers/#context-variables","title":"Context Variables","text":"<pre><code>@task\ndef show_dates(**context):\n    # What date/time this run is \"for\"\n    logical_date = context[\"logical_date\"]\n\n    # The data interval this run covers\n    start = context[\"data_interval_start\"]\n    end = context[\"data_interval_end\"]\n\n    # ds is a string shortcut\n    ds = context[\"ds\"]  # \"2024-01-01\"\n    ds_nodash = context[\"ds_nodash\"]  # \"20240101\"\n\n    print(f\"Logical date: {logical_date}\")\n    print(f\"Processing data from {start} to {end}\")\n</code></pre>"},{"location":"modules/04-scheduling-triggers/#example-daily-dag-running-on-jan-2","title":"Example: Daily DAG Running on Jan 2","text":"<pre><code>logical_date:        2024-01-01T00:00:00\ndata_interval_start: 2024-01-01T00:00:00  \ndata_interval_end:   2024-01-02T00:00:00\nActual execution:    2024-01-02T00:00:00 (or shortly after)\n</code></pre>"},{"location":"modules/04-scheduling-triggers/#3-trigger-rules","title":"3. Trigger Rules","text":"<p>Trigger rules determine when a task runs based on upstream task states.</p> <pre><code>from airflow.sdk import task\nfrom airflow.utils.trigger_rule import TriggerRule\n\n@task(trigger_rule=TriggerRule.ALL_SUCCESS)  # Default\ndef default_task():\n    pass\n\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef cleanup_task():\n    \"\"\"Runs regardless of upstream success/failure\"\"\"\n    pass\n\n@task(trigger_rule=TriggerRule.ONE_SUCCESS)\ndef needs_any_success():\n    \"\"\"Runs if ANY upstream task succeeds\"\"\"\n    pass\n</code></pre>"},{"location":"modules/04-scheduling-triggers/#available-trigger-rules","title":"Available Trigger Rules","text":"Rule Behavior <code>ALL_SUCCESS</code> All upstream tasks succeeded (default) <code>ALL_FAILED</code> All upstream tasks failed <code>ALL_DONE</code> All upstream tasks completed (any state) <code>ALL_SKIPPED</code> All upstream tasks were skipped <code>ONE_SUCCESS</code> At least one upstream succeeded <code>ONE_FAILED</code> At least one upstream failed <code>ONE_DONE</code> At least one upstream completed <code>NONE_FAILED</code> No upstream failed (includes skipped) <code>NONE_FAILED_MIN_ONE_SUCCESS</code> No failures AND at least one success <code>NONE_SKIPPED</code> No upstream was skipped <code>ALWAYS</code> Run regardless of upstream states"},{"location":"modules/04-scheduling-triggers/#common-patterns","title":"Common Patterns","text":"<pre><code># Cleanup that always runs\n@task(trigger_rule=TriggerRule.ALL_DONE)\ndef cleanup():\n    \"\"\"Clean up temp files regardless of pipeline success\"\"\"\n    pass\n\n# Notification on any failure\n@task(trigger_rule=TriggerRule.ONE_FAILED)\ndef notify_failure():\n    \"\"\"Alert team if any step fails\"\"\"\n    pass\n\n# Run if main path skipped\n@task(trigger_rule=TriggerRule.ALL_SKIPPED)\ndef fallback():\n    \"\"\"Alternative path if primary was skipped\"\"\"\n    pass\n</code></pre>"},{"location":"modules/04-scheduling-triggers/#4-catchup-and-backfilling","title":"4. Catchup and Backfilling","text":""},{"location":"modules/04-scheduling-triggers/#catchup-configuration","title":"Catchup Configuration","text":"<pre><code>with DAG(\n    dag_id=\"catchup_example\",\n    start_date=datetime(2024, 1, 1),\n    schedule=\"@daily\",\n    catchup=True,   # Default is False in Airflow 3!\n):\n    ...\n</code></pre> <p>With <code>catchup=True</code>: If DAG is first enabled on Jan 10, Airflow will create runs for Jan 1-9.</p> <p>With <code>catchup=False</code>: Only future runs are scheduled.</p>"},{"location":"modules/04-scheduling-triggers/#manual-backfilling","title":"Manual Backfilling","text":"<pre><code># Backfill specific date range\nairflow dags backfill \\\n    --start-date 2024-01-01 \\\n    --end-date 2024-01-31 \\\n    my_dag_id\n</code></pre>"},{"location":"modules/04-scheduling-triggers/#5-timezones","title":"5. Timezones","text":""},{"location":"modules/04-scheduling-triggers/#setting-dag-timezone","title":"Setting DAG Timezone","text":"<pre><code>import pendulum\n\nwith DAG(\n    dag_id=\"timezone_aware\",\n    start_date=pendulum.datetime(2024, 1, 1, tz=\"America/New_York\"),\n    schedule=\"0 9 * * *\",  # 9 AM Eastern Time\n):\n    ...\n</code></pre>"},{"location":"modules/04-scheduling-triggers/#timezone-aware-tasks","title":"Timezone-Aware Tasks","text":"<pre><code>@task\ndef timezone_task(**context):\n    import pendulum\n\n    # Logical date is always UTC\n    logical_date_utc = context[\"logical_date\"]\n\n    # Convert to local timezone\n    local_tz = pendulum.timezone(\"America/New_York\")\n    local_time = logical_date_utc.in_timezone(local_tz)\n\n    print(f\"UTC: {logical_date_utc}\")\n    print(f\"Local: {local_time}\")\n</code></pre>"},{"location":"modules/04-scheduling-triggers/#6-manual-triggers-with-parameters","title":"6. Manual Triggers with Parameters","text":""},{"location":"modules/04-scheduling-triggers/#dag-parameters","title":"DAG Parameters","text":"<pre><code>from airflow.models.param import Param\n\nwith DAG(\n    dag_id=\"parameterized_dag\",\n    schedule=None,\n    params={\n        \"environment\": Param(\n            default=\"dev\",\n            type=\"string\",\n            enum=[\"dev\", \"staging\", \"prod\"],\n        ),\n        \"batch_size\": Param(\n            default=100,\n            type=\"integer\",\n            minimum=1,\n            maximum=10000,\n        ),\n        \"dry_run\": Param(\n            default=True,\n            type=\"boolean\",\n        ),\n    },\n):\n    @task\n    def use_params(**context):\n        params = context[\"params\"]\n        env = params[\"environment\"]\n        batch = params[\"batch_size\"]\n        dry_run = params[\"dry_run\"]\n\n        print(f\"Running in {env} with batch {batch}\")\n        if dry_run:\n            print(\"DRY RUN - no changes made\")\n</code></pre>"},{"location":"modules/04-scheduling-triggers/#triggering-with-parameters-cli","title":"Triggering with Parameters (CLI)","text":"<pre><code>airflow dags trigger parameterized_dag \\\n    --conf '{\"environment\": \"prod\", \"batch_size\": 500, \"dry_run\": false}'\n</code></pre>"},{"location":"modules/04-scheduling-triggers/#7-custom-timetables","title":"7. Custom Timetables","text":"<p>For complex schedules that cron can't express:</p> <pre><code>from airflow.timetables.base import DagRunInfo, DataInterval, Timetable\nfrom pendulum import DateTime, Duration\n\nclass BusinessDaysTimetable(Timetable):\n    \"\"\"Run only on business days (Mon-Fri)\"\"\"\n\n    def next_dagrun_info(\n        self,\n        *,\n        last_automated_data_interval: DataInterval | None,\n        restriction,\n    ) -&gt; DagRunInfo | None:\n\n        if last_automated_data_interval is None:\n            # First run\n            next_start = restriction.earliest\n        else:\n            next_start = last_automated_data_interval.end\n\n        # Skip to next business day\n        while next_start.weekday() &gt;= 5:  # Saturday=5, Sunday=6\n            next_start = next_start.add(days=1)\n\n        next_end = next_start.add(days=1)\n\n        return DagRunInfo(\n            run_after=next_end,\n            data_interval=DataInterval(start=next_start, end=next_end),\n        )\n\n# Register in plugins\n# Then use in DAG:\nwith DAG(\n    dag_id=\"business_days_only\",\n    timetable=BusinessDaysTimetable(),\n):\n    ...\n</code></pre>"},{"location":"modules/04-scheduling-triggers/#exercises","title":"\ud83d\udcdd Exercises","text":""},{"location":"modules/04-scheduling-triggers/#exercise-41-schedule-interpretation","title":"Exercise 4.1: Schedule Interpretation","text":"<p>Create a DAG with <code>schedule=\"0 */6 * * 1-5\"</code> and add a task that prints: - The logical date - The data interval start and end - What date's data this run is processing</p> <p>Run <code>airflow dags test</code> for a few different dates and verify your understanding.</p>"},{"location":"modules/04-scheduling-triggers/#exercise-42-trigger-rules-pipeline","title":"Exercise 4.2: Trigger Rules Pipeline","text":"<p>Create a DAG with this structure: - 3 parallel tasks (one always succeeds, one always fails, one is random) - A task that runs only if ALL complete (regardless of state) - A task that runs only if AT LEAST ONE failed - A final cleanup task that ALWAYS runs</p>"},{"location":"modules/04-scheduling-triggers/#exercise-43-parameterized-dag","title":"Exercise 4.3: Parameterized DAG","text":"<p>Create a data processing DAG with parameters: - <code>source_table</code>: string, required - <code>target_table</code>: string, required - <code>mode</code>: enum [\"append\", \"overwrite\"], default \"append\" - <code>limit</code>: integer, optional, default null (no limit)</p>"},{"location":"modules/04-scheduling-triggers/#checkpoint","title":"\u2705 Checkpoint","text":"<p>Before moving to Module 05, ensure you can:</p> <ul> <li> Write cron expressions for various schedules</li> <li> Explain data intervals vs logical date</li> <li> Use trigger rules for conditional execution</li> <li> Configure catchup behavior appropriately</li> <li> Handle timezones in DAGs</li> <li> Create parameterized DAGs</li> </ul> <p>Next: Module 05: Assets &amp; Data-Aware Scheduling \u2192</p> <p>_[DAG]: Directed Acyclic Graph - Airflow's core workflow abstraction _[XCom]: Cross-Communication - Airflow's mechanism for task-to-task data passing _[Executor]: The component that runs tasks (Local, Celery, Kubernetes, etc.) _[Scheduler]: The component that triggers DAGs and determines task execution order _[Webserver]: The component that serves the Airflow UI _[Provider]: An Airflow package that integrates with external systems _[TaskFlow]: The modern Python-native API for defining Airflow tasks using decorators _[Asset]: A logical representation of data that DAGs can produce or consume _[SDK]: Software Development Kit - the airflow.sdk module in Airflow 3.x _[K8s]: Kubernetes - container orchestration platform _[uv]: A fast Python package manager by Astral _[ruff]: A fast Python linter and formatter by Astral _[CI]: Continuous Integration _[CD]: Continuous Deployment *[PR]: Pull Request</p>"},{"location":"modules/04-scheduling-triggers/exercises/exercise_4_1_schedule_interpretation/","title":"Exercise 4.1: Schedule Interpretation","text":""},{"location":"modules/04-scheduling-triggers/exercises/exercise_4_1_schedule_interpretation/#objective","title":"Objective","text":"<p>Create a DAG that helps you understand Airflow's scheduling concepts, including cron expressions, logical dates, and data intervals.</p>"},{"location":"modules/04-scheduling-triggers/exercises/exercise_4_1_schedule_interpretation/#requirements","title":"Requirements","text":"<p>Your DAG should: 1. Be named <code>exercise_4_1_schedule_interpretation</code> 2. Have a start date of January 1, 2024 3. Use schedule <code>\"0 */6 * * 1-5\"</code> (every 6 hours on weekdays) 4. Include tags: <code>[\"exercise\", \"module-04\", \"scheduling\"]</code></p>"},{"location":"modules/04-scheduling-triggers/exercises/exercise_4_1_schedule_interpretation/#task-requirements","title":"Task Requirements","text":"<ol> <li>explain_schedule task:</li> <li>Print the schedule in human-readable format</li> <li> <p>Explain what the cron expression means</p> </li> <li> <p>show_execution_context task:</p> </li> <li>Print the <code>logical_date</code></li> <li>Print <code>data_interval_start</code> and <code>data_interval_end</code></li> <li>Calculate and print the interval duration</li> <li> <p>Explain what data this run processes</p> </li> <li> <p>demonstrate_templating task:</p> </li> <li>Use Jinja templates: <code>{{ ds }}</code>, <code>{{ data_interval_start }}</code></li> <li>Show how templating works with scheduling</li> </ol>"},{"location":"modules/04-scheduling-triggers/exercises/exercise_4_1_schedule_interpretation/#key-concepts","title":"Key Concepts","text":"Concept Description <code>logical_date</code> The logical execution time (formerly <code>execution_date</code>) <code>data_interval_start</code> Start of the data period this run covers <code>data_interval_end</code> End of the data period (equals logical_date for scheduled runs) <code>ds</code> Date stamp: <code>YYYY-MM-DD</code> format of logical_date"},{"location":"modules/04-scheduling-triggers/exercises/exercise_4_1_schedule_interpretation/#cron-expression-reference","title":"Cron Expression Reference","text":"<p><code>0 */6 * * 1-5</code> means: - <code>0</code> - At minute 0 - <code>*/6</code> - Every 6 hours (0, 6, 12, 18) - <code>*</code> - Every day of month - <code>*</code> - Every month - <code>1-5</code> - Monday through Friday</p>"},{"location":"modules/04-scheduling-triggers/exercises/exercise_4_1_schedule_interpretation/#starter-code","title":"Starter Code","text":"<p>Create a file <code>dags/playground/exercise_4_1_schedule_interpretation.py</code>:</p> <pre><code>\"\"\"\nExercise 4.1: Schedule Interpretation\n=====================================\nUnderstand Airflow scheduling concepts.\n\"\"\"\n\nfrom datetime import datetime\n# TODO: Import dag and task from airflow.sdk\n\n\n# TODO: Create the DAG with the specified schedule\n# Note: Use @dag decorator or context manager\n\n    # TODO: Create explain_schedule task\n    # Print what the schedule means\n\n    # TODO: Create show_execution_context task\n    # Access and print context variables\n\n    # TODO: Create demonstrate_templating task\n    # Show Jinja template usage\n</code></pre>"},{"location":"modules/04-scheduling-triggers/exercises/exercise_4_1_schedule_interpretation/#testing-your-dag","title":"Testing Your DAG","text":"<p>Use <code>airflow dags test</code> to simulate runs for different dates:</p> <pre><code># Test for a Monday at 6am\nairflow dags test exercise_4_1_schedule_interpretation 2024-01-08T06:00:00\n\n# Test for a Friday at midnight\nairflow dags test exercise_4_1_schedule_interpretation 2024-01-12T00:00:00\n\n# This should NOT run (Saturday)\nairflow dags test exercise_4_1_schedule_interpretation 2024-01-13T06:00:00\n</code></pre>"},{"location":"modules/04-scheduling-triggers/exercises/exercise_4_1_schedule_interpretation/#hints","title":"Hints","text":"Hint 1: Accessing context <pre><code>@task\ndef show_execution_context(**context):\n    logical_date = context[\"logical_date\"]\n    interval_start = context[\"data_interval_start\"]\n    interval_end = context[\"data_interval_end\"]\n\n    print(f\"Logical date: {logical_date}\")\n    print(f\"Data interval: {interval_start} to {interval_end}\")\n</code></pre> Hint 2: Using BashOperator with templates <pre><code>from airflow.providers.standard.operators.bash import BashOperator\n\ntemplated_task = BashOperator(\n    task_id=\"demonstrate_templating\",\n    bash_command=\"\"\"\n        echo \"Date stamp: {{ ds }}\"\n        echo \"Interval start: {{ data_interval_start }}\"\n        echo \"Interval end: {{ data_interval_end }}\"\n    \"\"\",\n)\n</code></pre>"},{"location":"modules/04-scheduling-triggers/exercises/exercise_4_1_schedule_interpretation/#success-criteria","title":"Success Criteria","text":"<ul> <li> DAG has correct cron schedule</li> <li> explain_schedule correctly describes the schedule</li> <li> show_execution_context displays all interval information</li> <li> Templating task shows correct date values</li> <li> You can explain the difference between logical_date and data_interval</li> </ul> <p>_[DAG]: Directed Acyclic Graph - Airflow's core workflow abstraction _[XCom]: Cross-Communication - Airflow's mechanism for task-to-task data passing _[Executor]: The component that runs tasks (Local, Celery, Kubernetes, etc.) _[Scheduler]: The component that triggers DAGs and determines task execution order _[Webserver]: The component that serves the Airflow UI _[Provider]: An Airflow package that integrates with external systems _[TaskFlow]: The modern Python-native API for defining Airflow tasks using decorators _[Asset]: A logical representation of data that DAGs can produce or consume _[SDK]: Software Development Kit - the airflow.sdk module in Airflow 3.x _[K8s]: Kubernetes - container orchestration platform _[uv]: A fast Python package manager by Astral _[ruff]: A fast Python linter and formatter by Astral _[CI]: Continuous Integration _[CD]: Continuous Deployment *[PR]: Pull Request</p>"},{"location":"modules/04-scheduling-triggers/exercises/exercise_4_2_trigger_rules/","title":"Exercise 4.2: Trigger Rules Pipeline","text":""},{"location":"modules/04-scheduling-triggers/exercises/exercise_4_2_trigger_rules/#objective","title":"Objective","text":"<p>Create a DAG that demonstrates Airflow's trigger rules for controlling task execution based on the states of upstream tasks.</p>"},{"location":"modules/04-scheduling-triggers/exercises/exercise_4_2_trigger_rules/#background","title":"Background","text":"<p>Trigger rules determine when a task should run based on the status of its upstream dependencies. Understanding trigger rules is essential for building robust pipelines that handle failures gracefully.</p>"},{"location":"modules/04-scheduling-triggers/exercises/exercise_4_2_trigger_rules/#available-trigger-rules","title":"Available Trigger Rules","text":"Rule Behavior <code>all_success</code> (Default) Run if ALL upstream tasks succeeded <code>all_failed</code> Run if ALL upstream tasks failed <code>all_done</code> Run if ALL upstream tasks completed (any state) <code>all_skipped</code> Run if ALL upstream tasks were skipped <code>one_success</code> Run if AT LEAST ONE upstream succeeded <code>one_failed</code> Run if AT LEAST ONE upstream failed <code>one_done</code> Run if AT LEAST ONE upstream completed <code>none_failed</code> Run if NO upstream tasks failed (success or skipped) <code>none_skipped</code> Run if NO upstream tasks were skipped <code>always</code> Run regardless of upstream states"},{"location":"modules/04-scheduling-triggers/exercises/exercise_4_2_trigger_rules/#requirements","title":"Requirements","text":"<p>Your DAG should: 1. Be named <code>exercise_4_2_trigger_rules</code> 2. Have a start date of January 1, 2024 3. Use <code>schedule=None</code> (manual trigger only) 4. Include tags: <code>[\"exercise\", \"module-04\", \"trigger-rules\"]</code></p>"},{"location":"modules/04-scheduling-triggers/exercises/exercise_4_2_trigger_rules/#task-structure","title":"Task Structure","text":"<p>Create a pipeline with the following structure:</p> <pre><code>                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502  always_succeed \u2502\u2500\u2500\u2510\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n                                         \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  start  \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\u2502  always_fail    \u2502\u2500\u2500\u253c\u2500\u2500\u2500\u25b6\u2502  all_done    \u2502\u2500\u2500\u2500\u25b6\u2502 cleanup \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                         \u2502           \u2502\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502 random_outcome  \u2502\u2500\u2500\u2518    \u2502  one_failed  \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"modules/04-scheduling-triggers/exercises/exercise_4_2_trigger_rules/#task-requirements","title":"Task Requirements","text":"<ol> <li> <p>start: Simple task that marks the beginning of the pipeline</p> </li> <li> <p>always_succeed: Always completes successfully</p> </li> <li> <p>Print \"This task always succeeds!\"</p> </li> <li> <p>always_fail: Always raises an exception</p> </li> <li> <p>Raise <code>AirflowException</code> with message \"This task always fails!\"</p> </li> <li> <p>random_outcome: Randomly succeeds or fails (50/50 chance)</p> </li> <li> <p>Use <code>random.random() &lt; 0.5</code> to determine outcome</p> </li> <li> <p>all_done_task: Runs when ALL upstream tasks complete</p> </li> <li>Use <code>trigger_rule=TriggerRule.ALL_DONE</code></li> <li> <p>Print the states of upstream tasks</p> </li> <li> <p>one_failed_task: Runs when AT LEAST ONE upstream failed</p> </li> <li>Use <code>trigger_rule=TriggerRule.ONE_FAILED</code></li> <li> <p>Log which tasks failed</p> </li> <li> <p>cleanup: ALWAYS runs regardless of upstream states</p> </li> <li>Use <code>trigger_rule=TriggerRule.ALWAYS</code></li> <li>Print \"Cleanup completed - this always runs!\"</li> </ol>"},{"location":"modules/04-scheduling-triggers/exercises/exercise_4_2_trigger_rules/#key-concepts","title":"Key Concepts","text":""},{"location":"modules/04-scheduling-triggers/exercises/exercise_4_2_trigger_rules/#importing-triggerrule","title":"Importing TriggerRule","text":"<pre><code>from airflow.utils.trigger_rule import TriggerRule\n</code></pre>"},{"location":"modules/04-scheduling-triggers/exercises/exercise_4_2_trigger_rules/#applying-trigger-rules","title":"Applying Trigger Rules","text":"<pre><code>@task(trigger_rule=TriggerRule.ALL_DONE)\ndef my_task():\n    pass\n</code></pre>"},{"location":"modules/04-scheduling-triggers/exercises/exercise_4_2_trigger_rules/#handling-expected-failures","title":"Handling Expected Failures","text":"<p>When a task is designed to fail, downstream tasks with appropriate trigger rules will still execute.</p>"},{"location":"modules/04-scheduling-triggers/exercises/exercise_4_2_trigger_rules/#starter-code","title":"Starter Code","text":"<p>See <code>exercise_4_2_trigger_rules_starter.py</code></p>"},{"location":"modules/04-scheduling-triggers/exercises/exercise_4_2_trigger_rules/#testing-your-dag","title":"Testing Your DAG","text":"<pre><code># Run the DAG - observe how trigger rules affect execution\nairflow dags test exercise_4_2_trigger_rules 2024-01-15\n\n# Check the UI to see task states and which tasks ran\n</code></pre>"},{"location":"modules/04-scheduling-triggers/exercises/exercise_4_2_trigger_rules/#hints","title":"Hints","text":"Hint 1: Raising an exception <pre><code>from airflow.exceptions import AirflowException\n\n@task\ndef always_fail():\n    raise AirflowException(\"This task always fails!\")\n</code></pre> Hint 2: Random task outcome <pre><code>import random\n\n@task\ndef random_outcome():\n    if random.random() &lt; 0.5:\n        raise AirflowException(\"Random failure!\")\n    return \"Random success!\"\n</code></pre> Hint 3: Checking upstream states <pre><code>@task(trigger_rule=TriggerRule.ALL_DONE)\ndef all_done_task(**context):\n    ti = context[\"ti\"]\n    dag_run = context[\"dag_run\"]\n\n    # Get all task instances in this run\n    task_instances = dag_run.get_task_instances()\n\n    for task_instance in task_instances:\n        print(f\"{task_instance.task_id}: {task_instance.state}\")\n</code></pre>"},{"location":"modules/04-scheduling-triggers/exercises/exercise_4_2_trigger_rules/#success-criteria","title":"Success Criteria","text":"<ul> <li> DAG has correct structure with 7 tasks</li> <li> <code>always_succeed</code> always completes successfully</li> <li> <code>always_fail</code> always fails with AirflowException</li> <li> <code>random_outcome</code> randomly succeeds or fails</li> <li> <code>all_done_task</code> runs even when upstream tasks fail</li> <li> <code>one_failed_task</code> runs when any upstream task fails</li> <li> <code>cleanup</code> ALWAYS runs regardless of other task states</li> <li> You can explain when each trigger rule is useful</li> </ul>"},{"location":"modules/04-scheduling-triggers/exercises/exercise_4_2_trigger_rules/#real-world-applications","title":"Real-World Applications","text":"Trigger Rule Use Case <code>all_done</code> Notification tasks that report on pipeline completion <code>one_failed</code> Alert tasks that notify on any failure <code>always</code> Cleanup tasks, resource deallocation, logging <code>none_failed</code> Tasks that can proceed if nothing explicitly failed <code>one_success</code> Tasks that need at least one successful input <p>_[DAG]: Directed Acyclic Graph - Airflow's core workflow abstraction _[XCom]: Cross-Communication - Airflow's mechanism for task-to-task data passing _[Executor]: The component that runs tasks (Local, Celery, Kubernetes, etc.) _[Scheduler]: The component that triggers DAGs and determines task execution order _[Webserver]: The component that serves the Airflow UI _[Provider]: An Airflow package that integrates with external systems _[TaskFlow]: The modern Python-native API for defining Airflow tasks using decorators _[Asset]: A logical representation of data that DAGs can produce or consume _[SDK]: Software Development Kit - the airflow.sdk module in Airflow 3.x _[K8s]: Kubernetes - container orchestration platform _[uv]: A fast Python package manager by Astral _[ruff]: A fast Python linter and formatter by Astral _[CI]: Continuous Integration _[CD]: Continuous Deployment *[PR]: Pull Request</p>"},{"location":"modules/04-scheduling-triggers/exercises/exercise_4_3_parameterized_dag/","title":"Exercise 4.3: Parameterized DAG","text":""},{"location":"modules/04-scheduling-triggers/exercises/exercise_4_3_parameterized_dag/#objective","title":"Objective","text":"<p>Create a data processing DAG that accepts runtime parameters, allowing users to customize pipeline behavior without modifying code.</p>"},{"location":"modules/04-scheduling-triggers/exercises/exercise_4_3_parameterized_dag/#background","title":"Background","text":"<p>Airflow 3.x provides powerful parameterization through the <code>Param</code> class, enabling: - Type-safe parameter validation - Default values and required parameters - Enum constraints for limited choices - Parameter descriptions for documentation - UI-friendly parameter forms</p>"},{"location":"modules/04-scheduling-triggers/exercises/exercise_4_3_parameterized_dag/#param-class","title":"Param Class","text":"<pre><code>from airflow.sdk.definitions.param import Param\n\n@dag(\n    params={\n        \"my_param\": Param(\n            default=\"value\",\n            type=\"string\",\n            description=\"Parameter description\",\n        ),\n    }\n)\n</code></pre>"},{"location":"modules/04-scheduling-triggers/exercises/exercise_4_3_parameterized_dag/#accessing-parameters","title":"Accessing Parameters","text":"<pre><code>@task\ndef my_task(**context):\n    params = context[\"params\"]\n    value = params[\"my_param\"]\n</code></pre> <p>Or with Jinja templating: <pre><code>BashOperator(\n    task_id=\"bash_task\",\n    bash_command=\"echo {{ params.my_param }}\",\n)\n</code></pre></p>"},{"location":"modules/04-scheduling-triggers/exercises/exercise_4_3_parameterized_dag/#requirements","title":"Requirements","text":"<p>Your DAG should: 1. Be named <code>exercise_4_3_parameterized_dag</code> 2. Have a start date of January 1, 2024 3. Use <code>schedule=None</code> (manual trigger only) 4. Include tags: <code>[\"exercise\", \"module-04\", \"params\"]</code></p>"},{"location":"modules/04-scheduling-triggers/exercises/exercise_4_3_parameterized_dag/#required-parameters","title":"Required Parameters","text":"Parameter Type Required Default Description <code>source_table</code> string Yes - Source table to read from <code>target_table</code> string Yes - Target table to write to <code>mode</code> enum No \"append\" Write mode: \"append\" or \"overwrite\" <code>limit</code> integer No null Row limit (null = no limit)"},{"location":"modules/04-scheduling-triggers/exercises/exercise_4_3_parameterized_dag/#task-structure","title":"Task Structure","text":"<ol> <li>validate_params: Validate all parameters and print configuration</li> <li>extract_data: Simulate reading from source table with optional limit</li> <li>transform_data: Process the extracted data</li> <li>load_data: Simulate writing to target table with specified mode</li> <li>log_completion: Log the completed operation details</li> </ol>"},{"location":"modules/04-scheduling-triggers/exercises/exercise_4_3_parameterized_dag/#key-concepts","title":"Key Concepts","text":""},{"location":"modules/04-scheduling-triggers/exercises/exercise_4_3_parameterized_dag/#defining-parameters","title":"Defining Parameters","text":"<pre><code>@dag(\n    params={\n        \"required_param\": Param(\n            type=\"string\",\n            description=\"This parameter is required\",\n        ),\n        \"optional_param\": Param(\n            default=\"default_value\",\n            type=\"string\",\n            description=\"This parameter has a default\",\n        ),\n        \"enum_param\": Param(\n            default=\"option1\",\n            type=\"string\",\n            enum=[\"option1\", \"option2\", \"option3\"],\n            description=\"Choose from predefined options\",\n        ),\n        \"nullable_int\": Param(\n            default=None,\n            type=[\"null\", \"integer\"],\n            description=\"Optional integer (can be null)\",\n        ),\n    }\n)\n</code></pre>"},{"location":"modules/04-scheduling-triggers/exercises/exercise_4_3_parameterized_dag/#parameter-types","title":"Parameter Types","text":"<ul> <li><code>\"string\"</code> - Text values</li> <li><code>\"integer\"</code> - Whole numbers</li> <li><code>\"number\"</code> - Decimal numbers</li> <li><code>\"boolean\"</code> - True/False</li> <li><code>\"array\"</code> - Lists</li> <li><code>\"object\"</code> - Dictionaries</li> <li><code>[\"null\", \"type\"]</code> - Nullable values</li> </ul>"},{"location":"modules/04-scheduling-triggers/exercises/exercise_4_3_parameterized_dag/#starter-code","title":"Starter Code","text":"<p>See <code>exercise_4_3_parameterized_dag_starter.py</code></p>"},{"location":"modules/04-scheduling-triggers/exercises/exercise_4_3_parameterized_dag/#testing-your-dag","title":"Testing Your DAG","text":""},{"location":"modules/04-scheduling-triggers/exercises/exercise_4_3_parameterized_dag/#via-cli","title":"Via CLI","text":"<pre><code># Test with required parameters only\nairflow dags test exercise_4_3_parameterized_dag 2024-01-15 \\\n  -c '{\"source_table\": \"raw_events\", \"target_table\": \"processed_events\"}'\n\n# Test with all parameters\nairflow dags test exercise_4_3_parameterized_dag 2024-01-15 \\\n  -c '{\"source_table\": \"raw_events\", \"target_table\": \"processed_events\", \"mode\": \"overwrite\", \"limit\": 1000}'\n</code></pre>"},{"location":"modules/04-scheduling-triggers/exercises/exercise_4_3_parameterized_dag/#via-ui","title":"Via UI","text":"<ol> <li>Go to the DAG in the Airflow UI</li> <li>Click \"Trigger DAG\" button</li> <li>Fill in the parameter form</li> <li>Click \"Trigger\"</li> </ol>"},{"location":"modules/04-scheduling-triggers/exercises/exercise_4_3_parameterized_dag/#hints","title":"Hints","text":"Hint 1: Defining required string parameter <pre><code>\"source_table\": Param(\n    type=\"string\",\n    description=\"Source table name to read data from\",\n    minLength=1,  # Ensures non-empty string\n),\n</code></pre> Hint 2: Defining enum parameter <pre><code>\"mode\": Param(\n    default=\"append\",\n    type=\"string\",\n    enum=[\"append\", \"overwrite\"],\n    description=\"Write mode for target table\",\n),\n</code></pre> Hint 3: Defining nullable integer <pre><code>\"limit\": Param(\n    default=None,\n    type=[\"null\", \"integer\"],\n    minimum=1,  # If provided, must be at least 1\n    description=\"Row limit (null for no limit)\",\n),\n</code></pre> Hint 4: Accessing params in task <pre><code>@task\ndef extract_data(**context):\n    params = context[\"params\"]\n    source = params[\"source_table\"]\n    limit = params.get(\"limit\")  # May be None\n\n    if limit:\n        print(f\"Reading {limit} rows from {source}\")\n    else:\n        print(f\"Reading all rows from {source}\")\n</code></pre>"},{"location":"modules/04-scheduling-triggers/exercises/exercise_4_3_parameterized_dag/#success-criteria","title":"Success Criteria","text":"<ul> <li> DAG accepts <code>source_table</code> and <code>target_table</code> as required parameters</li> <li> DAG accepts <code>mode</code> with enum validation (append/overwrite)</li> <li> DAG accepts <code>limit</code> as optional nullable integer</li> <li> Parameters are validated before processing begins</li> <li> Tasks correctly use parameter values</li> <li> DAG can be triggered from UI with parameter form</li> <li> You can explain how to add new parameters</li> </ul>"},{"location":"modules/04-scheduling-triggers/exercises/exercise_4_3_parameterized_dag/#real-world-use-cases","title":"Real-World Use Cases","text":"Use Case Parameters ETL Pipeline source_db, target_db, schema, batch_size Report Generation report_date, format, recipients Data Backfill start_date, end_date, partitions API Sync api_endpoint, page_size, incremental ML Training model_type, hyperparameters, dataset <p>_[DAG]: Directed Acyclic Graph - Airflow's core workflow abstraction _[XCom]: Cross-Communication - Airflow's mechanism for task-to-task data passing _[Executor]: The component that runs tasks (Local, Celery, Kubernetes, etc.) _[Scheduler]: The component that triggers DAGs and determines task execution order _[Webserver]: The component that serves the Airflow UI _[Provider]: An Airflow package that integrates with external systems _[TaskFlow]: The modern Python-native API for defining Airflow tasks using decorators _[Asset]: A logical representation of data that DAGs can produce or consume _[SDK]: Software Development Kit - the airflow.sdk module in Airflow 3.x _[K8s]: Kubernetes - container orchestration platform _[uv]: A fast Python package manager by Astral _[ruff]: A fast Python linter and formatter by Astral _[CI]: Continuous Integration _[CD]: Continuous Deployment *[PR]: Pull Request</p>"},{"location":"modules/05-assets-data-aware/","title":"Module 05: Assets &amp; Data-Aware Scheduling","text":""},{"location":"modules/05-assets-data-aware/#learning-objectives","title":"\ud83c\udfaf Learning Objectives","text":"<p>By the end of this module, you will:</p> <ul> <li>Understand Assets (formerly Datasets) and data-aware scheduling</li> <li>Create DAGs that trigger based on data availability</li> <li>Use the <code>@asset</code> decorator for asset-centric workflows</li> <li>Implement Asset Watchers for external event monitoring</li> <li>Design multi-DAG pipelines with data dependencies</li> </ul>"},{"location":"modules/05-assets-data-aware/#estimated-time-4-5-hours","title":"\u23f1\ufe0f Estimated Time: 4-5 hours","text":""},{"location":"modules/05-assets-data-aware/#1-what-are-assets","title":"1. What Are Assets?","text":"<p>Assets represent logical data entities that can trigger DAG runs when updated. Instead of time-based scheduling, DAGs can be triggered when their input data becomes available.</p>"},{"location":"modules/05-assets-data-aware/#key-concepts","title":"Key Concepts","text":"Term Definition Asset A logical data reference (URI) that represents data Producer A task that creates/updates an Asset Consumer A DAG that runs when Assets are updated Outlet An Asset that a task produces"},{"location":"modules/05-assets-data-aware/#naming-change-airflow-2x-3","title":"Naming Change (Airflow 2.x \u2192 3)","text":"<pre><code># Airflow 2.x\n\n# Airflow 3\n</code></pre>"},{"location":"modules/05-assets-data-aware/#2-basic-asset-usage","title":"2. Basic Asset Usage","text":""},{"location":"modules/05-assets-data-aware/#defining-an-asset","title":"Defining an Asset","text":"<pre><code>from airflow.sdk import Asset\n\n# Assets are identified by URI\nusers_table = Asset(\"postgres://warehouse/users\")\ndaily_report = Asset(\"s3://reports/daily/{{ ds }}.csv\")\napi_data = Asset(\"https://api.example.com/data\")\n\n# URIs can be any string - Airflow doesn't validate them\n# They're logical identifiers, not actual connections\n</code></pre>"},{"location":"modules/05-assets-data-aware/#producer-dag","title":"Producer DAG","text":"<pre><code>from datetime import datetime\n\nfrom airflow.sdk import DAG, Asset, task\n\n# Define the Asset this DAG produces\nusers_asset = Asset(\"postgres://warehouse/users\")\n\nwith DAG(\n    dag_id=\"etl_users\",\n    schedule=\"@daily\",\n    start_date=datetime(2024, 1, 1),\n):\n\n    @task(outlets=[users_asset])  # Marks this task as producing the Asset\n    def load_users():\n        # Your ETL logic here\n        print(\"Loading users to warehouse...\")\n        return {\"rows_loaded\": 1000}\n</code></pre>"},{"location":"modules/05-assets-data-aware/#consumer-dag","title":"Consumer DAG","text":"<pre><code>from datetime import datetime\n\nfrom airflow.sdk import DAG, Asset, task\n\n# Reference the same Asset\nusers_asset = Asset(\"postgres://warehouse/users\")\n\nwith DAG(\n    dag_id=\"report_users\",\n    schedule=[users_asset],  # Trigger when Asset is updated!\n    start_date=datetime(2024, 1, 1),\n):\n\n    @task\n    def generate_report():\n        print(\"Generating user report...\")\n</code></pre>"},{"location":"modules/05-assets-data-aware/#the-flow","title":"The Flow","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   etl_users     \u2502           \u2502  report_users   \u2502\n\u2502   (Producer)    \u2502           \u2502  (Consumer)     \u2502\n\u2502                 \u2502           \u2502                 \u2502\n\u2502  @daily         \u2502  triggers \u2502  schedule=[     \u2502\n\u2502  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba   \u2502  \u2500\u2500\u2500\u2500\u2500\u2500\u25ba  \u2502    users_asset] \u2502\n\u2502  outlets=[      \u2502           \u2502                 \u2502\n\u2502    users_asset] \u2502           \u2502                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"modules/05-assets-data-aware/#3-multiple-asset-dependencies","title":"3. Multiple Asset Dependencies","text":""},{"location":"modules/05-assets-data-aware/#waiting-for-multiple-assets","title":"Waiting for Multiple Assets","text":"<pre><code>from airflow.sdk import DAG, Asset, task\n\n# Define multiple Assets\norders_asset = Asset(\"postgres://warehouse/orders\")\nproducts_asset = Asset(\"postgres://warehouse/products\")\ncustomers_asset = Asset(\"postgres://warehouse/customers\")\n\nwith DAG(\n    dag_id=\"sales_analytics\",\n    schedule=[orders_asset, products_asset, customers_asset],\n    # Triggers only when ALL three are updated\n):\n\n    @task\n    def run_analytics():\n        print(\"All data available, running analytics...\")\n</code></pre>"},{"location":"modules/05-assets-data-aware/#logical-operators-andor","title":"Logical Operators (AND/OR)","text":"<pre><code>from airflow.sdk import Asset\n\n# Default: AND logic (all must be updated)\nschedule = [asset_a, asset_b]  # Wait for both\n\n# OR logic: trigger when ANY is updated\nschedule = Asset.any(asset_a, asset_b)  # Wait for either\n\n# Complex combinations\nschedule = Asset.all(asset_a, Asset.any(asset_b, asset_c))  # a AND (b OR c)\n</code></pre>"},{"location":"modules/05-assets-data-aware/#4-the-asset-decorator-airflow-3","title":"4. The @asset Decorator (Airflow 3)","text":"<p>Airflow 3 introduces <code>@asset</code> for a more Pythonic, asset-centric approach:</p> <pre><code>from airflow.sdk import asset\n\n\n# Define an Asset that's also its own producer\n@asset(schedule=\"@daily\", uri=\"s3://data-lake/processed/users\")\ndef processed_users():\n    \"\"\"This function IS the asset AND the task that produces it\"\"\"\n    raw_data = fetch_raw_users()\n    cleaned = clean_data(raw_data)\n    save_to_s3(cleaned, \"s3://data-lake/processed/users\")\n    return cleaned\n\n\n# Another asset that depends on the first\n@asset(\n    schedule=[processed_users],  # Triggered by processed_users\n    uri=\"s3://data-lake/analytics/user_metrics\",\n)\ndef user_metrics(processed_users):  # Receives upstream asset\n    \"\"\"Computes metrics from processed users\"\"\"\n    metrics = compute_metrics(processed_users)\n    save_to_s3(metrics, \"s3://data-lake/analytics/user_metrics\")\n    return metrics\n</code></pre>"},{"location":"modules/05-assets-data-aware/#benefits-of-asset","title":"Benefits of @asset","text":"<ol> <li>Single definition: Asset and its producer in one place</li> <li>Clear dependencies: Function parameters show data flow</li> <li>Automatic scheduling: No separate DAG definition needed</li> <li>Self-documenting: Code structure matches data flow</li> </ol>"},{"location":"modules/05-assets-data-aware/#5-asset-watchers","title":"5. Asset Watchers","text":"<p>Asset Watchers monitor external systems and trigger Assets without running tasks:</p> <pre><code>from airflow.sdk import Asset, AssetWatcher\n\n# Define an Asset\nexternal_file = Asset(\"s3://partner-data/daily-feed\")\n\n# Create a watcher that monitors for updates\nwatcher = AssetWatcher(\n    asset=external_file,\n    trigger_type=\"s3\",  # Built-in S3 watcher\n    trigger_kwargs={\n        \"bucket\": \"partner-data\",\n        \"prefix\": \"daily-feed\",\n        \"aws_conn_id\": \"aws_default\",\n    },\n)\n\n# Consumer DAG\nwith DAG(\n    dag_id=\"process_partner_data\",\n    schedule=[external_file],\n):\n\n    @task\n    def process():\n        print(\"Partner data arrived, processing...\")\n</code></pre>"},{"location":"modules/05-assets-data-aware/#sqs-integration-airflow-31","title":"SQS Integration (Airflow 3.1+)","text":"<pre><code>from airflow.providers.amazon.aws.triggers.sqs import SqsAssetTrigger\nfrom airflow.sdk import Asset\n\n# Asset triggered by SQS messages\nsqs_triggered_asset = Asset(\n    uri=\"events://order-events\",\n    watchers=[\n        SqsAssetTrigger(\n            sqs_queue=\"order-events-queue\",\n            aws_conn_id=\"aws_default\",\n        )\n    ],\n)\n</code></pre>"},{"location":"modules/05-assets-data-aware/#6-asset-aliases-and-patterns","title":"6. Asset Aliases and Patterns","text":""},{"location":"modules/05-assets-data-aware/#asset-aliases","title":"Asset Aliases","text":"<p>Group related Assets under a single identifier:</p> <pre><code>from airflow.sdk import Asset, AssetAlias\n\n# Individual Assets\nraw_orders = Asset(\"s3://raw/orders\")\nraw_products = Asset(\"s3://raw/products\")\nraw_customers = Asset(\"s3://raw/customers\")\n\n# Alias for all raw data\nall_raw_data = AssetAlias(\"raw-data-complete\")\n\n\n# Producer marks alias as complete\n@task(outlets=[all_raw_data])\ndef mark_raw_complete():\n    \"\"\"Signal that all raw data is loaded\"\"\"\n    pass\n\n\n# Consumer waits on alias\nwith DAG(schedule=[all_raw_data]):\n    ...\n</code></pre>"},{"location":"modules/05-assets-data-aware/#dynamic-assets","title":"Dynamic Assets","text":"<pre><code>from airflow.sdk import Asset\n\n\ndef get_partition_asset(date: str) -&gt; Asset:\n    \"\"\"Create date-partitioned Asset\"\"\"\n    return Asset(f\"s3://data/partitions/{date}\")\n\n\n# In a task\n@task(outlets=[get_partition_asset(\"{{ ds }}\")])\ndef process_partition(**context):\n    date = context[\"ds\"]\n    # Process partition for this date\n    ...\n</code></pre>"},{"location":"modules/05-assets-data-aware/#7-viewing-assets-in-the-ui","title":"7. Viewing Assets in the UI","text":"<p>Airflow 3's new UI has dedicated Asset views:</p> <ol> <li>Assets View: See all defined Assets</li> <li>Asset Details: View producers and consumers</li> <li>Lineage Graph: Visualize data flow across DAGs</li> <li>Update History: Track when Assets were updated</li> </ol> <p>Navigate: DAGs \u2192 Assets in the top navigation</p>"},{"location":"modules/05-assets-data-aware/#8-best-practices","title":"8. Best Practices","text":""},{"location":"modules/05-assets-data-aware/#do","title":"DO \u2705","text":"<pre><code># Use descriptive, hierarchical URIs\nAsset(\"s3://data-lake/bronze/orders/v1\")\nAsset(\"postgres://warehouse.analytics/user_metrics\")\n\n# Document Assets\norders_asset = Asset(uri=\"s3://data-lake/orders\", extra={\"owner\": \"data-team\", \"sla_hours\": 2})\n\n# Keep producer/consumer relationships clean\n# One clear producer per Asset when possible\n</code></pre>"},{"location":"modules/05-assets-data-aware/#dont","title":"DON'T \u274c","text":"<pre><code># Don't use vague URIs\nAsset(\"data\")  # Bad - not descriptive\n\n# Don't have multiple uncoordinated producers\n# (Can lead to unexpected triggers)\n\n# Don't use Assets for fine-grained triggering\n# (Not a replacement for message queues)\n</code></pre>"},{"location":"modules/05-assets-data-aware/#exercises","title":"\ud83d\udcdd Exercises","text":""},{"location":"modules/05-assets-data-aware/#exercise-51-basic-producerconsumer","title":"Exercise 5.1: Basic Producer/Consumer","text":"<p>Create a two-DAG pipeline:</p> <ol> <li><code>data_producer</code>: Runs hourly, produces a \"processed_data\" Asset</li> <li><code>data_consumer</code>: Triggers when Asset is updated, prints a message</li> </ol> <p>Test by manually triggering the producer.</p>"},{"location":"modules/05-assets-data-aware/#exercise-52-multi-asset-dependencies","title":"Exercise 5.2: Multi-Asset Dependencies","text":"<p>Create a pipeline where:</p> <ol> <li>Three producer DAGs each update their own Asset</li> <li>A consumer DAG only runs when ALL three are updated</li> <li>Add logging to see which triggered</li> </ol>"},{"location":"modules/05-assets-data-aware/#exercise-53-asset-pattern","title":"Exercise 5.3: @asset Pattern","text":"<p>Rewrite a traditional ETL DAG using <code>@asset</code>:</p> <ul> <li>Extract raw data (Asset A)</li> <li>Transform data (Asset B, depends on A)</li> <li>Load to warehouse (Asset C, depends on B)</li> </ul>"},{"location":"modules/05-assets-data-aware/#checkpoint","title":"\u2705 Checkpoint","text":"<p>Before moving to Module 06, ensure you can:</p> <ul> <li> Define and reference Assets by URI</li> <li> Create producer tasks with <code>outlets</code></li> <li> Create consumer DAGs with <code>schedule=[asset]</code></li> <li> Use multiple Asset dependencies (AND/OR)</li> <li> Understand the @asset decorator pattern</li> <li> Navigate Asset views in the UI</li> </ul>"},{"location":"modules/05-assets-data-aware/#industry-spotlight-uber","title":"\ud83c\udfed Industry Spotlight: Uber","text":"<p>How Uber Uses Asset-Driven ML Retraining</p> <p>Uber's machine learning platform processes billions of events daily to power features like surge pricing, ETA predictions, and driver matching. Asset-driven scheduling ensures models retrain only when necessary:</p> Challenge Asset Solution Feature freshness Feature stores emit Assets when updated Training efficiency Models only retrain when dependencies change Cross-team coordination Clear Asset contracts between data and ML teams Cost optimization No unnecessary training runs <p>Pattern in Use: Uber-style ML retraining with Assets:</p> <pre><code>from airflow.sdk import Asset, dag, task\n\n# Feature store assets\nride_features = Asset(\"feature-store://rides/v3\")\ndriver_features = Asset(\"feature-store://drivers/v3\")\n\n\n# Model retrains when BOTH feature sets update\n@dag(schedule=[ride_features, driver_features])\ndef retrain_eta_model():\n    @task\n    def train_model():\n        \"\"\"Only runs when fresh features are available.\"\"\"\n        features = load_features([\"rides/v3\", \"drivers/v3\"])\n        model = train_eta_predictor(features)\n        deploy_model(model)\n</code></pre> <p>Key Insight: Asset-driven scheduling reduced Uber's unnecessary ML training runs by 60%, saving significant compute costs while maintaining model freshness.</p> <p>\ud83d\udcd6 Related Exercise: Exercise 5.4: Embedding Assets - Build Asset-driven AI/ML pipelines</p>"},{"location":"modules/05-assets-data-aware/#further-reading","title":"\ud83d\udcda Further Reading","text":"<ul> <li>Assets Documentation</li> <li>Data-Aware Scheduling</li> <li>Case Study: Spotify Recommendations</li> </ul> <p>Next: Module 06: Dynamic Task Mapping \u2192</p> <p>_[DAG]: Directed Acyclic Graph - Airflow's core workflow abstraction _[XCom]: Cross-Communication - Airflow's mechanism for task-to-task data passing _[Executor]: The component that runs tasks (Local, Celery, Kubernetes, etc.) _[Scheduler]: The component that triggers DAGs and determines task execution order _[Webserver]: The component that serves the Airflow UI _[Provider]: An Airflow package that integrates with external systems _[TaskFlow]: The modern Python-native API for defining Airflow tasks using decorators _[Asset]: A logical representation of data that DAGs can produce or consume _[SDK]: Software Development Kit - the airflow.sdk module in Airflow 3.x _[K8s]: Kubernetes - container orchestration platform _[uv]: A fast Python package manager by Astral _[ruff]: A fast Python linter and formatter by Astral _[CI]: Continuous Integration _[CD]: Continuous Deployment *[PR]: Pull Request</p>"},{"location":"modules/05-assets-data-aware/exercises/exercise_5_1_producer_consumer/","title":"Exercise 5.1: Basic Producer/Consumer","text":""},{"location":"modules/05-assets-data-aware/exercises/exercise_5_1_producer_consumer/#objective","title":"Objective","text":"<p>Create a two-DAG pipeline demonstrating the fundamental Asset producer/consumer pattern in Airflow 3.x.</p>"},{"location":"modules/05-assets-data-aware/exercises/exercise_5_1_producer_consumer/#background","title":"Background","text":"<p>Assets (formerly Datasets in Airflow 2.x) represent logical data entities that trigger DAG runs when updated. This enables data-aware scheduling where downstream DAGs automatically run when their input data becomes available.</p>"},{"location":"modules/05-assets-data-aware/exercises/exercise_5_1_producer_consumer/#key-concepts","title":"Key Concepts","text":"Concept Description Asset A logical data reference identified by URI Producer A task with <code>outlets=[asset]</code> that signals data availability Consumer A DAG with <code>schedule=[asset]</code> that triggers on updates URI A unique identifier string (doesn't need to be a real URL)"},{"location":"modules/05-assets-data-aware/exercises/exercise_5_1_producer_consumer/#how-it-works","title":"How It Works","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Producer DAG  \u2502    Asset     \u2502  Consumer DAG   \u2502\n\u2502                 \u2502   Update     \u2502                 \u2502\n\u2502 @task(outlets=  \u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba  \u2502 schedule=[      \u2502\n\u2502   [my_asset])   \u2502              \u2502   my_asset]     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"modules/05-assets-data-aware/exercises/exercise_5_1_producer_consumer/#requirements","title":"Requirements","text":""},{"location":"modules/05-assets-data-aware/exercises/exercise_5_1_producer_consumer/#dag-1-data_producer","title":"DAG 1: data_producer","text":"<ol> <li>DAG ID: <code>exercise_5_1_producer</code></li> <li>Schedule: <code>@hourly</code> (or cron equivalent)</li> <li>Start date: January 1, 2024</li> <li>Tags: <code>[\"exercise\", \"module-05\", \"assets\", \"producer\"]</code></li> <li>Contains a task that:</li> <li>Simulates data processing</li> <li>Produces an Asset with URI <code>s3://data-lake/processed_data</code></li> <li>Logs the processing timestamp</li> </ol>"},{"location":"modules/05-assets-data-aware/exercises/exercise_5_1_producer_consumer/#dag-2-data_consumer","title":"DAG 2: data_consumer","text":"<ol> <li>DAG ID: <code>exercise_5_1_consumer</code></li> <li>Schedule: Triggered by the <code>processed_data</code> Asset</li> <li>Start date: January 1, 2024</li> <li>Tags: <code>[\"exercise\", \"module-05\", \"assets\", \"consumer\"]</code></li> <li>Contains a task that:</li> <li>Prints \"Data is available for processing!\"</li> <li>Logs which Asset triggered the run</li> <li>Accesses triggering event information</li> </ol>"},{"location":"modules/05-assets-data-aware/exercises/exercise_5_1_producer_consumer/#key-code-patterns","title":"Key Code Patterns","text":""},{"location":"modules/05-assets-data-aware/exercises/exercise_5_1_producer_consumer/#defining-an-asset","title":"Defining an Asset","text":"<pre><code>from airflow.sdk import Asset\n\n# Asset URIs are logical identifiers\nmy_asset = Asset(\"s3://bucket/path/to/data\")\n</code></pre>"},{"location":"modules/05-assets-data-aware/exercises/exercise_5_1_producer_consumer/#producer-task","title":"Producer Task","text":"<pre><code>@task(outlets=[my_asset])\ndef produce_data():\n    # Process data...\n    return {\"status\": \"complete\"}\n</code></pre>"},{"location":"modules/05-assets-data-aware/exercises/exercise_5_1_producer_consumer/#consumer-dag-schedule","title":"Consumer DAG Schedule","text":"<pre><code>@dag(\n    schedule=[my_asset],  # Triggers when asset is updated\n)\ndef consumer_dag():\n    pass\n</code></pre>"},{"location":"modules/05-assets-data-aware/exercises/exercise_5_1_producer_consumer/#accessing-triggering-events","title":"Accessing Triggering Events","text":"<pre><code>@task\ndef process_data(**context):\n    triggering_asset_events = context.get(\"triggering_asset_events\", {})\n    for asset, events in triggering_asset_events.items():\n        print(f\"Triggered by: {asset.uri}\")\n</code></pre>"},{"location":"modules/05-assets-data-aware/exercises/exercise_5_1_producer_consumer/#starter-code","title":"Starter Code","text":"<p>See <code>exercise_5_1_producer_consumer_starter.py</code></p>"},{"location":"modules/05-assets-data-aware/exercises/exercise_5_1_producer_consumer/#testing-your-dags","title":"Testing Your DAGs","text":"<pre><code># 1. First, ensure both DAGs are loaded\nairflow dags list | grep exercise_5_1\n\n# 2. Trigger the producer DAG\nairflow dags trigger exercise_5_1_producer\n\n# 3. Check if consumer was triggered\nairflow dags list-runs -d exercise_5_1_consumer\n\n# 4. View Asset in the UI: DAGs \u2192 Assets\n</code></pre>"},{"location":"modules/05-assets-data-aware/exercises/exercise_5_1_producer_consumer/#hints","title":"Hints","text":"Hint 1: Defining the Asset <pre><code>from airflow.sdk import Asset\n\n# Define the Asset that connects producer and consumer\nprocessed_data = Asset(\"s3://data-lake/processed_data\")\n</code></pre>  Both DAGs must reference the SAME Asset (same URI).   Hint 2: Producer task with outlets <pre><code>@task(outlets=[processed_data])\ndef produce_data():\n    print(\"Processing data...\")\n    return {\"timestamp\": datetime.now().isoformat()}\n</code></pre>  The `outlets` parameter signals that this task produces the Asset.   Hint 3: Consumer DAG schedule <pre><code>@dag(\n    dag_id=\"exercise_5_1_consumer\",\n    schedule=[processed_data],  # List of Assets to watch\n    start_date=datetime(2024, 1, 1),\n)\ndef consumer_dag():\n    pass\n</code></pre> Hint 4: Accessing triggering event info <pre><code>@task\ndef consume_data(**context):\n    events = context.get(\"triggering_asset_events\", {})\n    for asset, event_list in events.items():\n        print(f\"Asset: {asset.uri}\")\n        for event in event_list:\n            print(f\"  Timestamp: {event.timestamp}\")\n</code></pre>"},{"location":"modules/05-assets-data-aware/exercises/exercise_5_1_producer_consumer/#success-criteria","title":"Success Criteria","text":"<ul> <li> Both DAGs are registered and visible in the UI</li> <li> Producer DAG runs on schedule and produces the Asset</li> <li> Consumer DAG automatically triggers when producer completes</li> <li> Consumer task can access triggering event information</li> <li> You can see the Asset and its relationships in the Assets view</li> <li> You understand the producer/consumer pattern</li> </ul>"},{"location":"modules/05-assets-data-aware/exercises/exercise_5_1_producer_consumer/#real-world-applications","title":"Real-World Applications","text":"Use Case Producer Consumer ETL Pipeline Data extraction DAG Data transformation DAG ML Pipeline Feature engineering DAG Model training DAG Reporting Data aggregation DAG Report generation DAG Data Quality Ingestion DAG Validation DAG <p>_[DAG]: Directed Acyclic Graph - Airflow's core workflow abstraction _[XCom]: Cross-Communication - Airflow's mechanism for task-to-task data passing _[Executor]: The component that runs tasks (Local, Celery, Kubernetes, etc.) _[Scheduler]: The component that triggers DAGs and determines task execution order _[Webserver]: The component that serves the Airflow UI _[Provider]: An Airflow package that integrates with external systems _[TaskFlow]: The modern Python-native API for defining Airflow tasks using decorators _[Asset]: A logical representation of data that DAGs can produce or consume _[SDK]: Software Development Kit - the airflow.sdk module in Airflow 3.x _[K8s]: Kubernetes - container orchestration platform _[uv]: A fast Python package manager by Astral _[ruff]: A fast Python linter and formatter by Astral _[CI]: Continuous Integration _[CD]: Continuous Deployment *[PR]: Pull Request</p>"},{"location":"modules/05-assets-data-aware/exercises/exercise_5_2_multi_asset/","title":"Exercise 5.2: Multi-Asset Dependencies","text":""},{"location":"modules/05-assets-data-aware/exercises/exercise_5_2_multi_asset/#objective","title":"Objective","text":"<p>Create a pipeline with multiple producer DAGs and a consumer that waits for ALL data sources to be ready before processing.</p>"},{"location":"modules/05-assets-data-aware/exercises/exercise_5_2_multi_asset/#background","title":"Background","text":"<p>Real-world data pipelines often need data from multiple sources before processing can begin. Airflow's Asset system supports multi-Asset dependencies with both AND and OR logic.</p>"},{"location":"modules/05-assets-data-aware/exercises/exercise_5_2_multi_asset/#multi-asset-behavior","title":"Multi-Asset Behavior","text":"<pre><code># AND logic (default) - waits for ALL Assets\nschedule = [asset_a, asset_b, asset_c]\n\n# OR logic - triggers when ANY is updated\nschedule = Asset.any(asset_a, asset_b, asset_c)\n\n# Complex combinations\nschedule = Asset.all(asset_a, Asset.any(asset_b, asset_c))\n</code></pre>"},{"location":"modules/05-assets-data-aware/exercises/exercise_5_2_multi_asset/#requirements","title":"Requirements","text":""},{"location":"modules/05-assets-data-aware/exercises/exercise_5_2_multi_asset/#three-producer-dags","title":"Three Producer DAGs","text":"<p>Create three separate producer DAGs:</p> <ol> <li>orders_producer</li> <li>DAG ID: <code>exercise_5_2_orders_producer</code></li> <li>Asset: <code>postgres://warehouse/orders</code></li> <li> <p>Schedule: <code>None</code> (manual trigger)</p> </li> <li> <p>products_producer</p> </li> <li>DAG ID: <code>exercise_5_2_products_producer</code></li> <li>Asset: <code>postgres://warehouse/products</code></li> <li> <p>Schedule: <code>None</code> (manual trigger)</p> </li> <li> <p>customers_producer</p> </li> <li>DAG ID: <code>exercise_5_2_customers_producer</code></li> <li>Asset: <code>postgres://warehouse/customers</code></li> <li>Schedule: <code>None</code> (manual trigger)</li> </ol>"},{"location":"modules/05-assets-data-aware/exercises/exercise_5_2_multi_asset/#analytics-consumer-dag","title":"Analytics Consumer DAG","text":"<ol> <li>DAG ID: <code>exercise_5_2_analytics_consumer</code></li> <li>Schedule: All three Assets (AND logic)</li> <li>Contains a task that:</li> <li>Logs which Assets triggered the run</li> <li>Simulates running analytics on the combined data</li> <li>Reports completion status</li> </ol>"},{"location":"modules/05-assets-data-aware/exercises/exercise_5_2_multi_asset/#key-concepts","title":"Key Concepts","text":""},{"location":"modules/05-assets-data-aware/exercises/exercise_5_2_multi_asset/#default-and-logic","title":"Default AND Logic","text":"<pre><code># Consumer runs only when ALL Assets are updated\n@dag(\n    schedule=[orders_asset, products_asset, customers_asset],\n)\n</code></pre>"},{"location":"modules/05-assets-data-aware/exercises/exercise_5_2_multi_asset/#accessing-multiple-triggering-events","title":"Accessing Multiple Triggering Events","text":"<pre><code>@task\ndef analyze(**context):\n    events = context.get(\"triggering_asset_events\", {})\n    for asset, event_list in events.items():\n        print(f\"Asset: {asset.uri} - {len(event_list)} events\")\n</code></pre>"},{"location":"modules/05-assets-data-aware/exercises/exercise_5_2_multi_asset/#starter-code","title":"Starter Code","text":"<p>See <code>exercise_5_2_multi_asset_starter.py</code></p>"},{"location":"modules/05-assets-data-aware/exercises/exercise_5_2_multi_asset/#testing-your-dags","title":"Testing Your DAGs","text":"<pre><code># 1. Trigger each producer one at a time\nairflow dags trigger exercise_5_2_orders_producer\nairflow dags trigger exercise_5_2_products_producer\n\n# 2. Consumer should NOT run yet (still waiting for customers)\n\n# 3. Trigger the last producer\nairflow dags trigger exercise_5_2_customers_producer\n\n# 4. Now the consumer should trigger!\nairflow dags list-runs -d exercise_5_2_analytics_consumer\n\n# 5. View Asset dependencies in UI: DAGs \u2192 Assets\n</code></pre>"},{"location":"modules/05-assets-data-aware/exercises/exercise_5_2_multi_asset/#hints","title":"Hints","text":"Hint 1: Defining multiple Assets <pre><code>from airflow.sdk import Asset\n\norders_asset = Asset(\"postgres://warehouse/orders\")\nproducts_asset = Asset(\"postgres://warehouse/products\")\ncustomers_asset = Asset(\"postgres://warehouse/customers\")\n</code></pre> Hint 2: Consumer with multiple Asset dependencies <pre><code>@dag(\n    dag_id=\"exercise_5_2_analytics_consumer\",\n    schedule=[orders_asset, products_asset, customers_asset],\n    # Triggers only when ALL three are updated\n)\ndef analytics_consumer():\n    pass\n</code></pre> Hint 3: Iterating over triggering events <pre><code>@task\ndef run_analytics(**context):\n    events = context.get(\"triggering_asset_events\", {})\n\n    print(\"Triggered by these Assets:\")\n    for asset, event_list in events.items():\n        print(f\"  - {asset.uri}\")\n        for event in event_list:\n            print(f\"      Updated at: {event.timestamp}\")\n            print(f\"      By: {event.source_dag_id}/{event.source_task_id}\")\n</code></pre>"},{"location":"modules/05-assets-data-aware/exercises/exercise_5_2_multi_asset/#success-criteria","title":"Success Criteria","text":"<ul> <li> Three producer DAGs are registered</li> <li> Consumer DAG is registered with multi-Asset schedule</li> <li> Consumer does NOT run until ALL three Assets are updated</li> <li> Consumer correctly logs all triggering events</li> <li> You understand AND vs OR logic for Asset dependencies</li> <li> Assets view shows the dependency graph</li> </ul>"},{"location":"modules/05-assets-data-aware/exercises/exercise_5_2_multi_asset/#challenge-extension","title":"Challenge Extension","text":"<p>Try modifying the consumer to use OR logic instead:</p> <pre><code>from airflow.sdk import Asset\n\n# Consumer triggers when ANY Asset is updated\n@dag(\n    schedule=Asset.any(orders_asset, products_asset, customers_asset),\n)\ndef quick_update_consumer():\n    pass\n</code></pre>"},{"location":"modules/05-assets-data-aware/exercises/exercise_5_2_multi_asset/#real-world-applications","title":"Real-World Applications","text":"Scenario Assets Behavior Daily Report orders + products + customers Wait for all (AND) Real-time Alert any data change Trigger on any (OR) Data Quality raw_data + validation_rules Wait for both (AND) ML Training features + labels Wait for both (AND) <p>_[DAG]: Directed Acyclic Graph - Airflow's core workflow abstraction _[XCom]: Cross-Communication - Airflow's mechanism for task-to-task data passing _[Executor]: The component that runs tasks (Local, Celery, Kubernetes, etc.) _[Scheduler]: The component that triggers DAGs and determines task execution order _[Webserver]: The component that serves the Airflow UI _[Provider]: An Airflow package that integrates with external systems _[TaskFlow]: The modern Python-native API for defining Airflow tasks using decorators _[Asset]: A logical representation of data that DAGs can produce or consume _[SDK]: Software Development Kit - the airflow.sdk module in Airflow 3.x _[K8s]: Kubernetes - container orchestration platform _[uv]: A fast Python package manager by Astral _[ruff]: A fast Python linter and formatter by Astral _[CI]: Continuous Integration _[CD]: Continuous Deployment *[PR]: Pull Request</p>"},{"location":"modules/05-assets-data-aware/exercises/exercise_5_3_asset_decorator/","title":"Exercise 5.3: @asset Decorator Pattern","text":""},{"location":"modules/05-assets-data-aware/exercises/exercise_5_3_asset_decorator/#objective","title":"Objective","text":"<p>Learn to use the <code>@asset</code> decorator introduced in Airflow 3.x for a more Pythonic, asset-centric approach to data pipelines.</p>"},{"location":"modules/05-assets-data-aware/exercises/exercise_5_3_asset_decorator/#background","title":"Background","text":"<p>The <code>@asset</code> decorator combines Asset definition and task implementation in a single, self-documenting unit. This pattern:</p> <ul> <li>Defines the Asset and its producer in one place</li> <li>Shows data dependencies through function parameters</li> <li>Automatically manages scheduling based on upstream Assets</li> <li>Creates cleaner, more maintainable code</li> </ul>"},{"location":"modules/05-assets-data-aware/exercises/exercise_5_3_asset_decorator/#traditional-vs-asset-pattern","title":"Traditional vs @asset Pattern","text":"<p>Traditional Pattern: <pre><code># Asset defined separately\nmy_asset = Asset(\"s3://bucket/data\")\n\n@dag(...)\ndef my_dag():\n    @task(outlets=[my_asset])\n    def produce():\n        pass\n</code></pre></p> <p>@asset Pattern: <pre><code>@asset(\n    uri=\"s3://bucket/data\",\n    schedule=\"@daily\",\n)\ndef my_asset():\n    # This function IS both the asset and its producer\n    pass\n</code></pre></p>"},{"location":"modules/05-assets-data-aware/exercises/exercise_5_3_asset_decorator/#requirements","title":"Requirements","text":"<p>Create an ETL pipeline using the <code>@asset</code> decorator pattern:</p>"},{"location":"modules/05-assets-data-aware/exercises/exercise_5_3_asset_decorator/#asset-a-raw_data","title":"Asset A: raw_data","text":"<ul> <li>URI: <code>s3://data-lake/bronze/raw_data</code></li> <li>Schedule: <code>@daily</code></li> <li>Simulates extracting data from a source API</li> <li>Returns raw data dict</li> </ul>"},{"location":"modules/05-assets-data-aware/exercises/exercise_5_3_asset_decorator/#asset-b-transformed_data","title":"Asset B: transformed_data","text":"<ul> <li>URI: <code>s3://data-lake/silver/transformed_data</code></li> <li>Schedule: Depends on <code>raw_data</code></li> <li>Receives raw_data as input parameter</li> <li>Returns cleaned/transformed data</li> </ul>"},{"location":"modules/05-assets-data-aware/exercises/exercise_5_3_asset_decorator/#asset-c-warehouse_data","title":"Asset C: warehouse_data","text":"<ul> <li>URI: <code>postgres://warehouse/analytics_table</code></li> <li>Schedule: Depends on <code>transformed_data</code></li> <li>Receives transformed_data as input</li> <li>Simulates loading to warehouse</li> </ul>"},{"location":"modules/05-assets-data-aware/exercises/exercise_5_3_asset_decorator/#key-concepts","title":"Key Concepts","text":""},{"location":"modules/05-assets-data-aware/exercises/exercise_5_3_asset_decorator/#basic-asset-decorator","title":"Basic @asset Decorator","text":"<pre><code>from airflow.sdk import asset\n\n@asset(\n    uri=\"s3://data-lake/my_data\",\n    schedule=\"@daily\",\n)\ndef my_data():\n    \"\"\"This function produces the Asset.\"\"\"\n    return process_data()\n</code></pre>"},{"location":"modules/05-assets-data-aware/exercises/exercise_5_3_asset_decorator/#asset-with-dependencies","title":"Asset with Dependencies","text":"<pre><code>@asset(\n    uri=\"s3://data-lake/downstream\",\n    schedule=[my_data],  # Triggered by my_data\n)\ndef downstream_data(my_data):  # Receives upstream Asset as parameter\n    \"\"\"Process the upstream data.\"\"\"\n    return transform(my_data)\n</code></pre>"},{"location":"modules/05-assets-data-aware/exercises/exercise_5_3_asset_decorator/#accessing-asset-definition","title":"Accessing Asset Definition","text":"<pre><code># The decorated function can be used as an Asset reference\n@asset(uri=\"...\", schedule=\"@daily\")\ndef source_data():\n    return data\n\n@asset(uri=\"...\", schedule=[source_data])  # Reference the Asset\ndef consumer(source_data):  # Parameter receives the data\n    pass\n</code></pre>"},{"location":"modules/05-assets-data-aware/exercises/exercise_5_3_asset_decorator/#starter-code","title":"Starter Code","text":"<p>See <code>exercise_5_3_asset_decorator_starter.py</code></p>"},{"location":"modules/05-assets-data-aware/exercises/exercise_5_3_asset_decorator/#testing-your-assets","title":"Testing Your Assets","text":"<pre><code># 1. List all DAGs to see the asset-based DAGs\nairflow dags list | grep exercise_5_3\n\n# 2. Test the pipeline\nairflow dags test exercise_5_3_raw_data 2024-01-15\n\n# 3. View Asset dependencies in UI\n# Navigate to: DAGs \u2192 Assets\n</code></pre>"},{"location":"modules/05-assets-data-aware/exercises/exercise_5_3_asset_decorator/#hints","title":"Hints","text":"Hint 1: Creating the raw_data Asset <pre><code>from airflow.sdk import asset\n\n@asset(\n    uri=\"s3://data-lake/bronze/raw_data\",\n    schedule=\"@daily\",\n)\ndef raw_data():\n    \"\"\"Extract raw data from source.\"\"\"\n    print(\"Extracting raw data...\")\n    return {\n        \"records\": [...],\n        \"extracted_at\": datetime.now().isoformat(),\n    }\n</code></pre> Hint 2: Creating dependent Asset <pre><code>@asset(\n    uri=\"s3://data-lake/silver/transformed_data\",\n    schedule=[raw_data],  # Depends on raw_data Asset\n)\ndef transformed_data(raw_data):  # Receives raw_data output\n    \"\"\"Transform the raw data.\"\"\"\n    print(f\"Transforming {len(raw_data['records'])} records...\")\n    return {\n        \"records\": transformed_records,\n        \"transformations\": [\"clean\", \"normalize\"],\n    }\n</code></pre> Hint 3: Chain of dependencies <pre><code># raw_data \u2192 transformed_data \u2192 warehouse_data\n\n@asset(uri=\"...\", schedule=\"@daily\")\ndef raw_data():\n    return extract()\n\n@asset(uri=\"...\", schedule=[raw_data])\ndef transformed_data(raw_data):\n    return transform(raw_data)\n\n@asset(uri=\"...\", schedule=[transformed_data])\ndef warehouse_data(transformed_data):\n    return load(transformed_data)\n</code></pre>"},{"location":"modules/05-assets-data-aware/exercises/exercise_5_3_asset_decorator/#success-criteria","title":"Success Criteria","text":"<ul> <li> Three Assets are created using @asset decorator</li> <li> raw_data runs on daily schedule</li> <li> transformed_data triggers when raw_data updates</li> <li> warehouse_data triggers when transformed_data updates</li> <li> Data flows correctly through the pipeline</li> <li> You understand the benefits of @asset pattern</li> </ul>"},{"location":"modules/05-assets-data-aware/exercises/exercise_5_3_asset_decorator/#benefits-of-asset-pattern","title":"Benefits of @asset Pattern","text":"Benefit Description Colocation Asset and producer defined together Self-documenting Dependencies visible in function signature Type hints IDE support for data flow Simpler code Less boilerplate than traditional pattern Clear lineage Data flow obvious from code structure"},{"location":"modules/05-assets-data-aware/exercises/exercise_5_3_asset_decorator/#comparison","title":"Comparison","text":""},{"location":"modules/05-assets-data-aware/exercises/exercise_5_3_asset_decorator/#traditional-pattern","title":"Traditional Pattern","text":"<pre><code>raw_asset = Asset(\"s3://bronze/raw\")\ntransformed_asset = Asset(\"s3://silver/transformed\")\n\n@dag(schedule=\"@daily\", ...)\ndef extract_dag():\n    @task(outlets=[raw_asset])\n    def extract():\n        pass\n\n@dag(schedule=[raw_asset], ...)\ndef transform_dag():\n    @task(outlets=[transformed_asset])\n    def transform():\n        pass\n</code></pre>"},{"location":"modules/05-assets-data-aware/exercises/exercise_5_3_asset_decorator/#asset-pattern","title":"@asset Pattern","text":"<pre><code>@asset(uri=\"s3://bronze/raw\", schedule=\"@daily\")\ndef raw_data():\n    pass\n\n@asset(uri=\"s3://silver/transformed\", schedule=[raw_data])\ndef transformed_data(raw_data):\n    pass\n</code></pre> <p>The @asset pattern reduces boilerplate and makes data lineage immediately visible.</p> <p>_[DAG]: Directed Acyclic Graph - Airflow's core workflow abstraction _[XCom]: Cross-Communication - Airflow's mechanism for task-to-task data passing _[Executor]: The component that runs tasks (Local, Celery, Kubernetes, etc.) _[Scheduler]: The component that triggers DAGs and determines task execution order _[Webserver]: The component that serves the Airflow UI _[Provider]: An Airflow package that integrates with external systems _[TaskFlow]: The modern Python-native API for defining Airflow tasks using decorators _[Asset]: A logical representation of data that DAGs can produce or consume _[SDK]: Software Development Kit - the airflow.sdk module in Airflow 3.x _[K8s]: Kubernetes - container orchestration platform _[uv]: A fast Python package manager by Astral _[ruff]: A fast Python linter and formatter by Astral _[CI]: Continuous Integration _[CD]: Continuous Deployment *[PR]: Pull Request</p>"},{"location":"modules/05-assets-data-aware/exercises/exercise_5_4_embedding_assets/","title":"Exercise 5.4: Asset-Driven Embedding Regeneration","text":"<p>Build an asset-driven pipeline that automatically regenerates embeddings when source documents change, demonstrating data-aware scheduling for AI/ML workflows.</p>"},{"location":"modules/05-assets-data-aware/exercises/exercise_5_4_embedding_assets/#learning-goals","title":"Learning Goals","text":"<ul> <li>Use Assets to trigger embedding regeneration automatically</li> <li>Model document-to-embedding dependencies with Assets</li> <li>Implement incremental embedding updates based on asset changes</li> <li>Coordinate producer-consumer patterns for vector store updates</li> </ul>"},{"location":"modules/05-assets-data-aware/exercises/exercise_5_4_embedding_assets/#scenario","title":"Scenario","text":"<p>You're building a document embedding system where:</p> <ol> <li>Source documents are uploaded to a storage location (Asset: <code>documents</code>)</li> <li>Embedding pipeline triggers when documents change (consumes <code>documents</code>)</li> <li>Vector store is updated with new embeddings (Asset: <code>embeddings</code>)</li> <li>Search index rebuilds when embeddings change (consumes <code>embeddings</code>)</li> </ol> <p>This demonstrates how Assets provide data-aware scheduling for ML pipelines.</p>"},{"location":"modules/05-assets-data-aware/exercises/exercise_5_4_embedding_assets/#requirements","title":"Requirements","text":""},{"location":"modules/05-assets-data-aware/exercises/exercise_5_4_embedding_assets/#task-1-document-asset-producer","title":"Task 1: Document Asset Producer","text":"<p>Create a DAG that produces the document asset:</p> <ul> <li>Define an Asset for the document source location</li> <li>Emit the asset when new documents are added</li> <li>Include metadata about the document batch</li> </ul>"},{"location":"modules/05-assets-data-aware/exercises/exercise_5_4_embedding_assets/#task-2-embedding-consumer-producer","title":"Task 2: Embedding Consumer-Producer","text":"<p>Create a DAG that:</p> <ul> <li>Triggers on the <code>documents</code> asset (consumer)</li> <li>Generates embeddings for changed documents</li> <li>Produces the <code>embeddings</code> asset when complete</li> <li>Tracks which documents were processed</li> </ul>"},{"location":"modules/05-assets-data-aware/exercises/exercise_5_4_embedding_assets/#task-3-vector-store-consumer","title":"Task 3: Vector Store Consumer","text":"<p>Create a DAG that:</p> <ul> <li>Triggers on the <code>embeddings</code> asset</li> <li>Updates the vector store with new embeddings</li> <li>Handles incremental updates efficiently</li> <li>Logs update statistics</li> </ul>"},{"location":"modules/05-assets-data-aware/exercises/exercise_5_4_embedding_assets/#task-4-asset-metadata","title":"Task 4: Asset Metadata","text":"<p>Demonstrate asset metadata usage:</p> <ul> <li>Pass document counts through asset metadata</li> <li>Include processing timestamps</li> <li>Enable downstream tasks to access upstream metadata</li> </ul>"},{"location":"modules/05-assets-data-aware/exercises/exercise_5_4_embedding_assets/#success-criteria","title":"Success Criteria","text":"<ul> <li> Document upload triggers embedding generation</li> <li> Embedding completion triggers vector store update</li> <li> Asset dependencies are correctly modeled</li> <li> Metadata flows between producer and consumer</li> <li> Incremental updates work correctly</li> </ul>"},{"location":"modules/05-assets-data-aware/exercises/exercise_5_4_embedding_assets/#hints","title":"Hints","text":"Hint 1: Defining Assets <pre><code>from airflow.sdk import Asset\n\n# Define assets for the pipeline\ndocuments_asset = Asset(\n    uri=\"s3://bucket/documents/\",\n    extra={\"format\": \"pdf\", \"source\": \"upload_system\"},\n)\n\nembeddings_asset = Asset(\n    uri=\"s3://bucket/embeddings/\",\n    extra={\"model\": \"text-embedding-3-small\", \"dimension\": 384},\n)\n</code></pre> Hint 2: Producing Assets <pre><code>from airflow.sdk import Asset, dag, task\n\ndocuments_asset = Asset(\"s3://bucket/documents/\")\n\n\n@dag(schedule=None)\ndef document_producer():\n    @task(outlets=[documents_asset])\n    def upload_documents() -&gt; dict:\n        \"\"\"Upload documents and emit asset.\"\"\"\n        # Process documents\n        docs_processed = 10\n\n        # Return value becomes asset metadata\n        return {\n            \"documents_added\": docs_processed,\n            \"batch_id\": \"batch-001\",\n            \"timestamp\": datetime.now().isoformat(),\n        }\n\n    upload_documents()\n</code></pre> Hint 3: Consuming Assets <pre><code>from airflow.sdk import Asset, dag, task\n\ndocuments_asset = Asset(\"s3://bucket/documents/\")\nembeddings_asset = Asset(\"s3://bucket/embeddings/\")\n\n\n# This DAG triggers when documents_asset is updated\n@dag(schedule=documents_asset)\ndef embedding_generator():\n    @task(outlets=[embeddings_asset])\n    def generate_embeddings(**context) -&gt; dict:\n        \"\"\"Generate embeddings for new documents.\"\"\"\n        # Access triggering asset info\n        triggering_asset_events = context.get(\"triggering_asset_events\", {})\n\n        for asset_uri, events in triggering_asset_events.items():\n            for event in events:\n                # Access metadata from producer\n                batch_id = event.extra.get(\"batch_id\")\n                doc_count = event.extra.get(\"documents_added\", 0)\n\n                logger.info(f\"Processing batch {batch_id} with {doc_count} docs\")\n\n        return {\"embeddings_generated\": 10}\n\n    generate_embeddings()\n</code></pre> Hint 4: Asset Chain <pre><code>from airflow.sdk import Asset, dag, task\n\nembeddings_asset = Asset(\"s3://bucket/embeddings/\")\n\n\n# Final consumer in the chain\n@dag(schedule=embeddings_asset)\ndef vector_store_updater():\n    @task\n    def update_vector_store(**context) -&gt; dict:\n        \"\"\"Update vector store with new embeddings.\"\"\"\n        events = context.get(\"triggering_asset_events\", {})\n\n        total_embeddings = 0\n        for asset_uri, asset_events in events.items():\n            for event in asset_events:\n                total_embeddings += event.extra.get(\"embeddings_generated\", 0)\n\n        logger.info(f\"Updating vector store with {total_embeddings} embeddings\")\n\n        return {\n            \"vectors_upserted\": total_embeddings,\n            \"update_timestamp\": datetime.now().isoformat(),\n        }\n\n    update_vector_store()\n</code></pre>"},{"location":"modules/05-assets-data-aware/exercises/exercise_5_4_embedding_assets/#files","title":"Files","text":"<ul> <li>Starter: <code>exercise_5_4_embedding_assets_starter.py</code></li> <li>Solution: <code>../solutions/solution_5_4_embedding_assets.py</code></li> </ul>"},{"location":"modules/05-assets-data-aware/exercises/exercise_5_4_embedding_assets/#estimated-time","title":"Estimated Time","text":"<p>45-60 minutes</p>"},{"location":"modules/05-assets-data-aware/exercises/exercise_5_4_embedding_assets/#next-steps","title":"Next Steps","text":"<p>After completing this exercise:</p> <ol> <li>Add error handling for failed embedding generation</li> <li>Implement partial updates (only changed documents)</li> <li>Add monitoring for asset event delays</li> <li>See Module 15 for complete RAG pipeline patterns</li> </ol> <p>_[DAG]: Directed Acyclic Graph - Airflow's core workflow abstraction _[XCom]: Cross-Communication - Airflow's mechanism for task-to-task data passing _[Executor]: The component that runs tasks (Local, Celery, Kubernetes, etc.) _[Scheduler]: The component that triggers DAGs and determines task execution order _[Webserver]: The component that serves the Airflow UI _[Provider]: An Airflow package that integrates with external systems _[TaskFlow]: The modern Python-native API for defining Airflow tasks using decorators _[Asset]: A logical representation of data that DAGs can produce or consume _[SDK]: Software Development Kit - the airflow.sdk module in Airflow 3.x _[K8s]: Kubernetes - container orchestration platform _[uv]: A fast Python package manager by Astral _[ruff]: A fast Python linter and formatter by Astral _[CI]: Continuous Integration _[CD]: Continuous Deployment *[PR]: Pull Request</p>"},{"location":"modules/06-dynamic-tasks/","title":"Module 06: Dynamic Task Mapping","text":""},{"location":"modules/06-dynamic-tasks/#learning-objectives","title":"\ud83c\udfaf Learning Objectives","text":"<p>By the end of this module, you will:</p> <ul> <li>Create dynamic tasks that expand at runtime</li> <li>Use <code>expand()</code> for parallel processing over collections</li> <li>Combine <code>partial()</code> with <code>expand()</code> for complex patterns</li> <li>Aggregate results from mapped tasks</li> <li>Handle real-world scenarios like file processing and API pagination</li> </ul>"},{"location":"modules/06-dynamic-tasks/#estimated-time-4-5-hours","title":"\u23f1\ufe0f Estimated Time: 4-5 hours","text":""},{"location":"modules/06-dynamic-tasks/#1-why-dynamic-tasks","title":"1. Why Dynamic Tasks?","text":"<p>Sometimes you don't know at DAG authoring time how many tasks you need:</p> <ul> <li>Process each file in a directory (unknown count)</li> <li>Call API for each item in a list</li> <li>Run analysis for each partition</li> <li>Process each record from a query</li> </ul> <p>Before Airflow 2.3: You had to use hacky workarounds or know counts upfront. Now: Dynamic Task Mapping (DTM) handles this elegantly.</p>"},{"location":"modules/06-dynamic-tasks/#2-basic-expand-usage","title":"2. Basic expand() Usage","text":""},{"location":"modules/06-dynamic-tasks/#simple-expansion","title":"Simple Expansion","text":"<pre><code>from datetime import datetime\n\nfrom airflow.sdk import DAG, task\n\nwith DAG(dag_id=\"dynamic_basic\", start_date=datetime(2024, 1, 1), schedule=None):\n\n    @task\n    def get_files():\n        \"\"\"Returns a list of files to process\"\"\"\n        return [\"file1.csv\", \"file2.csv\", \"file3.csv\", \"file4.csv\"]\n\n    @task\n    def process_file(filename: str):\n        \"\"\"Process a single file - runs once per file\"\"\"\n        print(f\"Processing: {filename}\")\n        return {\"file\": filename, \"status\": \"done\"}\n\n    # The magic: expand() creates one task instance per item\n    files = get_files()\n    process_file.expand(filename=files)\n</code></pre> <p>Result: 4 parallel task instances, one for each file.</p>"},{"location":"modules/06-dynamic-tasks/#expanding-over-multiple-arguments","title":"Expanding Over Multiple Arguments","text":"<pre><code>@task\ndef process_item(item_id: int):\n    return item_id * 2\n\n\n# Expand over a list directly\nprocess_item.expand(item_id=[1, 2, 3, 4, 5])\n\n\n# Or from upstream task output\n@task\ndef get_ids():\n    return [10, 20, 30]\n\n\nids = get_ids()\nprocess_item.expand(item_id=ids)\n</code></pre>"},{"location":"modules/06-dynamic-tasks/#3-partial-expand-pattern","title":"3. partial() + expand() Pattern","text":"<p>Use <code>partial()</code> to fix some arguments while expanding others:</p> <pre><code>@task\ndef process_data(table: str, partition_date: str, mode: str):\n    \"\"\"Process a specific table partition\"\"\"\n    print(f\"Processing {table} for {partition_date} in {mode} mode\")\n    return {\"table\": table, \"date\": partition_date}\n\n\nwith DAG(...):\n    # Fixed arguments + dynamic arguments\n    process_data.partial(\n        mode=\"incremental\"  # Same for all\n    ).expand(\n        table=[\"users\", \"orders\", \"products\"],  # One task per table\n        partition_date=[\"2024-01-01\", \"2024-01-02\"],  # Cross-product!\n    )\n</code></pre> <p>\u26a0\ufe0f Warning: When expanding multiple arguments, you get the Cartesian product:</p> <ul> <li>3 tables \u00d7 2 dates = 6 task instances</li> </ul>"},{"location":"modules/06-dynamic-tasks/#explicit-combinations-with-expand_kwargs","title":"Explicit Combinations with expand_kwargs","text":"<p>To avoid Cartesian product, use <code>expand_kwargs</code>:</p> <pre><code>@task\ndef process(table: str, date: str, priority: int):\n    return f\"{table}:{date}:{priority}\"\n\n\n# Define explicit combinations\ncombinations = [\n    {\"table\": \"users\", \"date\": \"2024-01-01\", \"priority\": 1},\n    {\"table\": \"orders\", \"date\": \"2024-01-01\", \"priority\": 2},\n    {\"table\": \"products\", \"date\": \"2024-01-02\", \"priority\": 1},\n]\n\nprocess.expand_kwargs(combinations)\n# Creates exactly 3 task instances\n</code></pre>"},{"location":"modules/06-dynamic-tasks/#4-aggregating-mapped-results","title":"4. Aggregating Mapped Results","text":"<p>After parallel processing, you often need to aggregate:</p> <pre><code>from datetime import datetime\n\nfrom airflow.sdk import DAG, task\n\nwith DAG(dag_id=\"aggregate_example\", start_date=datetime(2024, 1, 1), schedule=None):\n\n    @task\n    def get_numbers():\n        return [1, 2, 3, 4, 5]\n\n    @task\n    def square(x: int) -&gt; int:\n        return x**2\n\n    @task\n    def sum_results(results: list[int]) -&gt; int:\n        \"\"\"Receives ALL mapped outputs as a list\"\"\"\n        total = sum(results)\n        print(f\"Sum of squares: {total}\")\n        return total\n\n    numbers = get_numbers()\n    squared = square.expand(x=numbers)  # 5 parallel tasks\n    sum_results(squared)  # Gets [1, 4, 9, 16, 25], returns 55\n</code></pre>"},{"location":"modules/06-dynamic-tasks/#aggregation-rules","title":"Aggregation Rules","text":"<p>When a downstream task receives a mapped task's output:</p> <ul> <li>Unmapped downstream: Receives a list of all outputs</li> <li>Mapped downstream: Each instance receives one output (zip behavior)</li> </ul>"},{"location":"modules/06-dynamic-tasks/#5-map-index-and-task-context","title":"5. Map Index and Task Context","text":"<p>Access information about which map instance you're in:</p> <pre><code>from airflow.sdk import get_current_context, task\n\n\n@task\ndef indexed_task(item: str):\n    context = get_current_context()\n    map_index = context[\"ti\"].map_index\n\n    print(f\"Processing item {map_index}: {item}\")\n    return {\"index\": map_index, \"item\": item}\n</code></pre>"},{"location":"modules/06-dynamic-tasks/#using-map_index-for-partitioning","title":"Using map_index for Partitioning","text":"<pre><code>@task\ndef process_partition(partition_id: int):\n    context = get_current_context()\n    total_partitions = context[\"ti\"].map_count or 1\n\n    print(f\"Processing partition {partition_id + 1} of {total_partitions}\")\n    return partition_id\n\n\n# Create 10 partitions\nprocess_partition.expand(partition_id=list(range(10)))\n</code></pre>"},{"location":"modules/06-dynamic-tasks/#6-real-world-patterns","title":"6. Real-World Patterns","text":""},{"location":"modules/06-dynamic-tasks/#pattern-1-process-files-in-s3","title":"Pattern 1: Process Files in S3","text":"<pre><code>from airflow.providers.amazon.aws.hooks.s3 import S3Hook\nfrom airflow.sdk import DAG, task\n\n\n@task\ndef list_s3_files(bucket: str, prefix: str) -&gt; list[str]:\n    \"\"\"List all files matching prefix\"\"\"\n    hook = S3Hook(aws_conn_id=\"aws_default\")\n    keys = hook.list_keys(bucket_name=bucket, prefix=prefix)\n    return keys or []\n\n\n@task\ndef process_s3_file(bucket: str, key: str) -&gt; dict:\n    \"\"\"Process a single S3 file\"\"\"\n    hook = S3Hook(aws_conn_id=\"aws_default\")\n    content = hook.read_key(key=key, bucket_name=bucket)\n    # Process content...\n    return {\"key\": key, \"size\": len(content)}\n\n\n@task\ndef summarize(results: list[dict]):\n    \"\"\"Aggregate processing results\"\"\"\n    total_size = sum(r[\"size\"] for r in results)\n    print(f\"Processed {len(results)} files, total {total_size} bytes\")\n\n\nwith DAG(...):\n    bucket = \"my-bucket\"\n    files = list_s3_files(bucket, \"data/incoming/\")\n    processed = process_s3_file.partial(bucket=bucket).expand(key=files)\n    summarize(processed)\n</code></pre>"},{"location":"modules/06-dynamic-tasks/#pattern-2-paginated-api-calls","title":"Pattern 2: Paginated API Calls","text":"<pre><code>@task\ndef get_page_count() -&gt; int:\n    \"\"\"Determine how many pages to fetch\"\"\"\n    response = requests.head(\"https://api.example.com/users\")\n    total = int(response.headers.get(\"X-Total-Count\", 100))\n    page_size = 50\n    return (total + page_size - 1) // page_size\n\n\n@task\ndef fetch_page(page: int) -&gt; list[dict]:\n    \"\"\"Fetch a single page\"\"\"\n    response = requests.get(\"https://api.example.com/users\", params={\"page\": page, \"limit\": 50})\n    return response.json()\n\n\n@task\ndef combine_results(pages: list[list[dict]]) -&gt; list[dict]:\n    \"\"\"Flatten all pages into single list\"\"\"\n    return [item for page in pages for item in page]\n\n\nwith DAG(...):\n    page_count = get_page_count()\n    # Generate page numbers dynamically\n    page_numbers = list(range(1, page_count + 1))  # Note: This won't work!\n\n    # Instead, use expand with range from task output\n    pages = fetch_page.expand(page=...)  # See note below\n    combine_results(pages)\n</code></pre> <p>Note: For truly dynamic range generation, you need:</p> <pre><code>@task\ndef generate_page_range(count: int) -&gt; list[int]:\n    return list(range(1, count + 1))\n\n\npage_count = get_page_count()\npages = generate_page_range(page_count)\nfetch_page.expand(page=pages)\n</code></pre>"},{"location":"modules/06-dynamic-tasks/#pattern-3-database-partitions","title":"Pattern 3: Database Partitions","text":"<pre><code>@task\ndef get_partitions() -&gt; list[str]:\n    \"\"\"Get list of date partitions to process\"\"\"\n    hook = PostgresHook(postgres_conn_id=\"warehouse\")\n    result = hook.get_records(\"\"\"\n        SELECT DISTINCT partition_date\n        FROM events\n        WHERE processed = false\n        ORDER BY partition_date\n    \"\"\")\n    return [row[0] for row in result]\n\n\n@task\ndef process_partition(partition_date: str):\n    \"\"\"Process a single partition\"\"\"\n    hook = PostgresHook(postgres_conn_id=\"warehouse\")\n    hook.run(f\"\"\"\n        INSERT INTO events_processed\n        SELECT * FROM events WHERE partition_date = '{partition_date}'\n    \"\"\")\n    return partition_date\n\n\nwith DAG(...):\n    partitions = get_partitions()\n    process_partition.expand(partition_date=partitions)\n</code></pre>"},{"location":"modules/06-dynamic-tasks/#7-limits-and-best-practices","title":"7. Limits and Best Practices","text":""},{"location":"modules/06-dynamic-tasks/#setting-concurrency-limits","title":"Setting Concurrency Limits","text":"<pre><code># Limit parallel mapped instances\n@task(max_active_tis_per_dag=10)  # Max 10 concurrent instances\ndef process(item: str):\n    pass\n\n\nprocess.expand(item=large_list)\n</code></pre>"},{"location":"modules/06-dynamic-tasks/#empty-list-handling","title":"Empty List Handling","text":"<pre><code>@task\ndef might_be_empty() -&gt; list[str]:\n    return []  # Returns empty list\n\n\n@task\ndef process(item: str):\n    pass\n\n\n# With empty list: task is marked as skipped\nprocess.expand(item=might_be_empty())\n</code></pre>"},{"location":"modules/06-dynamic-tasks/#best-practices","title":"Best Practices","text":"<ol> <li>Keep mapped tasks idempotent - They may be retried individually</li> <li>Limit expansion size - Thousands of mapped tasks impact scheduler</li> <li>Use appropriate parallelism - Consider pool limits</li> <li>Handle empty expansions - Use trigger rules appropriately</li> </ol>"},{"location":"modules/06-dynamic-tasks/#exercises","title":"\ud83d\udcdd Exercises","text":""},{"location":"modules/06-dynamic-tasks/#exercise-61-basic-mapping","title":"Exercise 6.1: Basic Mapping","text":"<p>Create a DAG that:</p> <ol> <li>Returns a list of 10 random numbers</li> <li>Squares each number in parallel</li> <li>Sums all the squares</li> </ol>"},{"location":"modules/06-dynamic-tasks/#exercise-62-file-processing-simulation","title":"Exercise 6.2: File Processing Simulation","text":"<p>Create a DAG that:</p> <ol> <li>Simulates listing files (return list of filenames)</li> <li>Processes each file (add metadata like size, type)</li> <li>Generates a summary report</li> </ol>"},{"location":"modules/06-dynamic-tasks/#exercise-63-cross-product-processing","title":"Exercise 6.3: Cross-Product Processing","text":"<p>Create a DAG with:</p> <ul> <li>3 regions: [\"us\", \"eu\", \"apac\"]</li> <li>4 product types: [\"widget\", \"gadget\", \"doohickey\", \"thingamajig\"]</li> <li>A task that runs for each combination (12 total)</li> <li>An aggregation task that summarizes all results</li> </ul>"},{"location":"modules/06-dynamic-tasks/#checkpoint","title":"\u2705 Checkpoint","text":"<p>Before moving to Module 07, ensure you can:</p> <ul> <li> Use <code>expand()</code> for parallel task execution</li> <li> Combine <code>partial()</code> with <code>expand()</code> for mixed parameters</li> <li> Use <code>expand_kwargs()</code> for explicit combinations</li> <li> Aggregate results from mapped tasks</li> <li> Access map_index in task context</li> <li> Handle edge cases (empty lists, limits)</li> </ul>"},{"location":"modules/06-dynamic-tasks/#industry-spotlight-netflix","title":"\ud83c\udfed Industry Spotlight: Netflix","text":"<p>How Netflix Uses Dynamic Mapping for Video Encoding</p> <p>Netflix processes thousands of video assets daily, each requiring encoding into multiple resolutions and formats. Dynamic task mapping enables horizontal scaling without code changes:</p> Challenge Dynamic Mapping Solution Variable workloads <code>expand()</code> creates tasks per video/format Resource optimization Parallel encoding maximizes GPU utilization Format explosion One task definition, unlimited combinations Failure isolation Individual encode failures don't block others <p>Pattern in Use: Netflix-style video encoding pipeline:</p> <pre><code>@task\ndef get_encoding_jobs(video_id: str) -&gt; list[dict]:\n    \"\"\"Generate encoding configurations for one video.\"\"\"\n    formats = [\"4k_hdr\", \"1080p\", \"720p\", \"480p\"]\n    codecs = [\"h265\", \"av1\"]\n    return [{\"video_id\": video_id, \"format\": f, \"codec\": c} for f in formats for c in codecs]\n\n\n@task\ndef encode_video(job: dict) -&gt; dict:\n    \"\"\"Encode single video/format combination.\"\"\"\n    return run_encoder(job[\"video_id\"], job[\"format\"], job[\"codec\"])\n\n\n# Dynamic expansion: 1 video \u2192 8 parallel encode tasks\njobs = get_encoding_jobs(\"movie_12345\")\nresults = encode_video.expand(job=jobs)\n</code></pre> <p>Key Insight: Dynamic mapping reduced Netflix's encoding pipeline complexity by 80% while improving throughput 3x through automatic parallelization.</p> <p>\ud83d\udcd6 Related Exercise: Exercise 6.4: Parallel Embeddings - Apply dynamic mapping to AI/ML workloads</p>"},{"location":"modules/06-dynamic-tasks/#further-reading","title":"\ud83d\udcda Further Reading","text":"<ul> <li>Dynamic Task Mapping</li> <li>Task Expansion Patterns</li> <li>Case Study: Spotify Recommendations</li> </ul> <p>Next: Module 07: Testing &amp; Debugging \u2192</p> <p>_[DAG]: Directed Acyclic Graph - Airflow's core workflow abstraction _[XCom]: Cross-Communication - Airflow's mechanism for task-to-task data passing _[Executor]: The component that runs tasks (Local, Celery, Kubernetes, etc.) _[Scheduler]: The component that triggers DAGs and determines task execution order _[Webserver]: The component that serves the Airflow UI _[Provider]: An Airflow package that integrates with external systems _[TaskFlow]: The modern Python-native API for defining Airflow tasks using decorators _[Asset]: A logical representation of data that DAGs can produce or consume _[SDK]: Software Development Kit - the airflow.sdk module in Airflow 3.x _[K8s]: Kubernetes - container orchestration platform _[uv]: A fast Python package manager by Astral _[ruff]: A fast Python linter and formatter by Astral _[CI]: Continuous Integration _[CD]: Continuous Deployment *[PR]: Pull Request</p>"},{"location":"modules/06-dynamic-tasks/exercises/exercise_6_1_basic_mapping/","title":"Exercise 6.1: Basic Mapping","text":""},{"location":"modules/06-dynamic-tasks/exercises/exercise_6_1_basic_mapping/#objective","title":"Objective","text":"<p>Learn the fundamentals of Dynamic Task Mapping using <code>expand()</code> to process a list of items in parallel.</p>"},{"location":"modules/06-dynamic-tasks/exercises/exercise_6_1_basic_mapping/#background","title":"Background","text":"<p>Dynamic Task Mapping (DTM) allows you to create tasks at runtime based on data. Instead of defining a fixed number of tasks, you can:</p> <ul> <li>Process an unknown number of items</li> <li>Run tasks in parallel automatically</li> <li>Aggregate results from all mapped instances</li> </ul>"},{"location":"modules/06-dynamic-tasks/exercises/exercise_6_1_basic_mapping/#the-expand-pattern","title":"The expand() Pattern","text":"<pre><code>@task\ndef process(item: str):\n    return f\"Processed: {item}\"\n\n# Creates one task instance per item in the list\nprocess.expand(item=[\"a\", \"b\", \"c\"])  # 3 parallel tasks\n</code></pre>"},{"location":"modules/06-dynamic-tasks/exercises/exercise_6_1_basic_mapping/#requirements","title":"Requirements","text":"<p>Create a DAG that demonstrates the map-reduce pattern:</p> <ol> <li>DAG ID: <code>exercise_6_1_basic_mapping</code></li> <li>Schedule: <code>None</code> (manual trigger)</li> <li>Start date: January 1, 2024</li> <li>Tags: <code>[\"exercise\", \"module-06\", \"dynamic-tasks\"]</code></li> </ol>"},{"location":"modules/06-dynamic-tasks/exercises/exercise_6_1_basic_mapping/#tasks","title":"Tasks","text":"<ol> <li> <p>generate_numbers: Generate a list of 10 random integers (1-100)</p> </li> <li> <p>square_number: Receives a single number, returns its square</p> </li> <li>Use <code>expand()</code> to create parallel instances</li> <li> <p>Each instance processes one number</p> </li> <li> <p>sum_squares: Receives all squared results, calculates the sum</p> </li> <li>This demonstrates result aggregation</li> <li>Receives a list of all mapped task outputs</li> </ol>"},{"location":"modules/06-dynamic-tasks/exercises/exercise_6_1_basic_mapping/#expected-flow","title":"Expected Flow","text":"<pre><code>generate_numbers \u2192 square_number (\u00d710 parallel) \u2192 sum_squares\n     \u2502                    \u2502                           \u2502\n     \u2502                    \u251c\u2500 square[0]: 4\u00b2 = 16       \u2502\n     \u2502                    \u251c\u2500 square[1]: 7\u00b2 = 49       \u2502\n     \u2502                    \u251c\u2500 square[2]: 2\u00b2 = 4        \u2502\n     \u2502                    \u2514\u2500 ...                      \u2502\n     \u2502                                                \u2502\n     \u2514\u2500 [4, 7, 2, ...]        [16, 49, 4, ...] \u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                Sum = ?\n</code></pre>"},{"location":"modules/06-dynamic-tasks/exercises/exercise_6_1_basic_mapping/#key-concepts","title":"Key Concepts","text":""},{"location":"modules/06-dynamic-tasks/exercises/exercise_6_1_basic_mapping/#expanding-over-task-output","title":"Expanding Over Task Output","text":"<pre><code>@task\ndef get_items():\n    return [1, 2, 3]\n\n@task\ndef process(item: int):\n    return item * 2\n\n# expand() works with task outputs\nitems = get_items()\nprocess.expand(item=items)\n</code></pre>"},{"location":"modules/06-dynamic-tasks/exercises/exercise_6_1_basic_mapping/#aggregating-results","title":"Aggregating Results","text":"<pre><code>@task\ndef aggregate(results: list[int]):\n    # Receives ALL outputs from mapped tasks as a list\n    return sum(results)\n\n# Wire up aggregation\nsquared_results = square.expand(x=numbers)\naggregate(squared_results)  # Gets [result1, result2, ...]\n</code></pre>"},{"location":"modules/06-dynamic-tasks/exercises/exercise_6_1_basic_mapping/#starter-code","title":"Starter Code","text":"<p>See <code>exercise_6_1_basic_mapping_starter.py</code></p>"},{"location":"modules/06-dynamic-tasks/exercises/exercise_6_1_basic_mapping/#testing-your-dag","title":"Testing Your DAG","text":"<pre><code># Test the DAG\nairflow dags test exercise_6_1_basic_mapping 2024-01-15\n\n# In the UI, you'll see:\n# - generate_numbers: 1 instance\n# - square_number: 10 instances (one per number)\n# - sum_squares: 1 instance\n</code></pre>"},{"location":"modules/06-dynamic-tasks/exercises/exercise_6_1_basic_mapping/#hints","title":"Hints","text":"Hint 1: Generating random numbers <pre><code>import random\n\n@task\ndef generate_numbers() -&gt; list[int]:\n    return [random.randint(1, 100) for _ in range(10)]\n</code></pre> Hint 2: Using expand() <pre><code>@task\ndef square_number(x: int) -&gt; int:\n    return x ** 2\n\nnumbers = generate_numbers()\nsquared = square_number.expand(x=numbers)\n</code></pre> Hint 3: Aggregating results <pre><code>@task\ndef sum_squares(squares: list[int]) -&gt; int:\n    total = sum(squares)\n    print(f\"Sum of squares: {total}\")\n    return total\n\n# squared is the mapped task output - passes as list\nsum_squares(squared)\n</code></pre>"},{"location":"modules/06-dynamic-tasks/exercises/exercise_6_1_basic_mapping/#success-criteria","title":"Success Criteria","text":"<ul> <li> DAG generates 10 random numbers</li> <li> 10 parallel task instances process the numbers</li> <li> Each instance correctly squares its input</li> <li> Aggregation task receives all results as a list</li> <li> Final sum is calculated correctly</li> <li> You can view individual mapped instances in the UI</li> </ul>"},{"location":"modules/06-dynamic-tasks/exercises/exercise_6_1_basic_mapping/#understanding-the-ui","title":"Understanding the UI","text":"<p>When you run this DAG in the UI:</p> <ol> <li>Grid View: Shows <code>square_number</code> as a single row, but with 10 instances</li> <li>Click the task: See <code>[0]</code>, <code>[1]</code>, <code>[2]</code> etc. for each mapped instance</li> <li>Each instance has: Its own logs, XCom, and status</li> </ol>"},{"location":"modules/06-dynamic-tasks/exercises/exercise_6_1_basic_mapping/#real-world-applications","title":"Real-World Applications","text":"Scenario Pattern Process files expand over file list API pagination expand over page numbers Database shards expand over shard IDs ML hyperparameters expand over param combinations <p>_[DAG]: Directed Acyclic Graph - Airflow's core workflow abstraction _[XCom]: Cross-Communication - Airflow's mechanism for task-to-task data passing _[Executor]: The component that runs tasks (Local, Celery, Kubernetes, etc.) _[Scheduler]: The component that triggers DAGs and determines task execution order _[Webserver]: The component that serves the Airflow UI _[Provider]: An Airflow package that integrates with external systems _[TaskFlow]: The modern Python-native API for defining Airflow tasks using decorators _[Asset]: A logical representation of data that DAGs can produce or consume _[SDK]: Software Development Kit - the airflow.sdk module in Airflow 3.x _[K8s]: Kubernetes - container orchestration platform _[uv]: A fast Python package manager by Astral _[ruff]: A fast Python linter and formatter by Astral _[CI]: Continuous Integration _[CD]: Continuous Deployment *[PR]: Pull Request</p>"},{"location":"modules/06-dynamic-tasks/exercises/exercise_6_2_file_processing/","title":"Exercise 6.2: File Processing Simulation","text":""},{"location":"modules/06-dynamic-tasks/exercises/exercise_6_2_file_processing/#objective","title":"Objective","text":"<p>Create a DAG that simulates a real-world file processing pipeline using Dynamic Task Mapping.</p>"},{"location":"modules/06-dynamic-tasks/exercises/exercise_6_2_file_processing/#background","title":"Background","text":"<p>One of the most common use cases for Dynamic Task Mapping is processing files: - You don't know how many files exist until runtime - Each file can be processed independently - Results need to be combined into a summary</p>"},{"location":"modules/06-dynamic-tasks/exercises/exercise_6_2_file_processing/#real-world-scenarios","title":"Real-World Scenarios","text":"Scenario Files Processing Data Lake Ingestion CSV/Parquet files Validate and load Log Analysis Log files Parse and aggregate Image Processing Images Resize/compress Report Generation Data files Transform and merge"},{"location":"modules/06-dynamic-tasks/exercises/exercise_6_2_file_processing/#requirements","title":"Requirements","text":"<p>Create a DAG that simulates file processing:</p> <ol> <li>DAG ID: <code>exercise_6_2_file_processing</code></li> <li>Schedule: <code>None</code> (manual trigger)</li> <li>Start date: January 1, 2024</li> <li>Tags: <code>[\"exercise\", \"module-06\", \"dynamic-tasks\", \"files\"]</code></li> </ol>"},{"location":"modules/06-dynamic-tasks/exercises/exercise_6_2_file_processing/#tasks","title":"Tasks","text":"<ol> <li>list_files: Simulate listing files from a directory</li> <li>Return a list of file names (e.g., <code>[\"file_001.csv\", \"file_002.csv\", ...]</code>)</li> <li> <p>Generate 5-10 random file names</p> </li> <li> <p>process_file: Process a single file (mapped task)</p> </li> <li>Receives one filename</li> <li>Simulates extracting metadata (size, type, rows)</li> <li> <p>Returns a dict with file info</p> </li> <li> <p>generate_report: Aggregate all processed files</p> </li> <li>Receives all file processing results</li> <li>Generates a summary report</li> <li>Calculates totals (total size, total rows, etc.)</li> </ol>"},{"location":"modules/06-dynamic-tasks/exercises/exercise_6_2_file_processing/#expected-output","title":"Expected Output","text":"<pre><code># generate_report should produce something like:\n{\n    \"total_files\": 8,\n    \"total_size_mb\": 156.4,\n    \"total_rows\": 45000,\n    \"file_types\": {\"csv\": 5, \"json\": 3},\n    \"processing_time\": \"2.3s\"\n}\n</code></pre>"},{"location":"modules/06-dynamic-tasks/exercises/exercise_6_2_file_processing/#key-concepts","title":"Key Concepts","text":""},{"location":"modules/06-dynamic-tasks/exercises/exercise_6_2_file_processing/#accessing-map-index","title":"Accessing Map Index","text":"<p>Each mapped task instance knows its index:</p> <pre><code>@task\ndef process_file(filename: str, **context):\n    # Access the map index\n    map_index = context.get(\"map_index\", 0)\n    print(f\"Processing file {map_index}: {filename}\")\n</code></pre>"},{"location":"modules/06-dynamic-tasks/exercises/exercise_6_2_file_processing/#returning-rich-data","title":"Returning Rich Data","text":"<p>Mapped tasks can return complex dictionaries:</p> <pre><code>@task\ndef process_file(filename: str) -&gt; dict:\n    return {\n        \"filename\": filename,\n        \"size_mb\": 12.5,\n        \"row_count\": 5000,\n        \"status\": \"success\"\n    }\n</code></pre>"},{"location":"modules/06-dynamic-tasks/exercises/exercise_6_2_file_processing/#starter-code","title":"Starter Code","text":"<p>See <code>exercise_6_2_file_processing_starter.py</code></p>"},{"location":"modules/06-dynamic-tasks/exercises/exercise_6_2_file_processing/#testing-your-dag","title":"Testing Your DAG","text":"<pre><code># Test the DAG\nairflow dags test exercise_6_2_file_processing 2024-01-15\n\n# Check logs for each file processing instance\n# The report task should show aggregated statistics\n</code></pre>"},{"location":"modules/06-dynamic-tasks/exercises/exercise_6_2_file_processing/#hints","title":"Hints","text":"Hint 1: Generating file list <pre><code>import random\n\n@task\ndef list_files() -&gt; list[str]:\n    num_files = random.randint(5, 10)\n    extensions = [\"csv\", \"json\", \"parquet\"]\n\n    files = []\n    for i in range(num_files):\n        ext = random.choice(extensions)\n        files.append(f\"data_{i:03d}.{ext}\")\n\n    return files\n</code></pre> Hint 2: Processing with metadata <pre><code>import random\n\n@task\ndef process_file(filename: str) -&gt; dict:\n    # Simulate processing\n    extension = filename.split(\".\")[-1]\n\n    return {\n        \"filename\": filename,\n        \"extension\": extension,\n        \"size_mb\": round(random.uniform(1, 50), 2),\n        \"row_count\": random.randint(100, 10000),\n        \"status\": \"processed\",\n    }\n</code></pre> Hint 3: Aggregating file results <pre><code>@task\ndef generate_report(file_results: list[dict]) -&gt; dict:\n    total_size = sum(f[\"size_mb\"] for f in file_results)\n    total_rows = sum(f[\"row_count\"] for f in file_results)\n\n    # Count by extension\n    ext_counts = {}\n    for f in file_results:\n        ext = f[\"extension\"]\n        ext_counts[ext] = ext_counts.get(ext, 0) + 1\n\n    return {\n        \"total_files\": len(file_results),\n        \"total_size_mb\": round(total_size, 2),\n        \"total_rows\": total_rows,\n        \"by_extension\": ext_counts,\n    }\n</code></pre>"},{"location":"modules/06-dynamic-tasks/exercises/exercise_6_2_file_processing/#success-criteria","title":"Success Criteria","text":"<ul> <li> DAG generates a variable number of files (5-10)</li> <li> Each file is processed by a separate task instance</li> <li> File processing extracts simulated metadata</li> <li> Report correctly aggregates all file statistics</li> <li> You understand how to access map_index</li> <li> Results are properly typed and structured</li> </ul>"},{"location":"modules/06-dynamic-tasks/exercises/exercise_6_2_file_processing/#extension-challenge","title":"Extension Challenge","text":"<p>Try adding error handling:</p> <pre><code>@task\ndef process_file(filename: str) -&gt; dict:\n    import random\n\n    # Simulate occasional failures\n    if random.random() &lt; 0.1:\n        raise ValueError(f\"Failed to process {filename}\")\n\n    # Normal processing...\n</code></pre> <p>Then add a downstream task with <code>trigger_rule=TriggerRule.ALL_DONE</code> to handle the report even when some files fail.</p> <p>_[DAG]: Directed Acyclic Graph - Airflow's core workflow abstraction _[XCom]: Cross-Communication - Airflow's mechanism for task-to-task data passing _[Executor]: The component that runs tasks (Local, Celery, Kubernetes, etc.) _[Scheduler]: The component that triggers DAGs and determines task execution order _[Webserver]: The component that serves the Airflow UI _[Provider]: An Airflow package that integrates with external systems _[TaskFlow]: The modern Python-native API for defining Airflow tasks using decorators _[Asset]: A logical representation of data that DAGs can produce or consume _[SDK]: Software Development Kit - the airflow.sdk module in Airflow 3.x _[K8s]: Kubernetes - container orchestration platform _[uv]: A fast Python package manager by Astral _[ruff]: A fast Python linter and formatter by Astral _[CI]: Continuous Integration _[CD]: Continuous Deployment *[PR]: Pull Request</p>"},{"location":"modules/06-dynamic-tasks/exercises/exercise_6_3_cross_product/","title":"Exercise 6.3: Cross-Product Processing","text":""},{"location":"modules/06-dynamic-tasks/exercises/exercise_6_3_cross_product/#objective","title":"Objective","text":"<p>Learn to use <code>partial()</code> with <code>expand()</code> for Cartesian product patterns, and <code>expand_kwargs()</code> for explicit combinations.</p>"},{"location":"modules/06-dynamic-tasks/exercises/exercise_6_3_cross_product/#background","title":"Background","text":"<p>When you need to run tasks for every combination of multiple variables (like region \u00d7 product type), Dynamic Task Mapping provides two approaches:</p>"},{"location":"modules/06-dynamic-tasks/exercises/exercise_6_3_cross_product/#approach-1-cartesian-product-with-partial-expand","title":"Approach 1: Cartesian Product with partial() + expand()","text":"<pre><code>@task\ndef process(region: str, product: str):\n    pass\n\n# Expands to ALL combinations (3 \u00d7 2 = 6 tasks)\nprocess.partial().expand(\n    region=[\"us\", \"eu\", \"apac\"],\n    product=[\"widget\", \"gadget\"]\n)\n</code></pre>"},{"location":"modules/06-dynamic-tasks/exercises/exercise_6_3_cross_product/#approach-2-explicit-combinations-with-expand_kwargs","title":"Approach 2: Explicit Combinations with expand_kwargs()","text":"<pre><code># Define specific combinations (3 tasks, not 6)\ncombinations = [\n    {\"region\": \"us\", \"product\": \"widget\"},\n    {\"region\": \"eu\", \"product\": \"gadget\"},\n    {\"region\": \"apac\", \"product\": \"widget\"},\n]\nprocess.expand_kwargs(combinations)\n</code></pre>"},{"location":"modules/06-dynamic-tasks/exercises/exercise_6_3_cross_product/#requirements","title":"Requirements","text":"<p>Create a DAG that processes regional product data:</p> <ol> <li>DAG ID: <code>exercise_6_3_cross_product</code></li> <li>Schedule: <code>None</code> (manual trigger)</li> <li>Start date: January 1, 2024</li> <li>Tags: <code>[\"exercise\", \"module-06\", \"dynamic-tasks\", \"cross-product\"]</code></li> </ol>"},{"location":"modules/06-dynamic-tasks/exercises/exercise_6_3_cross_product/#configuration","title":"Configuration","text":"<ul> <li>Regions: <code>[\"us\", \"eu\", \"apac\"]</code></li> <li>Products: <code>[\"widget\", \"gadget\", \"doohickey\", \"thingamajig\"]</code></li> <li>Total combinations: 3 \u00d7 4 = 12 task instances</li> </ul>"},{"location":"modules/06-dynamic-tasks/exercises/exercise_6_3_cross_product/#tasks","title":"Tasks","text":"<ol> <li> <p>get_config: Return the regions and products lists</p> </li> <li> <p>process_combination: Process a single region + product</p> </li> <li>Use <code>partial()</code> and <code>expand()</code> for cross-product</li> <li>Simulate fetching and processing data</li> <li> <p>Return result with simulated metrics</p> </li> <li> <p>aggregate_results: Combine all results</p> </li> <li>Summarize by region</li> <li>Summarize by product</li> <li>Calculate totals</li> </ol>"},{"location":"modules/06-dynamic-tasks/exercises/exercise_6_3_cross_product/#expected-flow","title":"Expected Flow","text":"<pre><code>get_config \u2192 process_combination (\u00d712) \u2192 aggregate_results\n                     \u2502\n                     \u251c\u2500 [0]: us + widget\n                     \u251c\u2500 [1]: us + gadget\n                     \u251c\u2500 [2]: us + doohickey\n                     \u251c\u2500 [3]: us + thingamajig\n                     \u251c\u2500 [4]: eu + widget\n                     \u251c\u2500 [5]: eu + gadget\n                     \u251c\u2500 ...\n                     \u2514\u2500 [11]: apac + thingamajig\n</code></pre>"},{"location":"modules/06-dynamic-tasks/exercises/exercise_6_3_cross_product/#key-concepts","title":"Key Concepts","text":""},{"location":"modules/06-dynamic-tasks/exercises/exercise_6_3_cross_product/#using-partial-with-expand","title":"Using partial() with expand()","text":"<pre><code>@task\ndef process(region: str, product: str, mode: str = \"full\"):\n    pass\n\n# partial() fixes some args, expand() varies others\nprocess.partial(\n    mode=\"incremental\"  # Same for all\n).expand(\n    region=[\"us\", \"eu\", \"apac\"],\n    product=[\"a\", \"b\"]  # Creates 6 combinations\n)\n</code></pre>"},{"location":"modules/06-dynamic-tasks/exercises/exercise_6_3_cross_product/#understanding-cartesian-product","title":"Understanding Cartesian Product","text":"<p>When expanding multiple arguments: - <code>expand(a=[1,2], b=[x,y])</code> creates: <code>(1,x), (1,y), (2,x), (2,y)</code> - Order: First varies slowest, last varies fastest</p>"},{"location":"modules/06-dynamic-tasks/exercises/exercise_6_3_cross_product/#using-expand_kwargs-for-explicit-control","title":"Using expand_kwargs() for Explicit Control","text":"<pre><code># When you don't want all combinations\ncombos = [\n    {\"region\": \"us\", \"product\": \"premium\"},  # Only some regions\n    {\"region\": \"eu\", \"product\": \"premium\"},  # get premium product\n]\nprocess.expand_kwargs(combos)\n</code></pre>"},{"location":"modules/06-dynamic-tasks/exercises/exercise_6_3_cross_product/#starter-code","title":"Starter Code","text":"<p>See <code>exercise_6_3_cross_product_starter.py</code></p>"},{"location":"modules/06-dynamic-tasks/exercises/exercise_6_3_cross_product/#testing-your-dag","title":"Testing Your DAG","text":"<pre><code># Test the DAG\nairflow dags test exercise_6_3_cross_product 2024-01-15\n\n# You should see 12 parallel task instances\n# Each with a unique region + product combination\n</code></pre>"},{"location":"modules/06-dynamic-tasks/exercises/exercise_6_3_cross_product/#hints","title":"Hints","text":"Hint 1: Returning config <pre><code>@task\ndef get_config() -&gt; dict:\n    return {\n        \"regions\": [\"us\", \"eu\", \"apac\"],\n        \"products\": [\"widget\", \"gadget\", \"doohickey\", \"thingamajig\"],\n    }\n</code></pre> Hint 2: Cross-product expansion <pre><code>config = get_config()\n\n# Extract lists from config\n# Then use partial().expand() for cross-product\nprocess_combination.partial().expand(\n    region=config[\"regions\"],\n    product=config[\"products\"]\n)\n\n# Or expand from separate upstream tasks\n# regions = get_regions()\n# products = get_products()\n# process.expand(region=regions, product=products)\n</code></pre> Hint 3: Processing combination <pre><code>@task\ndef process_combination(region: str, product: str) -&gt; dict:\n    import random\n\n    return {\n        \"region\": region,\n        \"product\": product,\n        \"sales\": random.randint(1000, 50000),\n        \"units\": random.randint(100, 5000),\n        \"status\": \"processed\"\n    }\n</code></pre> Hint 4: Aggregating by dimension <pre><code>@task\ndef aggregate_results(results: list[dict]) -&gt; dict:\n    # Group by region\n    by_region = {}\n    for r in results:\n        region = r[\"region\"]\n        by_region[region] = by_region.get(region, 0) + r[\"sales\"]\n\n    # Group by product\n    by_product = {}\n    for r in results:\n        product = r[\"product\"]\n        by_product[product] = by_product.get(product, 0) + r[\"sales\"]\n\n    return {\n        \"by_region\": by_region,\n        \"by_product\": by_product,\n        \"total\": sum(r[\"sales\"] for r in results)\n    }\n</code></pre>"},{"location":"modules/06-dynamic-tasks/exercises/exercise_6_3_cross_product/#success-criteria","title":"Success Criteria","text":"<ul> <li> DAG creates exactly 12 task instances (3 \u00d7 4)</li> <li> Each combination is processed once</li> <li> Results are correctly aggregated by region</li> <li> Results are correctly aggregated by product</li> <li> You understand partial() vs expand_kwargs()</li> <li> You can predict task count from input dimensions</li> </ul>"},{"location":"modules/06-dynamic-tasks/exercises/exercise_6_3_cross_product/#when-to-use-each-pattern","title":"When to Use Each Pattern","text":"Pattern Use Case <code>expand()</code> with single arg Process list of items <code>partial().expand()</code> multi-arg All combinations needed <code>expand_kwargs()</code> Specific combinations only"},{"location":"modules/06-dynamic-tasks/exercises/exercise_6_3_cross_product/#extension-challenge","title":"Extension Challenge","text":"<p>Try both approaches:</p> <ol> <li>Cartesian product (current exercise): All 12 combinations</li> <li>Selective combinations with <code>expand_kwargs()</code>: Only 5 specific pairs</li> </ol> <pre><code># Only process premium products in select regions\nselective_combos = [\n    {\"region\": \"us\", \"product\": \"widget\"},\n    {\"region\": \"us\", \"product\": \"gadget\"},\n    {\"region\": \"eu\", \"product\": \"widget\"},\n    {\"region\": \"apac\", \"product\": \"thingamajig\"},\n    {\"region\": \"apac\", \"product\": \"doohickey\"},\n]\nprocess.expand_kwargs(selective_combos)\n</code></pre> <p>_[DAG]: Directed Acyclic Graph - Airflow's core workflow abstraction _[XCom]: Cross-Communication - Airflow's mechanism for task-to-task data passing _[Executor]: The component that runs tasks (Local, Celery, Kubernetes, etc.) _[Scheduler]: The component that triggers DAGs and determines task execution order _[Webserver]: The component that serves the Airflow UI _[Provider]: An Airflow package that integrates with external systems _[TaskFlow]: The modern Python-native API for defining Airflow tasks using decorators _[Asset]: A logical representation of data that DAGs can produce or consume _[SDK]: Software Development Kit - the airflow.sdk module in Airflow 3.x _[K8s]: Kubernetes - container orchestration platform _[uv]: A fast Python package manager by Astral _[ruff]: A fast Python linter and formatter by Astral _[CI]: Continuous Integration _[CD]: Continuous Deployment *[PR]: Pull Request</p>"},{"location":"modules/06-dynamic-tasks/exercises/exercise_6_4_parallel_embeddings/","title":"Exercise 6.4: Parallel Embedding Generation","text":"<p>Build a dynamic task mapping pipeline that processes document chunks in parallel for embedding generation, demonstrating <code>expand()</code> patterns for AI/ML workloads.</p>"},{"location":"modules/06-dynamic-tasks/exercises/exercise_6_4_parallel_embeddings/#learning-goals","title":"Learning Goals","text":"<ul> <li>Use dynamic task mapping for parallel document processing</li> <li>Apply <code>expand()</code> to distribute embedding generation across workers</li> <li>Implement batching with <code>map_batches()</code> for API efficiency</li> <li>Handle variable-length inputs with mapped tasks</li> </ul>"},{"location":"modules/06-dynamic-tasks/exercises/exercise_6_4_parallel_embeddings/#scenario","title":"Scenario","text":"<p>You're processing a large document corpus where:</p> <ol> <li>Documents are split into variable-length chunks</li> <li>Each chunk needs an embedding generated (parallelizable)</li> <li>Embeddings should be batched to respect API rate limits</li> <li>Results must be aggregated for vector store upsert</li> </ol>"},{"location":"modules/06-dynamic-tasks/exercises/exercise_6_4_parallel_embeddings/#requirements","title":"Requirements","text":""},{"location":"modules/06-dynamic-tasks/exercises/exercise_6_4_parallel_embeddings/#task-1-dynamic-chunk-processing","title":"Task 1: Dynamic Chunk Processing","text":"<p>Create a pipeline that:</p> <ul> <li>Discovers documents and splits into chunks</li> <li>Uses <code>.expand()</code> to create one task per chunk</li> <li>Processes chunks in parallel within Airflow's constraints</li> </ul>"},{"location":"modules/06-dynamic-tasks/exercises/exercise_6_4_parallel_embeddings/#task-2-batched-api-calls","title":"Task 2: Batched API Calls","text":"<p>Implement batching with:</p> <ul> <li>Group chunks into batches of configurable size</li> <li>Use <code>.map_batches()</code> for efficient processing</li> <li>Handle partial batches at the end</li> </ul>"},{"location":"modules/06-dynamic-tasks/exercises/exercise_6_4_parallel_embeddings/#task-3-result-aggregation","title":"Task 3: Result Aggregation","text":"<p>Aggregate mapped task outputs:</p> <ul> <li>Collect embeddings from all mapped tasks</li> <li>Handle failures in individual tasks gracefully</li> <li>Produce summary statistics</li> </ul>"},{"location":"modules/06-dynamic-tasks/exercises/exercise_6_4_parallel_embeddings/#task-4-cross-product-mapping","title":"Task 4: Cross-Product Mapping","text":"<p>Demonstrate advanced patterns:</p> <ul> <li>Process chunks with multiple embedding models (cross-product)</li> <li>Compare model outputs for quality assessment</li> </ul>"},{"location":"modules/06-dynamic-tasks/exercises/exercise_6_4_parallel_embeddings/#success-criteria","title":"Success Criteria","text":"<ul> <li> Chunks process in parallel across mapped tasks</li> <li> Batching respects API limits</li> <li> Failed chunks don't break the entire pipeline</li> <li> Results are properly aggregated</li> </ul>"},{"location":"modules/06-dynamic-tasks/exercises/exercise_6_4_parallel_embeddings/#hints","title":"Hints","text":"Hint 1: Basic expand() Pattern <pre><code>@task\ndef get_chunks() -&gt; list[dict]:\n    \"\"\"Return list of chunks to process.\"\"\"\n    return [\n        {\"id\": \"chunk-1\", \"text\": \"First chunk...\"},\n        {\"id\": \"chunk-2\", \"text\": \"Second chunk...\"},\n        {\"id\": \"chunk-3\", \"text\": \"Third chunk...\"},\n    ]\n\n\n@task\ndef generate_embedding(chunk: dict) -&gt; dict:\n    \"\"\"Process single chunk - called once per chunk.\"\"\"\n    embedding = mock_embedding(chunk[\"text\"])\n    return {\"id\": chunk[\"id\"], \"embedding\": embedding}\n\n\n# Creates 3 parallel tasks - one per chunk\nchunks = get_chunks()\nembeddings = generate_embedding.expand(chunk=chunks)\n</code></pre> Hint 2: Batched Processing <pre><code>@task\ndef process_batch(batch: list[dict]) -&gt; list[dict]:\n    \"\"\"Process a batch of chunks together.\"\"\"\n    results = []\n    for chunk in batch:\n        embedding = mock_embedding(chunk[\"text\"])\n        results.append({\"id\": chunk[\"id\"], \"embedding\": embedding})\n    return results\n\n\n# Process in batches of 10\nchunks = get_chunks()\nbatched_results = process_batch.map_batches(chunks, batch_size=10)\n</code></pre> Hint 3: Aggregating Mapped Results <pre><code>@task\ndef aggregate_embeddings(embeddings: list[dict]) -&gt; dict:\n    \"\"\"\n    Aggregate results from mapped tasks.\n\n    The `embeddings` parameter receives a list of all outputs\n    from the mapped task.\n    \"\"\"\n    successful = [e for e in embeddings if e.get(\"embedding\")]\n    failed = [e for e in embeddings if not e.get(\"embedding\")]\n\n    return {\n        \"total\": len(embeddings),\n        \"successful\": len(successful),\n        \"failed\": len(failed),\n        \"embeddings\": successful,\n    }\n\n\n# Mapped task outputs are automatically collected into a list\nembeddings = generate_embedding.expand(chunk=chunks)\nsummary = aggregate_embeddings(embeddings)\n</code></pre> Hint 4: Cross-Product with expand_kwargs <pre><code>@task\ndef embed_with_model(chunk: dict, model: str) -&gt; dict:\n    \"\"\"Generate embedding with specific model.\"\"\"\n    embedding = mock_embedding(chunk[\"text\"], model=model)\n    return {\"id\": chunk[\"id\"], \"model\": model, \"embedding\": embedding}\n\n\n# Cross-product: each chunk x each model\nchunks = [{\"id\": \"1\", \"text\": \"...\"}, {\"id\": \"2\", \"text\": \"...\"}]\nmodels = [\"model-a\", \"model-b\"]\n\n# Creates len(chunks) * len(models) tasks\nresults = embed_with_model.expand(\n    chunk=chunks,\n    model=models,\n)\n</code></pre>"},{"location":"modules/06-dynamic-tasks/exercises/exercise_6_4_parallel_embeddings/#files","title":"Files","text":"<ul> <li>Starter: <code>exercise_6_4_parallel_embeddings_starter.py</code></li> <li>Solution: <code>../solutions/solution_6_4_parallel_embeddings.py</code></li> </ul>"},{"location":"modules/06-dynamic-tasks/exercises/exercise_6_4_parallel_embeddings/#estimated-time","title":"Estimated Time","text":"<p>45-60 minutes</p>"},{"location":"modules/06-dynamic-tasks/exercises/exercise_6_4_parallel_embeddings/#next-steps","title":"Next Steps","text":"<p>After completing this exercise:</p> <ol> <li>Experiment with different batch sizes</li> <li>Add progress tracking for long-running pipelines</li> <li>Implement partial failure recovery</li> <li>See Module 15 for complete RAG pipeline patterns</li> </ol> <p>_[DAG]: Directed Acyclic Graph - Airflow's core workflow abstraction _[XCom]: Cross-Communication - Airflow's mechanism for task-to-task data passing _[Executor]: The component that runs tasks (Local, Celery, Kubernetes, etc.) _[Scheduler]: The component that triggers DAGs and determines task execution order _[Webserver]: The component that serves the Airflow UI _[Provider]: An Airflow package that integrates with external systems _[TaskFlow]: The modern Python-native API for defining Airflow tasks using decorators _[Asset]: A logical representation of data that DAGs can produce or consume _[SDK]: Software Development Kit - the airflow.sdk module in Airflow 3.x _[K8s]: Kubernetes - container orchestration platform _[uv]: A fast Python package manager by Astral _[ruff]: A fast Python linter and formatter by Astral _[CI]: Continuous Integration _[CD]: Continuous Deployment *[PR]: Pull Request</p>"},{"location":"modules/07-testing-debugging/","title":"Module 07: Testing &amp; Debugging","text":""},{"location":"modules/07-testing-debugging/#learning-objectives","title":"\ud83c\udfaf Learning Objectives","text":"<p>By the end of this module, you will: - Write unit tests for DAGs and tasks - Use <code>dag.test()</code> for local testing - Debug DAGs using logs and the UI - Validate DAG integrity before deployment - Set up CI/CD testing pipelines</p>"},{"location":"modules/07-testing-debugging/#estimated-time-4-5-hours","title":"\u23f1\ufe0f Estimated Time: 4-5 hours","text":""},{"location":"modules/07-testing-debugging/#1-types-of-airflow-tests","title":"1. Types of Airflow Tests","text":"Test Type Purpose Tools DAG Integrity DAG loads without errors pytest, <code>airflow dags list</code> Unit Tests Task logic works correctly pytest, mocks Integration Tests Tasks work with real services pytest, test environments End-to-End Full DAG runs successfully <code>dag.test()</code>, staging env"},{"location":"modules/07-testing-debugging/#2-dag-integrity-tests","title":"2. DAG Integrity Tests","text":"<p>The minimum test every DAG should have:</p> <pre><code># tests/test_dag_integrity.py\nimport pytest\nfrom airflow.models import DagBag\n\ndef test_dagbag_loads_without_errors():\n    \"\"\"Verify all DAGs load without import errors\"\"\"\n    dagbag = DagBag(dag_folder=\"dags/\", include_examples=False)\n\n    assert len(dagbag.import_errors) == 0, f\"DAG import errors: {dagbag.import_errors}\"\n\ndef test_dags_have_tags():\n    \"\"\"Verify all DAGs have at least one tag\"\"\"\n    dagbag = DagBag(dag_folder=\"dags/\", include_examples=False)\n\n    for dag_id, dag in dagbag.dags.items():\n        assert dag.tags, f\"DAG {dag_id} has no tags\"\n\ndef test_dags_have_description():\n    \"\"\"Verify all DAGs have descriptions\"\"\"\n    dagbag = DagBag(dag_folder=\"dags/\", include_examples=False)\n\n    for dag_id, dag in dagbag.dags.items():\n        assert dag.description, f\"DAG {dag_id} has no description\"\n\ndef test_no_cycles():\n    \"\"\"Verify DAGs don't have cycles (should never happen, but check)\"\"\"\n    dagbag = DagBag(dag_folder=\"dags/\", include_examples=False)\n\n    for dag_id, dag in dagbag.dags.items():\n        # If DAG loaded, it has no cycles (Airflow validates this)\n        assert dag is not None\n</code></pre>"},{"location":"modules/07-testing-debugging/#3-unit-testing-tasks","title":"3. Unit Testing Tasks","text":""},{"location":"modules/07-testing-debugging/#testing-taskflow-functions","title":"Testing TaskFlow Functions","text":"<pre><code># tests/test_etl_tasks.py\nimport pytest\nfrom dags.etl_pipeline import extract_data, transform_data, validate_data\n\nclass TestExtractData:\n    def test_returns_expected_format(self):\n        \"\"\"Test extract returns proper structure\"\"\"\n        # Call the underlying function directly\n        result = extract_data.function()\n\n        assert isinstance(result, dict)\n        assert \"records\" in result\n        assert \"timestamp\" in result\n\n    def test_handles_empty_source(self, mocker):\n        \"\"\"Test behavior with empty data source\"\"\"\n        # Mock external calls\n        mocker.patch(\"dags.etl_pipeline.fetch_from_api\", return_value=[])\n\n        result = extract_data.function()\n\n        assert result[\"records\"] == []\n\nclass TestTransformData:\n    def test_transforms_correctly(self):\n        \"\"\"Test transformation logic\"\"\"\n        input_data = {\"records\": [{\"name\": \"test\", \"value\": 10}]}\n\n        result = transform_data.function(input_data)\n\n        assert result[\"transformed\"][0][\"value\"] == 20  # Doubled\n\n    def test_handles_missing_fields(self):\n        \"\"\"Test graceful handling of missing fields\"\"\"\n        input_data = {\"records\": [{\"name\": \"test\"}]}  # No 'value'\n\n        result = transform_data.function(input_data)\n\n        assert result[\"transformed\"][0][\"value\"] == 0  # Default\n</code></pre>"},{"location":"modules/07-testing-debugging/#mocking-external-dependencies","title":"Mocking External Dependencies","text":"<pre><code>import pytest\nfrom unittest.mock import Mock, patch\n\ndef test_database_task(mocker):\n    \"\"\"Test task that uses database connection\"\"\"\n    # Mock the hook\n    mock_hook = Mock()\n    mock_hook.get_records.return_value = [(\"row1\",), (\"row2\",)]\n\n    mocker.patch(\n        \"dags.my_dag.PostgresHook\",\n        return_value=mock_hook\n    )\n\n    from dags.my_dag import query_database\n    result = query_database.function()\n\n    assert len(result) == 2\n    mock_hook.get_records.assert_called_once()\n\ndef test_s3_task(mocker):\n    \"\"\"Test task that uses S3\"\"\"\n    mock_hook = Mock()\n    mock_hook.read_key.return_value = '{\"data\": \"test\"}'\n\n    mocker.patch(\n        \"dags.my_dag.S3Hook\",\n        return_value=mock_hook\n    )\n\n    from dags.my_dag import read_from_s3\n    result = read_from_s3.function(\"bucket\", \"key\")\n\n    assert result == {\"data\": \"test\"}\n</code></pre>"},{"location":"modules/07-testing-debugging/#4-using-dagtest","title":"4. Using dag.test()","text":"<p>Airflow 2.5+ provides <code>dag.test()</code> for local testing:</p> <pre><code># Run from command line\nairflow dags test my_dag 2024-01-01\n\n# Or in Python\nfrom dags.my_dag import dag\n\nif __name__ == \"__main__\":\n    dag.test()\n</code></pre>"},{"location":"modules/07-testing-debugging/#testing-with-parameters","title":"Testing with Parameters","text":"<pre><code>airflow dags test my_dag 2024-01-01 \\\n    --conf '{\"param1\": \"value1\", \"param2\": 42}'\n</code></pre>"},{"location":"modules/07-testing-debugging/#testing-specific-tasks","title":"Testing Specific Tasks","text":"<pre><code># Test a single task\nairflow tasks test my_dag my_task 2024-01-01\n\n# Test with verbose output\nairflow tasks test my_dag my_task 2024-01-01 -v\n</code></pre>"},{"location":"modules/07-testing-debugging/#5-debugging-techniques","title":"5. Debugging Techniques","text":""},{"location":"modules/07-testing-debugging/#reading-logs","title":"Reading Logs","text":"<pre><code># Scheduler logs\nairflow scheduler --log-file /var/log/airflow/scheduler.log\n\n# Task logs location (default)\n# $AIRFLOW_HOME/logs/dag_id/task_id/execution_date/\n\n# Via CLI\nairflow tasks logs my_dag my_task 2024-01-01 --local\n</code></pre>"},{"location":"modules/07-testing-debugging/#adding-debug-logging","title":"Adding Debug Logging","text":"<pre><code>from airflow.sdk import task\nimport logging\n\n@task\ndef my_task():\n    logger = logging.getLogger(\"airflow.task\")\n\n    logger.debug(\"Debug details: %s\", some_var)  # Won't show by default\n    logger.info(\"Processing started\")\n    logger.warning(\"Something unexpected but not fatal\")\n    logger.error(\"Something went wrong: %s\", error_details)\n\n    # Or use print (also captured in logs)\n    print(\"This appears in task logs too\")\n</code></pre>"},{"location":"modules/07-testing-debugging/#setting-log-levels","title":"Setting Log Levels","text":"<pre><code># Increase verbosity\nexport AIRFLOW__LOGGING__LOGGING_LEVEL=DEBUG\n\n# Or in airflow.cfg\n[logging]\nlogging_level = DEBUG\n</code></pre>"},{"location":"modules/07-testing-debugging/#using-the-ui-debugger","title":"Using the UI Debugger","text":"<ol> <li>Grid View: See task states and history</li> <li>Graph View: Visualize dependencies</li> <li>Task Instance Details: Click any task \u2192 \"Log\" tab</li> <li>XCom Tab: Inspect passed data</li> <li>Rendered Template: See actual values after Jinja rendering</li> </ol>"},{"location":"modules/07-testing-debugging/#common-issues-and-solutions","title":"Common Issues and Solutions","text":"Issue Symptom Solution Import Error DAG not appearing Check <code>airflow dags list</code> for errors Task Stuck Always \"running\" Check worker logs, increase timeout XCom Too Large Serialization error Pass references, not data Dependency Cycle DAG won't load Check task dependencies Template Error Task fails immediately Check \"Rendered Template\" in UI"},{"location":"modules/07-testing-debugging/#6-testing-configuration","title":"6. Testing Configuration","text":""},{"location":"modules/07-testing-debugging/#pytest-configuration","title":"pytest Configuration","text":"<pre><code># pytest.ini\n[pytest]\ntestpaths = tests\npython_files = test_*.py\npython_functions = test_*\naddopts = -v --tb=short\nfilterwarnings =\n    ignore::DeprecationWarning\n\n# Environment variables for tests\nenv =\n    AIRFLOW_HOME=./tests/airflow_home\n    AIRFLOW__CORE__LOAD_EXAMPLES=False\n</code></pre>"},{"location":"modules/07-testing-debugging/#fixtures","title":"Fixtures","text":"<pre><code># tests/conftest.py\nimport pytest\nimport os\nfrom airflow.models import DagBag\n\n@pytest.fixture(scope=\"session\")\ndef dagbag():\n    \"\"\"Shared DagBag for all tests\"\"\"\n    return DagBag(dag_folder=\"dags/\", include_examples=False)\n\n@pytest.fixture\ndef mock_context():\n    \"\"\"Mock Airflow context for testing\"\"\"\n    from datetime import datetime\n    from pendulum import DateTime\n    from unittest.mock import Mock\n\n    ti = Mock()\n    ti.xcom_pull = Mock(return_value=None)\n    ti.xcom_push = Mock()\n\n    return {\n        \"ti\": ti,\n        \"ds\": \"2024-01-01\",\n        \"logical_date\": DateTime(2024, 1, 1),\n        \"data_interval_start\": DateTime(2024, 1, 1),\n        \"data_interval_end\": DateTime(2024, 1, 2),\n        \"params\": {},\n    }\n\n@pytest.fixture\ndef temp_dag_folder(tmp_path):\n    \"\"\"Temporary folder for testing DAG files\"\"\"\n    dag_folder = tmp_path / \"dags\"\n    dag_folder.mkdir()\n    return dag_folder\n</code></pre>"},{"location":"modules/07-testing-debugging/#7-cicd-integration","title":"7. CI/CD Integration","text":""},{"location":"modules/07-testing-debugging/#github-actions-example","title":"GitHub Actions Example","text":"<pre><code># .github/workflows/airflow-tests.yml\nname: Airflow Tests\n\non:\n  push:\n    branches: [main]\n  pull_request:\n    branches: [main]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Set up Python\n        uses: actions/setup-python@v5\n        with:\n          python-version: '3.11'\n\n      - name: Install dependencies\n        run: |\n          pip install -r requirements.txt\n          pip install pytest pytest-mock\n\n      - name: Lint DAGs with Ruff\n        run: |\n          pip install ruff\n          ruff check dags/ --select AIR\n\n      - name: Run DAG integrity tests\n        run: |\n          export AIRFLOW_HOME=$(pwd)/tests/airflow_home\n          airflow db init\n          pytest tests/test_dag_integrity.py -v\n\n      - name: Run unit tests\n        run: |\n          pytest tests/ -v --ignore=tests/test_dag_integrity.py\n</code></pre>"},{"location":"modules/07-testing-debugging/#pre-commit-hooks","title":"Pre-commit Hooks","text":"<pre><code># .pre-commit-config.yaml\nrepos:\n  - repo: https://github.com/astral-sh/ruff-pre-commit\n    rev: v0.1.5\n    hooks:\n      - id: ruff\n        args: [--select, AIR, --fix]\n      - id: ruff-format\n\n  - repo: local\n    hooks:\n      - id: dag-integrity\n        name: DAG Integrity Check\n        entry: python -c \"from airflow.models import DagBag; db=DagBag('dags/'); assert not db.import_errors, db.import_errors\"\n        language: system\n        pass_filenames: false\n        files: ^dags/.*\\.py$\n</code></pre>"},{"location":"modules/07-testing-debugging/#exercises","title":"\ud83d\udcdd Exercises","text":""},{"location":"modules/07-testing-debugging/#exercise-71-write-integrity-tests","title":"Exercise 7.1: Write Integrity Tests","text":"<p>Create a test suite that validates: - All DAGs load without errors - All DAGs have an owner in default_args - No DAG has schedule=None without explicit documentation - All task IDs follow naming convention (snake_case)</p>"},{"location":"modules/07-testing-debugging/#exercise-72-unit-test-a-pipeline","title":"Exercise 7.2: Unit Test a Pipeline","text":"<p>Take the ETL DAG from Module 02 and write: - Unit tests for each task function - Tests with mocked external dependencies - Edge case tests (empty data, malformed data)</p>"},{"location":"modules/07-testing-debugging/#exercise-73-debug-a-broken-dag","title":"Exercise 7.3: Debug a Broken DAG","text":"<p>A deliberately broken DAG is provided in <code>exercises/broken_dag.py</code>. Use debugging techniques to: - Identify all issues - Fix them - Add tests to prevent regression</p>"},{"location":"modules/07-testing-debugging/#checkpoint","title":"\u2705 Checkpoint","text":"<p>Before moving to Module 08, ensure you can:</p> <ul> <li> Write DAG integrity tests with pytest</li> <li> Unit test TaskFlow functions</li> <li> Mock external dependencies in tests</li> <li> Use <code>dag.test()</code> and <code>tasks test</code> commands</li> <li> Find and read task logs</li> <li> Set up basic CI/CD for DAG validation</li> </ul> <p>Next: Module 08: Kubernetes Executor \u2192</p> <p>_[DAG]: Directed Acyclic Graph - Airflow's core workflow abstraction _[XCom]: Cross-Communication - Airflow's mechanism for task-to-task data passing _[Executor]: The component that runs tasks (Local, Celery, Kubernetes, etc.) _[Scheduler]: The component that triggers DAGs and determines task execution order _[Webserver]: The component that serves the Airflow UI _[Provider]: An Airflow package that integrates with external systems _[TaskFlow]: The modern Python-native API for defining Airflow tasks using decorators _[Asset]: A logical representation of data that DAGs can produce or consume _[SDK]: Software Development Kit - the airflow.sdk module in Airflow 3.x _[K8s]: Kubernetes - container orchestration platform _[uv]: A fast Python package manager by Astral _[ruff]: A fast Python linter and formatter by Astral _[CI]: Continuous Integration _[CD]: Continuous Deployment *[PR]: Pull Request</p>"},{"location":"modules/07-testing-debugging/exercises/exercise_7_1_integrity_tests/","title":"Exercise 7.1: DAG Integrity Tests","text":""},{"location":"modules/07-testing-debugging/exercises/exercise_7_1_integrity_tests/#objective","title":"Objective","text":"<p>Create a comprehensive test suite that validates DAG structure, configuration, and coding standards.</p>"},{"location":"modules/07-testing-debugging/exercises/exercise_7_1_integrity_tests/#background","title":"Background","text":"<p>DAG integrity tests run automatically in CI/CD pipelines to catch issues before deployment: - Import errors that prevent DAGs from loading - Missing required configurations (owner, tags) - Naming convention violations - Potentially problematic configurations</p>"},{"location":"modules/07-testing-debugging/exercises/exercise_7_1_integrity_tests/#why-integrity-tests-matter","title":"Why Integrity Tests Matter","text":"Issue Cost to Fix Caught in PR Minutes Caught in staging Hours Caught in production Days + incidents"},{"location":"modules/07-testing-debugging/exercises/exercise_7_1_integrity_tests/#requirements","title":"Requirements","text":"<p>Create a test file <code>test_dag_integrity.py</code> that validates:</p> <ol> <li>All DAGs Load Successfully</li> <li>No import errors</li> <li>No syntax errors</li> <li> <p>All dependencies available</p> </li> <li> <p>Owner Configuration</p> </li> <li>All DAGs have an <code>owner</code> in default_args or tags</li> <li> <p>Owner is not the default \"airflow\"</p> </li> <li> <p>Schedule Documentation</p> </li> <li>DAGs with <code>schedule=None</code> must have a description</li> <li> <p>Undocumented manual-trigger DAGs are flagged</p> </li> <li> <p>Task ID Naming</p> </li> <li>All task IDs use snake_case</li> <li>No special characters (except underscore)</li> </ol>"},{"location":"modules/07-testing-debugging/exercises/exercise_7_1_integrity_tests/#key-concepts","title":"Key Concepts","text":""},{"location":"modules/07-testing-debugging/exercises/exercise_7_1_integrity_tests/#using-dagbag-for-testing","title":"Using DagBag for Testing","text":"<pre><code>from airflow.models import DagBag\n\ndef test_no_import_errors():\n    dag_bag = DagBag(dag_folder=\"dags/\", include_examples=False)\n    assert len(dag_bag.import_errors) == 0, dag_bag.import_errors\n</code></pre>"},{"location":"modules/07-testing-debugging/exercises/exercise_7_1_integrity_tests/#iterating-over-dags","title":"Iterating Over DAGs","text":"<pre><code>dag_bag = DagBag(...)\nfor dag_id, dag in dag_bag.dags.items():\n    # Access DAG properties\n    owner = dag.default_args.get(\"owner\")\n    schedule = dag.schedule_interval\n    tasks = dag.tasks\n</code></pre>"},{"location":"modules/07-testing-debugging/exercises/exercise_7_1_integrity_tests/#checking-task-ids","title":"Checking Task IDs","text":"<pre><code>import re\n\ndef is_snake_case(name: str) -&gt; bool:\n    return bool(re.match(r'^[a-z][a-z0-9_]*$', name))\n\nfor task in dag.tasks:\n    assert is_snake_case(task.task_id)\n</code></pre>"},{"location":"modules/07-testing-debugging/exercises/exercise_7_1_integrity_tests/#starter-code","title":"Starter Code","text":"<p>See <code>exercise_7_1_integrity_tests_starter.py</code></p>"},{"location":"modules/07-testing-debugging/exercises/exercise_7_1_integrity_tests/#testing-your-tests","title":"Testing Your Tests","text":"<pre><code># Run the integrity tests\npytest modules/07-testing-debugging/exercises/test_dag_integrity.py -v\n\n# Or run against your dags folder\npytest tests/test_dag_integrity.py -v\n</code></pre>"},{"location":"modules/07-testing-debugging/exercises/exercise_7_1_integrity_tests/#hints","title":"Hints","text":"Hint 1: Setting up DagBag <pre><code>import os\nimport pytest\nfrom airflow.models import DagBag\n\n@pytest.fixture(scope=\"session\")\ndef dag_bag():\n    \"\"\"Load all DAGs for testing.\"\"\"\n    dag_folder = os.path.join(os.path.dirname(__file__), \"..\", \"..\", \"..\", \"dags\")\n    return DagBag(dag_folder=dag_folder, include_examples=False)\n</code></pre> Hint 2: Parametrized tests for each DAG <pre><code>@pytest.mark.parametrize(\"dag_id\", dag_bag.dags.keys())\ndef test_dag_has_owner(dag_bag, dag_id):\n    dag = dag_bag.dags[dag_id]\n    owner = dag.default_args.get(\"owner\")\n    assert owner is not None, f\"DAG {dag_id} missing owner\"\n    assert owner != \"airflow\", f\"DAG {dag_id} uses default owner\"\n</code></pre> Hint 3: Checking schedule documentation <pre><code>def test_manual_dags_documented(dag_bag):\n    for dag_id, dag in dag_bag.dags.items():\n        if dag.schedule_interval is None:\n            assert dag.description, (\n                f\"DAG {dag_id} has schedule=None but no description\"\n            )\n</code></pre>"},{"location":"modules/07-testing-debugging/exercises/exercise_7_1_integrity_tests/#success-criteria","title":"Success Criteria","text":"<ul> <li> Test suite catches import errors</li> <li> Test verifies owner is set and not default</li> <li> Test flags undocumented manual-trigger DAGs</li> <li> Test validates snake_case task IDs</li> <li> Tests can be run in CI/CD</li> <li> Clear error messages indicate what to fix</li> </ul>"},{"location":"modules/07-testing-debugging/exercises/exercise_7_1_integrity_tests/#real-world-test-patterns","title":"Real-World Test Patterns","text":"Test Purpose Import errors Catch syntax and dependency issues Required tags Ensure DAGs are categorized Email on failure Verify alerting is configured Retries configured Production resilience Pool assignments Resource management validation <p>_[DAG]: Directed Acyclic Graph - Airflow's core workflow abstraction _[XCom]: Cross-Communication - Airflow's mechanism for task-to-task data passing _[Executor]: The component that runs tasks (Local, Celery, Kubernetes, etc.) _[Scheduler]: The component that triggers DAGs and determines task execution order _[Webserver]: The component that serves the Airflow UI _[Provider]: An Airflow package that integrates with external systems _[TaskFlow]: The modern Python-native API for defining Airflow tasks using decorators _[Asset]: A logical representation of data that DAGs can produce or consume _[SDK]: Software Development Kit - the airflow.sdk module in Airflow 3.x _[K8s]: Kubernetes - container orchestration platform _[uv]: A fast Python package manager by Astral _[ruff]: A fast Python linter and formatter by Astral _[CI]: Continuous Integration _[CD]: Continuous Deployment *[PR]: Pull Request</p>"},{"location":"modules/07-testing-debugging/exercises/exercise_7_2_unit_testing/","title":"Exercise 7.2: Unit Test a Pipeline","text":""},{"location":"modules/07-testing-debugging/exercises/exercise_7_2_unit_testing/#objective","title":"Objective","text":"<p>Write comprehensive unit tests for an ETL pipeline, including mocking external dependencies and testing edge cases.</p>"},{"location":"modules/07-testing-debugging/exercises/exercise_7_2_unit_testing/#background","title":"Background","text":"<p>Unit testing DAG tasks ensures: - Business logic is correct - Edge cases are handled - Regressions are caught early - Code is maintainable and refactorable</p>"},{"location":"modules/07-testing-debugging/exercises/exercise_7_2_unit_testing/#testing-taskflow-functions","title":"Testing TaskFlow Functions","text":"<p>TaskFlow functions are regular Python functions, making them easy to test:</p> <pre><code># In your DAG file\n@task\ndef transform_data(raw: dict) -&gt; dict:\n    return {\"processed\": raw[\"value\"] * 2}\n\n# In your test file\ndef test_transform_data():\n    result = transform_data.function({\"value\": 5})\n    assert result == {\"processed\": 10}\n</code></pre>"},{"location":"modules/07-testing-debugging/exercises/exercise_7_2_unit_testing/#requirements","title":"Requirements","text":"<p>Create tests for an ETL pipeline with these tasks:</p> <ol> <li>extract_data: Fetches data from an API</li> <li>transform_data: Cleans and transforms data</li> <li>load_data: Loads data to a destination</li> </ol>"},{"location":"modules/07-testing-debugging/exercises/exercise_7_2_unit_testing/#test-categories","title":"Test Categories","text":"<ol> <li>Happy Path Tests</li> <li>Normal data processing</li> <li> <p>Expected transformations</p> </li> <li> <p>Edge Case Tests</p> </li> <li>Empty data</li> <li>Missing fields</li> <li> <p>Null values</p> </li> <li> <p>Mocked Dependency Tests</p> </li> <li>API calls</li> <li>Database connections</li> <li>File system operations</li> </ol>"},{"location":"modules/07-testing-debugging/exercises/exercise_7_2_unit_testing/#key-concepts","title":"Key Concepts","text":""},{"location":"modules/07-testing-debugging/exercises/exercise_7_2_unit_testing/#accessing-task-functions","title":"Accessing Task Functions","text":"<pre><code># TaskFlow decorator wraps the function\n# Access the original function with .function\nresult = my_task.function(input_data)\n</code></pre>"},{"location":"modules/07-testing-debugging/exercises/exercise_7_2_unit_testing/#mocking-external-calls","title":"Mocking External Calls","text":"<pre><code>from unittest.mock import patch, MagicMock\n\n@patch('requests.get')\ndef test_extract_with_mock(mock_get):\n    mock_get.return_value.json.return_value = {\"data\": [1, 2, 3]}\n    mock_get.return_value.status_code = 200\n\n    result = extract_data.function()\n    assert len(result[\"data\"]) == 3\n</code></pre>"},{"location":"modules/07-testing-debugging/exercises/exercise_7_2_unit_testing/#testing-with-pytest-fixtures","title":"Testing with pytest Fixtures","text":"<pre><code>@pytest.fixture\ndef sample_data():\n    return {\n        \"records\": [\n            {\"id\": 1, \"value\": 100},\n            {\"id\": 2, \"value\": 200},\n        ]\n    }\n\ndef test_transform(sample_data):\n    result = transform_data.function(sample_data)\n    assert result[\"total\"] == 300\n</code></pre>"},{"location":"modules/07-testing-debugging/exercises/exercise_7_2_unit_testing/#starter-code","title":"Starter Code","text":"<p>See <code>exercise_7_2_unit_testing_starter.py</code></p>"},{"location":"modules/07-testing-debugging/exercises/exercise_7_2_unit_testing/#testing-your-tests","title":"Testing Your Tests","text":"<pre><code># Run the unit tests\npytest modules/07-testing-debugging/exercises/test_etl_pipeline.py -v\n\n# Run with coverage\npytest --cov=dags --cov-report=html\n</code></pre>"},{"location":"modules/07-testing-debugging/exercises/exercise_7_2_unit_testing/#hints","title":"Hints","text":"Hint 1: Testing transform logic <pre><code>def test_transform_calculates_total():\n    input_data = {\n        \"records\": [\n            {\"id\": 1, \"amount\": 100},\n            {\"id\": 2, \"amount\": 200},\n        ]\n    }\n\n    result = transform_data.function(input_data)\n\n    assert result[\"total_amount\"] == 300\n    assert result[\"record_count\"] == 2\n</code></pre> Hint 2: Testing empty data handling <pre><code>def test_transform_handles_empty_data():\n    result = transform_data.function({\"records\": []})\n\n    assert result[\"total_amount\"] == 0\n    assert result[\"record_count\"] == 0\n</code></pre> Hint 3: Mocking API calls <pre><code>from unittest.mock import patch\n\n@patch('requests.get')\ndef test_extract_api_call(mock_get):\n    # Set up mock response\n    mock_get.return_value.json.return_value = {\n        \"data\": [{\"id\": 1}]\n    }\n    mock_get.return_value.status_code = 200\n\n    result = extract_data.function()\n\n    # Verify the mock was called\n    mock_get.assert_called_once()\n    assert len(result[\"data\"]) == 1\n</code></pre>"},{"location":"modules/07-testing-debugging/exercises/exercise_7_2_unit_testing/#success-criteria","title":"Success Criteria","text":"<ul> <li> Happy path tests pass</li> <li> Empty data is handled gracefully</li> <li> Missing fields don't cause crashes</li> <li> External calls are properly mocked</li> <li> Tests are fast (no real external calls)</li> <li> Coverage is at least 80%</li> </ul>"},{"location":"modules/07-testing-debugging/exercises/exercise_7_2_unit_testing/#test-structure-best-practices","title":"Test Structure Best Practices","text":"<pre><code>tests/\n\u251c\u2500\u2500 conftest.py          # Shared fixtures\n\u251c\u2500\u2500 test_dag_integrity.py\n\u2514\u2500\u2500 dags/\n    \u251c\u2500\u2500 test_etl_pipeline.py\n    \u251c\u2500\u2500 test_data_quality.py\n    \u2514\u2500\u2500 fixtures/\n        \u251c\u2500\u2500 sample_data.json\n        \u2514\u2500\u2500 expected_output.json\n</code></pre> <p>_[DAG]: Directed Acyclic Graph - Airflow's core workflow abstraction _[XCom]: Cross-Communication - Airflow's mechanism for task-to-task data passing _[Executor]: The component that runs tasks (Local, Celery, Kubernetes, etc.) _[Scheduler]: The component that triggers DAGs and determines task execution order _[Webserver]: The component that serves the Airflow UI _[Provider]: An Airflow package that integrates with external systems _[TaskFlow]: The modern Python-native API for defining Airflow tasks using decorators _[Asset]: A logical representation of data that DAGs can produce or consume _[SDK]: Software Development Kit - the airflow.sdk module in Airflow 3.x _[K8s]: Kubernetes - container orchestration platform _[uv]: A fast Python package manager by Astral _[ruff]: A fast Python linter and formatter by Astral _[CI]: Continuous Integration _[CD]: Continuous Deployment *[PR]: Pull Request</p>"},{"location":"modules/07-testing-debugging/exercises/exercise_7_3_debugging/","title":"Exercise 7.3: Debug a Broken DAG","text":""},{"location":"modules/07-testing-debugging/exercises/exercise_7_3_debugging/#objective","title":"Objective","text":"<p>Practice debugging skills by identifying and fixing multiple issues in a deliberately broken DAG.</p>"},{"location":"modules/07-testing-debugging/exercises/exercise_7_3_debugging/#background","title":"Background","text":"<p>Debugging DAGs requires: - Understanding error messages and stack traces - Checking logs at various levels - Using Airflow's testing tools - Systematic problem isolation</p>"},{"location":"modules/07-testing-debugging/exercises/exercise_7_3_debugging/#common-dag-issues","title":"Common DAG Issues","text":"Issue Type Symptoms Import errors DAG doesn't appear in UI Syntax errors DAG fails to parse Runtime errors Tasks fail during execution Logic errors Tasks complete but produce wrong results Dependency issues Tasks run in wrong order"},{"location":"modules/07-testing-debugging/exercises/exercise_7_3_debugging/#the-broken-dag","title":"The Broken DAG","text":"<p>A broken DAG is provided in <code>broken_dag.py</code>. It contains multiple intentional bugs:</p> <ol> <li>Import error: Missing or incorrect import</li> <li>Syntax error: Python syntax issue</li> <li>Configuration error: Invalid DAG parameter</li> <li>Runtime error: Task fails during execution</li> <li>Logic error: Incorrect business logic</li> <li>Dependency error: Wrong task order</li> </ol>"},{"location":"modules/07-testing-debugging/exercises/exercise_7_3_debugging/#requirements","title":"Requirements","text":"<ol> <li>Identify Issues</li> <li>List all bugs found</li> <li> <p>Explain what each bug causes</p> </li> <li> <p>Fix Each Bug</p> </li> <li>Correct all issues</li> <li> <p>Explain your fix</p> </li> <li> <p>Add Regression Tests</p> </li> <li>Write tests that would catch these bugs</li> <li>Prevent future regressions</li> </ol>"},{"location":"modules/07-testing-debugging/exercises/exercise_7_3_debugging/#debugging-techniques","title":"Debugging Techniques","text":""},{"location":"modules/07-testing-debugging/exercises/exercise_7_3_debugging/#check-for-import-errors","title":"Check for Import Errors","text":"<pre><code># Quick import check\npython -c \"from airflow.models import DagBag; db=DagBag('dags/'); print(db.import_errors)\"\n\n# Or use airflow CLI\nairflow dags list-import-errors\n</code></pre>"},{"location":"modules/07-testing-debugging/exercises/exercise_7_3_debugging/#test-dag-parsing","title":"Test DAG Parsing","text":"<pre><code># Test specific DAG\nairflow dags test broken_dag 2024-01-15\n</code></pre>"},{"location":"modules/07-testing-debugging/exercises/exercise_7_3_debugging/#check-task-logs","title":"Check Task Logs","text":"<pre><code># In production\nairflow tasks logs broken_dag task_id 2024-01-15\n\n# In UI: Click on task \u2192 View Log\n</code></pre>"},{"location":"modules/07-testing-debugging/exercises/exercise_7_3_debugging/#debug-interactively","title":"Debug Interactively","text":"<pre><code># In Python shell\nfrom airflow.models import DagBag\n\ndag_bag = DagBag()\ndag = dag_bag.dags.get(\"broken_dag\")\n\n# Inspect DAG\nprint(dag.schedule_interval)\nprint(dag.tasks)\n\n# Test task\ntask = dag.get_task(\"task_name\")\ntask.execute(context={})\n</code></pre>"},{"location":"modules/07-testing-debugging/exercises/exercise_7_3_debugging/#starter-code","title":"Starter Code","text":"<p>See <code>broken_dag.py</code> for the broken code.</p>"},{"location":"modules/07-testing-debugging/exercises/exercise_7_3_debugging/#hints","title":"Hints","text":"Hint 1: Finding import errors  Run this to see import errors: <pre><code>python -c \"from dags.exercises.broken_dag import *\"\n</code></pre>  If you see a traceback, that's an import error.   Hint 2: Common syntax issues  - Missing colons after function definitions - Incorrect indentation - Unclosed parentheses or brackets - Wrong decorator syntax   Hint 3: Configuration validation  Check these DAG parameters: - `schedule` format - `start_date` type (must be datetime) - `dag_id` uniqueness - `default_args` structure   Hint 4: Runtime debugging  If a task fails: 1. Check the task logs 2. Look for the exception type 3. Find the line number 4. Test the function in isolation"},{"location":"modules/07-testing-debugging/exercises/exercise_7_3_debugging/#success-criteria","title":"Success Criteria","text":"<ul> <li> All import errors identified and fixed</li> <li> All syntax errors identified and fixed</li> <li> All configuration errors identified and fixed</li> <li> All runtime errors identified and fixed</li> <li> All logic errors identified and fixed</li> <li> DAG runs successfully end-to-end</li> <li> Tests added to prevent regression</li> </ul>"},{"location":"modules/07-testing-debugging/exercises/exercise_7_3_debugging/#debugging-checklist","title":"Debugging Checklist","text":"<pre><code>\u25a1 DAG appears in Airflow UI\n\u25a1 No import errors shown\n\u25a1 DAG can be triggered\n\u25a1 All tasks complete successfully\n\u25a1 Output data is correct\n\u25a1 Dependencies execute in correct order\n\u25a1 Tests pass\n</code></pre> <p>_[DAG]: Directed Acyclic Graph - Airflow's core workflow abstraction _[XCom]: Cross-Communication - Airflow's mechanism for task-to-task data passing _[Executor]: The component that runs tasks (Local, Celery, Kubernetes, etc.) _[Scheduler]: The component that triggers DAGs and determines task execution order _[Webserver]: The component that serves the Airflow UI _[Provider]: An Airflow package that integrates with external systems _[TaskFlow]: The modern Python-native API for defining Airflow tasks using decorators _[Asset]: A logical representation of data that DAGs can produce or consume _[SDK]: Software Development Kit - the airflow.sdk module in Airflow 3.x _[K8s]: Kubernetes - container orchestration platform _[uv]: A fast Python package manager by Astral _[ruff]: A fast Python linter and formatter by Astral _[CI]: Continuous Integration _[CD]: Continuous Deployment *[PR]: Pull Request</p>"},{"location":"modules/08-kubernetes-executor/","title":"Module 08: Kubernetes Executor","text":""},{"location":"modules/08-kubernetes-executor/#learning-objectives","title":"\ud83c\udfaf Learning Objectives","text":"<p>By the end of this module, you will:</p> <ul> <li>Deploy Airflow on Kubernetes using the official Helm chart</li> <li>Understand KubernetesExecutor architecture and benefits</li> <li>Configure pod templates for task customization</li> <li>Manage resources, secrets, and persistent storage</li> <li>Debug Kubernetes-specific issues</li> </ul>"},{"location":"modules/08-kubernetes-executor/#estimated-time-6-8-hours-hands-on-deployment","title":"\u23f1\ufe0f Estimated Time: 6-8 hours (hands-on deployment)","text":""},{"location":"modules/08-kubernetes-executor/#1-why-kubernetes-for-airflow","title":"1. Why Kubernetes for Airflow?","text":""},{"location":"modules/08-kubernetes-executor/#kubernetesexecutor-benefits","title":"KubernetesExecutor Benefits","text":"Benefit Description Isolation Each task runs in its own pod Scalability Scale to zero when idle, up to cluster limits Resource Control Per-task CPU/memory limits No Queue Needed No Redis/RabbitMQ dependency Security Tasks don't need database access (Airflow 3)"},{"location":"modules/08-kubernetes-executor/#when-to-use-what","title":"When to Use What","text":"Executor Best For <code>LocalExecutor</code> Development, small workloads <code>CeleryExecutor</code> High-throughput, consistent workloads <code>KubernetesExecutor</code> Variable workloads, isolation needs, K8s native"},{"location":"modules/08-kubernetes-executor/#2-architecture-on-kubernetes","title":"2. Architecture on Kubernetes","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    Kubernetes Cluster                        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                              \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                   \u2502\n\u2502  \u2502   API Server    \u2502  \u2502   Scheduler     \u2502                   \u2502\n\u2502  \u2502   (Deployment)  \u2502  \u2502   (Deployment)  \u2502                   \u2502\n\u2502  \u2502   replicas: 2   \u2502  \u2502   replicas: 2   \u2502                   \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                   \u2502\n\u2502           \u2502                    \u2502                             \u2502\n\u2502           \u2502    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                            \u2502\n\u2502           \u2502    \u2502                                            \u2502\n\u2502           \u25bc    \u25bc                                            \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                        \u2502\n\u2502  \u2502    PostgreSQL   \u2502  \u25c4\u2500\u2500 Metadata database                 \u2502\n\u2502  \u2502   (or external) \u2502                                        \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                        \u2502\n\u2502                                                              \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502\n\u2502  \u2502              Task Pods (Dynamic)                     \u2502    \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510            \u2502    \u2502\n\u2502  \u2502  \u2502 Task Pod \u2502 \u2502 Task Pod \u2502 \u2502 Task Pod \u2502 ...        \u2502    \u2502\n\u2502  \u2502  \u2502 (my_task)\u2502 \u2502(process) \u2502 \u2502(extract) \u2502            \u2502    \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518            \u2502    \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502\n\u2502                                                              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"modules/08-kubernetes-executor/#3-helm-chart-deployment","title":"3. Helm Chart Deployment","text":""},{"location":"modules/08-kubernetes-executor/#prerequisites","title":"Prerequisites","text":"<pre><code># Verify tools\nkubectl version --client\nhelm version\n\n# Verify cluster access\nkubectl cluster-info\nkubectl get nodes\n</code></pre>"},{"location":"modules/08-kubernetes-executor/#basic-installation","title":"Basic Installation","text":"<pre><code># Add Apache Airflow Helm repo\nhelm repo add apache-airflow https://airflow.apache.org\nhelm repo update\n\n# Create namespace\nkubectl create namespace airflow\n\n# Install with defaults (for learning)\nhelm install airflow apache-airflow/airflow \\\n  --namespace airflow \\\n  --set executor=KubernetesExecutor\n\n# Watch pods come up\nkubectl get pods -n airflow -w\n</code></pre>"},{"location":"modules/08-kubernetes-executor/#access-the-ui","title":"Access the UI","text":"<pre><code># Port forward to webserver\nkubectl port-forward svc/airflow-webserver 8080:8080 -n airflow\n\n# Default credentials: admin / admin\n# Open: http://localhost:8080\n</code></pre>"},{"location":"modules/08-kubernetes-executor/#4-production-valuesyaml","title":"4. Production values.yaml","text":"<pre><code># infrastructure/helm/values.yaml\n\n# Core settings\nexecutor: KubernetesExecutor\ndefaultAirflowTag: \"3.0.2\"\nairflowVersion: \"3.0.2\"\n\n# Use standard naming (recommended)\nuseStandardNaming: true\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# DATABASE (External PostgreSQL - Required for Production)\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\npostgresql:\n  enabled: false # Disable built-in PostgreSQL\n\ndata:\n  metadataSecretName: airflow-database-secret\n\n# Connection pooling (highly recommended)\npgbouncer:\n  enabled: true\n  maxClientConn: 100\n  metadataPoolSize: 10\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# SCHEDULER\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nscheduler:\n  replicas: 2 # HA\n  resources:\n    requests:\n      cpu: 500m\n      memory: 1Gi\n    limits:\n      cpu: 2\n      memory: 4Gi\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# WEBSERVER / API SERVER\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nwebserver:\n  replicas: 2\n  resources:\n    requests:\n      cpu: 250m\n      memory: 512Mi\n    limits:\n      cpu: 1\n      memory: 2Gi\n  service:\n    type: ClusterIP # Use Ingress for external access\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# DAG SYNC (Git-Sync)\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ndags:\n  persistence:\n    enabled: false # Using git-sync instead\n  gitSync:\n    enabled: true\n    repo: \"git@github.com:your-org/airflow-dags.git\"\n    branch: main\n    subPath: \"dags\"\n    sshKeySecret: airflow-git-ssh-secret\n    wait: 60\n    containerName: git-sync\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# KUBERNETES EXECUTOR SETTINGS\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nworkers:\n  persistence:\n    enabled: false # KubernetesExecutor doesn't use workers deployment\n\n# Default pod template for tasks\nairflowPodAnnotations:\n  cluster-autoscaler.kubernetes.io/safe-to-evict: \"true\"\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# SECRETS\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nwebserverSecretKeySecretName: airflow-webserver-secret\nfernetKeySecretName: airflow-fernet-secret\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# LOGGING (Remote)\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nconfig:\n  logging:\n    remote_logging: \"True\"\n    remote_base_log_folder: \"s3://my-bucket/airflow-logs\"\n    remote_log_conn_id: \"aws_default\"\n\n  kubernetes:\n    delete_worker_pods: \"True\"\n    delete_worker_pods_on_failure: \"False\"\n    worker_pods_pending_timeout: 300\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# EXTRA ENVIRONMENT VARIABLES\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nenv:\n  - name: AIRFLOW__CORE__LOAD_EXAMPLES\n    value: \"False\"\n</code></pre>"},{"location":"modules/08-kubernetes-executor/#5-pod-templates","title":"5. Pod Templates","text":"<p>Customize task pod behavior:</p> <pre><code># infrastructure/helm/pod-template.yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: airflow-worker\n  annotations:\n    cluster-autoscaler.kubernetes.io/safe-to-evict: \"false\"\nspec:\n  serviceAccountName: airflow-worker\n\n  containers:\n    - name: base\n      image: apache/airflow:3.0.2\n      imagePullPolicy: IfNotPresent\n\n      resources:\n        requests:\n          cpu: \"500m\"\n          memory: \"512Mi\"\n        limits:\n          cpu: \"2\"\n          memory: \"2Gi\"\n\n      env:\n        - name: AIRFLOW__CORE__EXECUTOR\n          value: \"LocalExecutor\" # Inside the pod\n\n      volumeMounts:\n        - name: dags\n          mountPath: /opt/airflow/dags\n          readOnly: true\n        - name: logs\n          mountPath: /opt/airflow/logs\n\n  volumes:\n    - name: dags\n      emptyDir: {}\n    - name: logs\n      emptyDir: {}\n\n  restartPolicy: Never\n\n  # Security context\n  securityContext:\n    runAsUser: 50000\n    fsGroup: 0\n</code></pre>"},{"location":"modules/08-kubernetes-executor/#per-task-pod-customization","title":"Per-Task Pod Customization","text":"<pre><code>from airflow.sdk import DAG, task\nfrom kubernetes.client import models as k8s\n\n# Define custom pod configuration\npod_override = k8s.V1Pod(\n    spec=k8s.V1PodSpec(\n        containers=[\n            k8s.V1Container(\n                name=\"base\",\n                resources=k8s.V1ResourceRequirements(\n                    requests={\"cpu\": \"2\", \"memory\": \"4Gi\"}, limits={\"cpu\": \"4\", \"memory\": \"8Gi\"}\n                ),\n            )\n        ],\n        tolerations=[k8s.V1Toleration(key=\"high-memory\", operator=\"Equal\", value=\"true\", effect=\"NoSchedule\")],\n        node_selector={\"workload\": \"compute-intensive\"},\n    )\n)\n\nwith DAG(...):\n\n    @task(executor_config={\"pod_override\": pod_override})\n    def heavy_computation():\n        \"\"\"Task that needs more resources\"\"\"\n        pass\n</code></pre>"},{"location":"modules/08-kubernetes-executor/#6-secrets-management","title":"6. Secrets Management","text":""},{"location":"modules/08-kubernetes-executor/#creating-required-secrets","title":"Creating Required Secrets","text":"<pre><code># Database connection\nkubectl create secret generic airflow-database-secret \\\n  --from-literal=connection=postgresql://user:pass@host:5432/airflow \\\n  -n airflow\n\n# Webserver secret key (for session security)\nkubectl create secret generic airflow-webserver-secret \\\n  --from-literal=webserver-secret-key=$(python -c 'import secrets; print(secrets.token_hex(16))') \\\n  -n airflow\n\n# Fernet key (for connection encryption)\nkubectl create secret generic airflow-fernet-secret \\\n  --from-literal=fernet-key=$(python -c 'from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())') \\\n  -n airflow\n\n# Git SSH key for DAG sync\nkubectl create secret generic airflow-git-ssh-secret \\\n  --from-file=gitSshKey=/path/to/private/key \\\n  -n airflow\n</code></pre>"},{"location":"modules/08-kubernetes-executor/#using-secrets-in-dags","title":"Using Secrets in DAGs","text":"<pre><code>from airflow.models import Connection, Variable\nfrom airflow.sdk import task\n\n\n@task\ndef use_secrets():\n    # From Airflow Variables (stored in DB)\n    api_key = Variable.get(\"my_api_key\")\n\n    # From Airflow Connections\n    conn = Connection.get_connection_from_secrets(\"my_database\")\n\n    # From Kubernetes Secrets (as env vars in values.yaml)\n    import os\n\n    k8s_secret = os.environ.get(\"MY_K8S_SECRET\")\n</code></pre>"},{"location":"modules/08-kubernetes-executor/#7-debugging-kubernetes-deployments","title":"7. Debugging Kubernetes Deployments","text":""},{"location":"modules/08-kubernetes-executor/#common-commands","title":"Common Commands","text":"<pre><code># Check pod status\nkubectl get pods -n airflow\n\n# Check events (great for scheduling issues)\nkubectl get events -n airflow --sort-by='.lastTimestamp'\n\n# Scheduler logs\nkubectl logs -n airflow deployment/airflow-scheduler -c scheduler -f\n\n# API server logs\nkubectl logs -n airflow deployment/airflow-webserver -c webserver -f\n\n# Task pod logs (get pod name from UI or kubectl)\nkubectl logs -n airflow &lt;task-pod-name&gt;\n\n# Describe pod for details\nkubectl describe pod -n airflow &lt;pod-name&gt;\n\n# Shell into a pod\nkubectl exec -it -n airflow deployment/airflow-scheduler -- bash\n</code></pre>"},{"location":"modules/08-kubernetes-executor/#common-issues","title":"Common Issues","text":"Issue Symptom Solution Tasks stuck in \"queued\" Pods not starting Check scheduler logs, K8s events ImagePullBackOff Pod pending Check image name, registry auth CrashLoopBackOff Pod restarting Check container logs Database connection All pods crashing Verify secret, connection string DAGs not appearing UI empty Check git-sync logs, DAG folder"},{"location":"modules/08-kubernetes-executor/#validating-pod-template","title":"Validating Pod Template","text":"<pre><code># Generate pod YAML without running\nairflow kubernetes generate-dag-yaml my_dag my_task 2024-01-01\n\n# Review the output\n</code></pre>"},{"location":"modules/08-kubernetes-executor/#8-monitoring-and-observability","title":"8. Monitoring and Observability","text":""},{"location":"modules/08-kubernetes-executor/#enable-statsd","title":"Enable StatsD","text":"<pre><code># In values.yaml\nstatsd:\n  enabled: true\n\nconfig:\n  metrics:\n    statsd_on: \"True\"\n    statsd_host: \"localhost\"\n    statsd_port: 8125\n</code></pre>"},{"location":"modules/08-kubernetes-executor/#prometheus-integration","title":"Prometheus Integration","text":"<pre><code># In values.yaml\nextraEnv:\n  - name: AIRFLOW__METRICS__OTEL_ON\n    value: \"True\"\n  - name: AIRFLOW__METRICS__OTEL_HOST\n    value: \"otel-collector\"\n</code></pre>"},{"location":"modules/08-kubernetes-executor/#exercises","title":"\ud83d\udcdd Exercises","text":""},{"location":"modules/08-kubernetes-executor/#exercise-81-local-k8s-deployment","title":"Exercise 8.1: Local K8s Deployment","text":"<p>Using minikube or kind:</p> <ol> <li>Create a cluster</li> <li>Deploy Airflow with KubernetesExecutor</li> <li>Trigger a sample DAG</li> <li>Watch pods being created for tasks</li> </ol>"},{"location":"modules/08-kubernetes-executor/#exercise-82-custom-pod-template","title":"Exercise 8.2: Custom Pod Template","text":"<p>Create a DAG with:</p> <ol> <li>A task that requires 2GB memory</li> <li>A task that needs GPU (nodeSelector)</li> <li>A task with custom environment variables</li> </ol>"},{"location":"modules/08-kubernetes-executor/#exercise-83-production-checklist","title":"Exercise 8.3: Production Checklist","text":"<p>Prepare a production deployment:</p> <ol> <li>Configure external PostgreSQL</li> <li>Set up git-sync for DAGs</li> <li>Configure remote logging (S3/GCS)</li> <li>Enable PgBouncer</li> <li>Document all secrets created</li> </ol>"},{"location":"modules/08-kubernetes-executor/#checkpoint","title":"\u2705 Checkpoint","text":"<p>Before moving to Module 09, ensure you can:</p> <ul> <li> Deploy Airflow on Kubernetes using Helm</li> <li> Explain KubernetesExecutor pod lifecycle</li> <li> Configure values.yaml for production</li> <li> Create and use pod templates</li> <li> Manage secrets in Kubernetes</li> <li> Debug common K8s deployment issues</li> </ul>"},{"location":"modules/08-kubernetes-executor/#industry-spotlight-airbnb","title":"\ud83c\udfed Industry Spotlight: Airbnb","text":"<p>How Airbnb Scales Search Pipeline with KubernetesExecutor</p> <p>Airbnb's search infrastructure processes millions of queries daily, requiring elastic scaling to handle traffic spikes during peak booking seasons. The KubernetesExecutor enables dynamic resource allocation:</p> Challenge KubernetesExecutor Solution Traffic spikes Auto-scaling pods handle 10x load during peak seasons Resource isolation Each search pipeline task gets dedicated compute resources Cost optimization Spot instances for non-critical indexing tasks Multi-tenancy Namespace isolation between search, recommendations, and analytics <p>Pattern in Use: Airbnb-style dynamic resource allocation:</p> <pre><code>from airflow.sdk import DAG, task\nfrom kubernetes.client import models as k8s\n\n# Peak traffic configuration\npeak_resources = k8s.V1Pod(\n    spec=k8s.V1PodSpec(\n        containers=[\n            k8s.V1Container(\n                name=\"base\",\n                resources=k8s.V1ResourceRequirements(\n                    requests={\"cpu\": \"4\", \"memory\": \"8Gi\"}, limits={\"cpu\": \"8\", \"memory\": \"16Gi\"}\n                ),\n            )\n        ],\n        tolerations=[k8s.V1Toleration(key=\"workload\", value=\"search-indexing\", effect=\"NoSchedule\")],\n        node_selector={\"node-pool\": \"high-memory\"},\n    )\n)\n\nwith DAG(...):\n\n    @task(executor_config={\"pod_override\": peak_resources})\n    def reindex_search_listings():\n        \"\"\"Scale up during peak booking seasons.\"\"\"\n        return rebuild_search_index(region=\"global\")\n</code></pre> <p>Key Insight: KubernetesExecutor reduced Airbnb's infrastructure costs by 40% through intelligent pod scheduling and spot instance utilization, while maintaining 99.9% search availability.</p> <p>\ud83d\udcd6 Related Exercise: Exercise 8.2: Custom Pod Template - Configure resource-specific pod templates</p>"},{"location":"modules/08-kubernetes-executor/#further-reading","title":"\ud83d\udcda Further Reading","text":"<ul> <li>Kubernetes Executor Documentation</li> <li>Helm Chart Configuration</li> <li>Case Study: Airbnb Experimentation</li> </ul> <p>Next: Module 09: Production Patterns \u2192</p> <p>_[DAG]: Directed Acyclic Graph - Airflow's core workflow abstraction _[XCom]: Cross-Communication - Airflow's mechanism for task-to-task data passing _[Executor]: The component that runs tasks (Local, Celery, Kubernetes, etc.) _[Scheduler]: The component that triggers DAGs and determines task execution order _[Webserver]: The component that serves the Airflow UI _[Provider]: An Airflow package that integrates with external systems _[TaskFlow]: The modern Python-native API for defining Airflow tasks using decorators _[Asset]: A logical representation of data that DAGs can produce or consume _[SDK]: Software Development Kit - the airflow.sdk module in Airflow 3.x _[K8s]: Kubernetes - container orchestration platform _[uv]: A fast Python package manager by Astral _[ruff]: A fast Python linter and formatter by Astral _[CI]: Continuous Integration _[CD]: Continuous Deployment *[PR]: Pull Request</p>"},{"location":"modules/08-kubernetes-executor/exercises/exercise_8_1_local_deployment/","title":"Exercise 8.1: Local Kubernetes Deployment","text":""},{"location":"modules/08-kubernetes-executor/exercises/exercise_8_1_local_deployment/#objective","title":"Objective","text":"<p>Deploy Apache Airflow on a local Kubernetes cluster using minikube or kind, and understand the KubernetesExecutor task lifecycle.</p>"},{"location":"modules/08-kubernetes-executor/exercises/exercise_8_1_local_deployment/#background","title":"Background","text":"<p>The KubernetesExecutor runs each task in its own Pod, providing: - Complete task isolation - Dynamic resource allocation - Scale-to-zero capability when idle - No need for external message queues</p>"},{"location":"modules/08-kubernetes-executor/exercises/exercise_8_1_local_deployment/#architecture-overview","title":"Architecture Overview","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    Kubernetes Cluster                        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502  Scheduler  \u2502  \u2502  Webserver  \u2502  \u2502  Task Pods (Dynamic) \u2502  \u2502\n\u2502  \u2502    Pod      \u2502  \u2502    Pod      \u2502  \u2502  created per task    \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502         \u2502                                                    \u2502\n\u2502         \u25bc                                                    \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                            \u2502\n\u2502  \u2502  PostgreSQL \u2502  \u2190 Metadata Database                       \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"modules/08-kubernetes-executor/exercises/exercise_8_1_local_deployment/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker Desktop installed and running</li> <li>kubectl CLI installed</li> <li>Helm 3.x installed</li> <li>At least 8GB RAM available for local cluster</li> </ul>"},{"location":"modules/08-kubernetes-executor/exercises/exercise_8_1_local_deployment/#requirements","title":"Requirements","text":""},{"location":"modules/08-kubernetes-executor/exercises/exercise_8_1_local_deployment/#part-1-set-up-local-kubernetes-cluster","title":"Part 1: Set Up Local Kubernetes Cluster","text":"<p>Choose ONE of these options:</p> <p>Option A: minikube (Recommended for beginners) <pre><code># Install minikube\nbrew install minikube  # macOS\n# OR: choco install minikube  # Windows\n# OR: apt install minikube  # Linux\n\n# Start cluster with adequate resources\nminikube start --cpus=4 --memory=8192 --driver=docker\n\n# Verify cluster\nkubectl cluster-info\nkubectl get nodes\n</code></pre></p> <p>Option B: kind (Kubernetes IN Docker) <pre><code># Install kind\nbrew install kind  # macOS\n# OR: go install sigs.k8s.io/kind@latest\n\n# Create cluster\nkind create cluster --name airflow-cluster\n\n# Verify\nkubectl cluster-info --context kind-airflow-cluster\n</code></pre></p>"},{"location":"modules/08-kubernetes-executor/exercises/exercise_8_1_local_deployment/#part-2-deploy-airflow-with-helm","title":"Part 2: Deploy Airflow with Helm","text":"<ol> <li> <p>Add the Apache Airflow Helm repository <pre><code>helm repo add apache-airflow https://airflow.apache.org\nhelm repo update\n</code></pre></p> </li> <li> <p>Create namespace <pre><code>kubectl create namespace airflow\n</code></pre></p> </li> <li> <p>Create a basic values file (<code>local-values.yaml</code>): <pre><code># Exercise 8.1: Local Development Configuration\n\n# Use KubernetesExecutor\nexecutor: KubernetesExecutor\n\n# Airflow version\ndefaultAirflowTag: \"3.0.2\"\nairflowVersion: \"3.0.2\"\n\n# Minimal resources for local development\nscheduler:\n  replicas: 1\n  resources:\n    requests:\n      cpu: 500m\n      memory: 1Gi\n    limits:\n      cpu: 1\n      memory: 2Gi\n\nwebserver:\n  replicas: 1\n  resources:\n    requests:\n      cpu: 250m\n      memory: 512Mi\n    limits:\n      cpu: 500m\n      memory: 1Gi\n\n# Use built-in PostgreSQL for local dev\npostgresql:\n  enabled: true\n  resources:\n    requests:\n      cpu: 250m\n      memory: 256Mi\n\n# Disable features not needed locally\nstatsd:\n  enabled: false\n\nredis:\n  enabled: false\n\npgbouncer:\n  enabled: false\n\n# Enable example DAGs for testing\nenv:\n  - name: AIRFLOW__CORE__LOAD_EXAMPLES\n    value: \"True\"\n\n# Pod cleanup (useful for debugging)\nconfig:\n  kubernetes:\n    delete_worker_pods: \"True\"\n    delete_worker_pods_on_failure: \"False\"\n</code></pre></p> </li> <li> <p>Install Airflow <pre><code>helm install airflow apache-airflow/airflow \\\n  --namespace airflow \\\n  --values local-values.yaml \\\n  --timeout 10m\n</code></pre></p> </li> <li> <p>Watch deployment progress <pre><code>kubectl get pods -n airflow -w\n</code></pre></p> </li> </ol>"},{"location":"modules/08-kubernetes-executor/exercises/exercise_8_1_local_deployment/#part-3-access-and-verify","title":"Part 3: Access and Verify","text":"<ol> <li> <p>Port forward to the webserver <pre><code>kubectl port-forward svc/airflow-webserver 8080:8080 -n airflow\n</code></pre></p> </li> <li> <p>Access the UI at http://localhost:8080</p> </li> <li>Username: <code>admin</code></li> <li> <p>Password: Get from secret    <pre><code>kubectl get secret -n airflow airflow-webserver-secret -o jsonpath='{.data.webserver-secret-key}' | base64 --decode\n# Or for simpler setup, use:\necho \"Password for admin: admin (default Helm chart)\"\n</code></pre></p> </li> <li> <p>Verify scheduler is running <pre><code>kubectl logs -n airflow deployment/airflow-scheduler -c scheduler | tail -20\n</code></pre></p> </li> </ol>"},{"location":"modules/08-kubernetes-executor/exercises/exercise_8_1_local_deployment/#part-4-observe-kubernetesexecutor-in-action","title":"Part 4: Observe KubernetesExecutor in Action","text":"<ol> <li> <p>Trigger a sample DAG from the UI (e.g., <code>example_bash_operator</code>)</p> </li> <li> <p>Watch task pods being created <pre><code># In a separate terminal\nkubectl get pods -n airflow -w\n</code></pre></p> </li> <li> <p>Observe pod lifecycle:</p> </li> <li>Pod created when task starts</li> <li>Pod runs to completion</li> <li>Pod terminated (status: Succeeded or Failed)</li> <li> <p>Pod deleted (if <code>delete_worker_pods: \"True\"</code>)</p> </li> <li> <p>Check task pod logs (while it's running) <pre><code># Get pod name from kubectl get pods\nkubectl logs -n airflow &lt;task-pod-name&gt; -f\n</code></pre></p> </li> </ol>"},{"location":"modules/08-kubernetes-executor/exercises/exercise_8_1_local_deployment/#deliverables","title":"Deliverables","text":"<p>Create a file <code>exercise_8_1_answers.md</code> with:</p> <ol> <li>Screenshots or terminal output showing:</li> <li>Cluster running (<code>kubectl get nodes</code>)</li> <li>All Airflow pods healthy (<code>kubectl get pods -n airflow</code>)</li> <li> <p>Task pods being created during DAG execution</p> </li> <li> <p>Answers to these questions:</p> </li> <li>How many pods are created when you trigger a DAG with 3 tasks?</li> <li>What happens to task pods after they complete?</li> <li>What namespace are task pods created in?</li> <li> <p>How does the scheduler communicate with the Kubernetes API?</p> </li> <li> <p>Pod Lifecycle Observations:</p> </li> <li>How long did task pods take to start?</li> <li>What was the pod status progression?</li> <li>What happens if you trigger the same task again?</li> </ol>"},{"location":"modules/08-kubernetes-executor/exercises/exercise_8_1_local_deployment/#verification-commands","title":"Verification Commands","text":"<pre><code># Check all pods\nkubectl get pods -n airflow\n\n# Check scheduler health\nkubectl exec -n airflow deployment/airflow-scheduler -- \\\n  airflow jobs check --hostname \"\" --limit 1\n\n# View Airflow config\nkubectl exec -n airflow deployment/airflow-scheduler -- \\\n  airflow config get-value core executor\n\n# List DAGs\nkubectl exec -n airflow deployment/airflow-scheduler -- \\\n  airflow dags list\n</code></pre>"},{"location":"modules/08-kubernetes-executor/exercises/exercise_8_1_local_deployment/#cleanup","title":"Cleanup","text":"<pre><code># Delete Airflow installation\nhelm uninstall airflow -n airflow\n\n# Delete namespace\nkubectl delete namespace airflow\n\n# Stop minikube (if using)\nminikube stop\n\n# OR delete kind cluster\nkind delete cluster --name airflow-cluster\n</code></pre>"},{"location":"modules/08-kubernetes-executor/exercises/exercise_8_1_local_deployment/#hints","title":"Hints","text":"Hint 1: Pods stuck in Pending  Check cluster resources: <pre><code>kubectl describe pod &lt;pod-name&gt; -n airflow\nkubectl get events -n airflow --sort-by='.lastTimestamp'\n</code></pre>  If resources are insufficient, increase minikube resources: <pre><code>minikube stop\nminikube delete\nminikube start --cpus=4 --memory=8192\n</code></pre> Hint 2: ImagePullBackOff  This usually means the image can't be pulled. For local development: <pre><code># Verify Docker daemon connectivity\ndocker ps\n\n# For minikube, use Docker daemon inside minikube\neval $(minikube docker-env)\n</code></pre> Hint 3: Database connection errors  Check PostgreSQL pod: <pre><code>kubectl logs -n airflow airflow-postgresql-0\nkubectl describe pod -n airflow airflow-postgresql-0\n</code></pre>"},{"location":"modules/08-kubernetes-executor/exercises/exercise_8_1_local_deployment/#success-criteria","title":"Success Criteria","text":"<ul> <li> Local Kubernetes cluster running</li> <li> All Airflow pods in Running status</li> <li> Webserver accessible at localhost:8080</li> <li> Successfully triggered a DAG</li> <li> Observed task pods being created and terminated</li> <li> Documented findings in exercise_8_1_answers.md</li> </ul>"},{"location":"modules/08-kubernetes-executor/exercises/exercise_8_1_local_deployment/#additional-challenge","title":"Additional Challenge","text":"<p>Try deploying with your own DAG:</p> <ol> <li>Create a simple DAG file locally</li> <li>Copy it into the scheduler pod: <pre><code>kubectl cp my_dag.py airflow/airflow-scheduler-xxx:/opt/airflow/dags/\n</code></pre></li> <li>Wait for the scheduler to pick it up</li> <li>Trigger it and observe execution</li> </ol> <p>Next: Exercise 8.2: Custom Pod Templates \u2192</p> <p>_[DAG]: Directed Acyclic Graph - Airflow's core workflow abstraction _[XCom]: Cross-Communication - Airflow's mechanism for task-to-task data passing _[Executor]: The component that runs tasks (Local, Celery, Kubernetes, etc.) _[Scheduler]: The component that triggers DAGs and determines task execution order _[Webserver]: The component that serves the Airflow UI _[Provider]: An Airflow package that integrates with external systems _[TaskFlow]: The modern Python-native API for defining Airflow tasks using decorators _[Asset]: A logical representation of data that DAGs can produce or consume _[SDK]: Software Development Kit - the airflow.sdk module in Airflow 3.x _[K8s]: Kubernetes - container orchestration platform _[uv]: A fast Python package manager by Astral _[ruff]: A fast Python linter and formatter by Astral _[CI]: Continuous Integration _[CD]: Continuous Deployment *[PR]: Pull Request</p>"},{"location":"modules/08-kubernetes-executor/exercises/exercise_8_2_pod_templates/","title":"Exercise 8.2: Custom Pod Templates","text":""},{"location":"modules/08-kubernetes-executor/exercises/exercise_8_2_pod_templates/#objective","title":"Objective","text":"<p>Learn to customize Kubernetes task pods using pod templates and executor_config for resource allocation, node selection, and environment configuration.</p>"},{"location":"modules/08-kubernetes-executor/exercises/exercise_8_2_pod_templates/#background","title":"Background","text":"<p>KubernetesExecutor allows you to customize how task pods are created:</p> <ol> <li>Global Pod Template: Default settings for all tasks (via Helm values)</li> <li>Per-Task Override: Customize individual tasks with <code>executor_config</code></li> <li>Kubernetes Python Client: Programmatic pod specification</li> </ol>"},{"location":"modules/08-kubernetes-executor/exercises/exercise_8_2_pod_templates/#use-cases-for-custom-pods","title":"Use Cases for Custom Pods","text":"Requirement Solution High memory task Custom resource limits GPU workload Node selector + tolerations Task-specific secrets Volume mounts Sidecar containers Multi-container pod Network policies Pod annotations"},{"location":"modules/08-kubernetes-executor/exercises/exercise_8_2_pod_templates/#requirements","title":"Requirements","text":""},{"location":"modules/08-kubernetes-executor/exercises/exercise_8_2_pod_templates/#part-1-create-a-global-pod-template","title":"Part 1: Create a Global Pod Template","text":"<p>Create a default pod template that all tasks will use:</p> <p>File: <code>pod-template.yaml</code> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: default-airflow-worker\n  labels:\n    app: airflow-worker\nspec:\n  serviceAccountName: airflow-worker\n  restartPolicy: Never\n\n  # Security best practices\n  securityContext:\n    runAsUser: 50000\n    runAsGroup: 0\n    fsGroup: 0\n\n  containers:\n    - name: base\n      image: apache/airflow:3.0.2\n      imagePullPolicy: IfNotPresent\n\n      # Default resources\n      resources:\n        requests:\n          cpu: \"250m\"\n          memory: \"256Mi\"\n        limits:\n          cpu: \"1\"\n          memory: \"1Gi\"\n\n      # Required volume mounts\n      volumeMounts:\n        - name: dags\n          mountPath: /opt/airflow/dags\n          readOnly: true\n        - name: logs\n          mountPath: /opt/airflow/logs\n\n  volumes:\n    - name: dags\n      emptyDir: {}\n    - name: logs\n      emptyDir: {}\n</code></pre></p>"},{"location":"modules/08-kubernetes-executor/exercises/exercise_8_2_pod_templates/#part-2-create-per-task-pod-configurations","title":"Part 2: Create Per-Task Pod Configurations","text":"<p>Create a DAG with tasks that have different resource requirements:</p> <p>File: <code>custom_pod_templates_dag.py</code> (starter in exercises/)</p> <pre><code>\"\"\"\nExercise 8.2: Custom Pod Templates\n==================================\n\nCreate a DAG demonstrating different pod configurations:\n1. Standard task (default resources)\n2. High-memory task (4Gi memory)\n3. GPU task (node selector)\n4. Task with custom environment variables\n5. Task with init container\n\"\"\"\n\nfrom airflow.sdk import dag, task\nfrom datetime import datetime\n\n# TODO: Import Kubernetes client models\n# from kubernetes.client import models as k8s\n\n@dag(\n    dag_id=\"exercise_8_2_custom_pods\",\n    start_date=datetime(2024, 1, 1),\n    schedule=None,\n    catchup=False,\n    tags=[\"exercise\", \"kubernetes\"],\n)\ndef custom_pods_dag():\n    \"\"\"DAG demonstrating custom pod configurations.\"\"\"\n\n    # Task 1: Standard task with default resources\n    @task\n    def standard_task():\n        \"\"\"Uses default pod template.\"\"\"\n        import os\n        print(f\"Running on node: {os.environ.get('HOSTNAME', 'unknown')}\")\n        return {\"status\": \"completed\"}\n\n    # TODO: Task 2: High memory task\n    # Create a task that requests 4Gi memory and 2 CPU cores\n    # Use executor_config with pod_override\n\n    # TODO: Task 3: GPU task\n    # Create a task that:\n    # - Uses node selector: {\"gpu\": \"true\"}\n    # - Has toleration for GPU nodes\n\n    # TODO: Task 4: Task with environment variables\n    # Create a task that has custom env vars injected\n    # - API_KEY from a Kubernetes secret\n    # - ENVIRONMENT = \"production\"\n\n    # TODO: Task 5: Task with init container\n    # Create a task with an init container that downloads data\n\n    # Wire up dependencies\n    result = standard_task()\n    # TODO: Add other tasks to the dependency chain\n\ncustom_pods_dag()\n</code></pre>"},{"location":"modules/08-kubernetes-executor/exercises/exercise_8_2_pod_templates/#part-3-implement-resource-intensive-pod-configuration","title":"Part 3: Implement Resource-Intensive Pod Configuration","text":"<pre><code>from kubernetes.client import models as k8s\n\n# High-memory pod configuration\nhigh_memory_pod = k8s.V1Pod(\n    spec=k8s.V1PodSpec(\n        containers=[\n            k8s.V1Container(\n                name=\"base\",\n                resources=k8s.V1ResourceRequirements(\n                    requests={\n                        \"cpu\": \"2\",\n                        \"memory\": \"4Gi\"\n                    },\n                    limits={\n                        \"cpu\": \"4\",\n                        \"memory\": \"8Gi\"\n                    }\n                )\n            )\n        ]\n    )\n)\n\n@task(executor_config={\"pod_override\": high_memory_pod})\ndef high_memory_task():\n    \"\"\"Task with increased memory allocation.\"\"\"\n    import resource\n    # Verify we can use more memory\n    mem_limit = resource.getrlimit(resource.RLIMIT_AS)\n    print(f\"Memory limit: {mem_limit}\")\n    return {\"memory_available\": \"4Gi\"}\n</code></pre>"},{"location":"modules/08-kubernetes-executor/exercises/exercise_8_2_pod_templates/#part-4-gpu-node-selection","title":"Part 4: GPU Node Selection","text":"<pre><code># GPU-enabled pod configuration\ngpu_pod = k8s.V1Pod(\n    spec=k8s.V1PodSpec(\n        containers=[\n            k8s.V1Container(\n                name=\"base\",\n                resources=k8s.V1ResourceRequirements(\n                    limits={\"nvidia.com/gpu\": \"1\"}\n                )\n            )\n        ],\n        node_selector={\"accelerator\": \"nvidia-tesla-t4\"},\n        tolerations=[\n            k8s.V1Toleration(\n                key=\"nvidia.com/gpu\",\n                operator=\"Equal\",\n                value=\"true\",\n                effect=\"NoSchedule\"\n            )\n        ]\n    )\n)\n\n@task(executor_config={\"pod_override\": gpu_pod})\ndef gpu_task():\n    \"\"\"Task that runs on GPU nodes.\"\"\"\n    # Check for GPU\n    import subprocess\n    result = subprocess.run([\"nvidia-smi\"], capture_output=True, text=True)\n    print(result.stdout)\n    return {\"gpu\": \"available\"}\n</code></pre>"},{"location":"modules/08-kubernetes-executor/exercises/exercise_8_2_pod_templates/#part-5-secret-injection","title":"Part 5: Secret Injection","text":"<pre><code># Pod with secrets mounted as environment variables\nsecret_pod = k8s.V1Pod(\n    spec=k8s.V1PodSpec(\n        containers=[\n            k8s.V1Container(\n                name=\"base\",\n                env=[\n                    k8s.V1EnvVar(\n                        name=\"API_KEY\",\n                        value_from=k8s.V1EnvVarSource(\n                            secret_key_ref=k8s.V1SecretKeySelector(\n                                name=\"api-credentials\",\n                                key=\"api-key\"\n                            )\n                        )\n                    ),\n                    k8s.V1EnvVar(\n                        name=\"ENVIRONMENT\",\n                        value=\"production\"\n                    )\n                ]\n            )\n        ]\n    )\n)\n\n@task(executor_config={\"pod_override\": secret_pod})\ndef secure_api_task():\n    \"\"\"Task with secrets from Kubernetes.\"\"\"\n    import os\n    api_key = os.environ.get(\"API_KEY\")\n    # Use api_key securely (don't log it!)\n    print(f\"API key present: {bool(api_key)}\")\n    return {\"authenticated\": True}\n</code></pre>"},{"location":"modules/08-kubernetes-executor/exercises/exercise_8_2_pod_templates/#deliverables","title":"Deliverables","text":"<ol> <li><code>pod-template.yaml</code> - Global default pod template</li> <li><code>custom_pod_templates_dag.py</code> - Complete DAG with 5 different pod configurations</li> <li><code>pod-configs/</code> directory with:</li> <li><code>high-memory-pod.yaml</code></li> <li><code>gpu-pod.yaml</code></li> <li><code>secret-pod.yaml</code></li> </ol>"},{"location":"modules/08-kubernetes-executor/exercises/exercise_8_2_pod_templates/#testing-your-configuration","title":"Testing Your Configuration","text":""},{"location":"modules/08-kubernetes-executor/exercises/exercise_8_2_pod_templates/#generate-pod-yaml-dry-run","title":"Generate Pod YAML (dry-run)","text":"<pre><code># Inside scheduler pod or with Airflow CLI\nairflow kubernetes generate-dag-yaml \\\n    exercise_8_2_custom_pods \\\n    high_memory_task \\\n    2024-01-01\n\n# Review generated pod spec\n</code></pre>"},{"location":"modules/08-kubernetes-executor/exercises/exercise_8_2_pod_templates/#verify-resource-allocation","title":"Verify Resource Allocation","text":"<pre><code># Get running task pod\nkubectl get pods -n airflow -l dag_id=exercise_8_2_custom_pods\n\n# Describe pod to see resource allocation\nkubectl describe pod &lt;pod-name&gt; -n airflow\n\n# Check actual resource usage\nkubectl top pod &lt;pod-name&gt; -n airflow\n</code></pre>"},{"location":"modules/08-kubernetes-executor/exercises/exercise_8_2_pod_templates/#hints","title":"Hints","text":"Hint 1: Importing Kubernetes models <pre><code>from kubernetes.client import models as k8s\n\n# Available models for pod configuration:\n# - k8s.V1Pod\n# - k8s.V1PodSpec\n# - k8s.V1Container\n# - k8s.V1ResourceRequirements\n# - k8s.V1EnvVar\n# - k8s.V1EnvVarSource\n# - k8s.V1SecretKeySelector\n# - k8s.V1Toleration\n# - k8s.V1Volume\n# - k8s.V1VolumeMount\n</code></pre> Hint 2: Combining multiple configurations <pre><code># You can specify multiple container settings\ncombined_pod = k8s.V1Pod(\n    spec=k8s.V1PodSpec(\n        containers=[\n            k8s.V1Container(\n                name=\"base\",\n                resources=k8s.V1ResourceRequirements(...),\n                env=[k8s.V1EnvVar(...)],\n                volume_mounts=[k8s.V1VolumeMount(...)]\n            )\n        ],\n        volumes=[k8s.V1Volume(...)],\n        node_selector={...},\n        tolerations=[...]\n    )\n)\n</code></pre> Hint 3: Init containers <pre><code>init_container_pod = k8s.V1Pod(\n    spec=k8s.V1PodSpec(\n        init_containers=[\n            k8s.V1Container(\n                name=\"download-data\",\n                image=\"alpine/curl\",\n                command=[\"curl\", \"-o\", \"/data/file.csv\", \"https://...\"],\n                volume_mounts=[\n                    k8s.V1VolumeMount(name=\"shared-data\", mount_path=\"/data\")\n                ]\n            )\n        ],\n        containers=[\n            k8s.V1Container(\n                name=\"base\",\n                volume_mounts=[\n                    k8s.V1VolumeMount(name=\"shared-data\", mount_path=\"/data\")\n                ]\n            )\n        ],\n        volumes=[\n            k8s.V1Volume(\n                name=\"shared-data\",\n                empty_dir=k8s.V1EmptyDirVolumeSource()\n            )\n        ]\n    )\n)\n</code></pre>"},{"location":"modules/08-kubernetes-executor/exercises/exercise_8_2_pod_templates/#success-criteria","title":"Success Criteria","text":"<ul> <li> Global pod template created and applied</li> <li> High-memory task runs with 4Gi memory limit</li> <li> GPU task correctly targets GPU nodes (or simulated)</li> <li> Secret injection working via environment variables</li> <li> Init container prepares data before main task</li> <li> All pod configurations verified via <code>kubectl describe</code></li> </ul> <p>Next: Exercise 8.3: Production Checklist \u2192</p> <p>_[DAG]: Directed Acyclic Graph - Airflow's core workflow abstraction _[XCom]: Cross-Communication - Airflow's mechanism for task-to-task data passing _[Executor]: The component that runs tasks (Local, Celery, Kubernetes, etc.) _[Scheduler]: The component that triggers DAGs and determines task execution order _[Webserver]: The component that serves the Airflow UI _[Provider]: An Airflow package that integrates with external systems _[TaskFlow]: The modern Python-native API for defining Airflow tasks using decorators _[Asset]: A logical representation of data that DAGs can produce or consume _[SDK]: Software Development Kit - the airflow.sdk module in Airflow 3.x _[K8s]: Kubernetes - container orchestration platform _[uv]: A fast Python package manager by Astral _[ruff]: A fast Python linter and formatter by Astral _[CI]: Continuous Integration _[CD]: Continuous Deployment *[PR]: Pull Request</p>"},{"location":"modules/08-kubernetes-executor/exercises/exercise_8_3_production_checklist/","title":"Exercise 8.3: Production Deployment Checklist","text":""},{"location":"modules/08-kubernetes-executor/exercises/exercise_8_3_production_checklist/#objective","title":"Objective","text":"<p>Prepare a production-ready Airflow deployment on Kubernetes by configuring external databases, git-sync DAG deployment, remote logging, connection pooling, and comprehensive secrets management.</p>"},{"location":"modules/08-kubernetes-executor/exercises/exercise_8_3_production_checklist/#background","title":"Background","text":"<p>Production Airflow deployments require careful attention to:</p> Area Development Production Database Built-in PostgreSQL External managed DB DAG Sync Manual copy Git-sync with CI/CD Logging Local files Remote storage (S3/GCS) Secrets Plain text Kubernetes Secrets + External vaults Scaling Single replica HA with PgBouncer Monitoring None StatsD + Prometheus"},{"location":"modules/08-kubernetes-executor/exercises/exercise_8_3_production_checklist/#requirements","title":"Requirements","text":"<p>Complete each section of this production checklist, documenting your configuration choices.</p>"},{"location":"modules/08-kubernetes-executor/exercises/exercise_8_3_production_checklist/#part-1-external-postgresql-configuration","title":"Part 1: External PostgreSQL Configuration","text":"<p>Why: Built-in PostgreSQL lacks persistence, backups, and HA.</p> <p>Task: Configure Airflow to use an external PostgreSQL database.</p> <ol> <li> <p>Create database secret: <pre><code># Create the secret with your connection string\nkubectl create secret generic airflow-database-secret \\\n  --namespace airflow \\\n  --from-literal=connection='postgresql://airflow:YOUR_PASSWORD@postgres-host.example.com:5432/airflow?sslmode=require'\n</code></pre></p> </li> <li> <p>Update values.yaml: <pre><code># Disable built-in PostgreSQL\npostgresql:\n  enabled: false\n\n# Reference external database\ndata:\n  metadataSecretName: airflow-database-secret\n  # For Airflow 3.x, ensure connection format is correct\n  # postgresql+psycopg2://... or postgresql://...\n</code></pre></p> </li> <li> <p>Verify connection: <pre><code># Test from scheduler pod\nkubectl exec -n airflow deployment/airflow-scheduler -- \\\n  airflow db check\n</code></pre></p> </li> </ol> <p>Documentation: Record your database configuration choices: - [ ] Database provider (AWS RDS, GCP Cloud SQL, Azure, self-hosted) - [ ] SSL/TLS enabled - [ ] Connection pooling at DB level - [ ] Backup strategy</p>"},{"location":"modules/08-kubernetes-executor/exercises/exercise_8_3_production_checklist/#part-2-git-sync-dag-deployment","title":"Part 2: Git-Sync DAG Deployment","text":"<p>Why: Enables GitOps workflow, version control, and CI/CD integration.</p> <p>Task: Configure git-sync to automatically pull DAGs from a repository.</p> <ol> <li> <p>Generate SSH key for git access: <pre><code># Generate deploy key\nssh-keygen -t ed25519 -C \"airflow-deploy\" -f airflow_deploy_key\n\n# Create Kubernetes secret\nkubectl create secret generic airflow-git-ssh-secret \\\n  --namespace airflow \\\n  --from-file=gitSshKey=airflow_deploy_key\n</code></pre></p> </li> <li> <p>Add deploy key to your repository (read-only access)</p> </li> <li> <p>Update values.yaml: <pre><code>dags:\n  persistence:\n    enabled: false  # Using git-sync instead\n\n  gitSync:\n    enabled: true\n    repo: \"git@github.com:your-org/airflow-dags.git\"\n    branch: main\n    rev: HEAD\n    subPath: \"dags\"  # Path to DAGs in repo\n\n    # Authentication\n    sshKeySecret: airflow-git-ssh-secret\n\n    # Sync configuration\n    wait: 60  # Sync interval in seconds\n    containerName: git-sync\n\n    # For HTTPS repos (alternative to SSH)\n    # credentialsSecret: airflow-git-credentials\n    # knownHosts: |\n    #   github.com ssh-rsa AAAA...\n</code></pre></p> </li> <li> <p>Verify sync: <pre><code># Check git-sync container logs\nkubectl logs -n airflow deployment/airflow-scheduler -c git-sync -f\n\n# Verify DAGs are loaded\nkubectl exec -n airflow deployment/airflow-scheduler -- \\\n  airflow dags list\n</code></pre></p> </li> </ol> <p>Documentation: - [ ] Repository URL - [ ] Branch strategy (main, staging, prod branches?) - [ ] Sync interval choice rationale - [ ] CI/CD pipeline for DAG validation</p>"},{"location":"modules/08-kubernetes-executor/exercises/exercise_8_3_production_checklist/#part-3-remote-logging-s3gcs","title":"Part 3: Remote Logging (S3/GCS)","text":"<p>Why: Task pods are ephemeral; logs must persist externally.</p> <p>Task: Configure remote logging to cloud storage.</p> <p>For AWS S3: <pre><code># values.yaml\nconfig:\n  logging:\n    remote_logging: \"True\"\n    remote_log_conn_id: \"aws_default\"\n    remote_base_log_folder: \"s3://my-airflow-logs-bucket/logs\"\n    # Optional: encrypt logs\n    encrypt_s3_logs: \"True\"\n\n# Create AWS credentials secret\n# Option 1: Access keys (not recommended for production)\nkubectl create secret generic aws-credentials \\\n  --namespace airflow \\\n  --from-literal=aws-access-key-id=AKIAXXXXXXXX \\\n  --from-literal=aws-secret-access-key=XXXXXXXXXX\n\n# Option 2: Use IRSA (IAM Roles for Service Accounts) - RECOMMENDED\n# Configure serviceAccount with IAM role annotation\n</code></pre></p> <p>For GCP GCS: <pre><code>config:\n  logging:\n    remote_logging: \"True\"\n    remote_log_conn_id: \"google_cloud_default\"\n    remote_base_log_folder: \"gs://my-airflow-logs-bucket/logs\"\n\n# Create GCP credentials\nkubectl create secret generic gcp-credentials \\\n  --namespace airflow \\\n  --from-file=key.json=service-account-key.json\n</code></pre></p> <p>Verify logging: <pre><code># Trigger a task and check logs appear in cloud storage\n# Check for errors in scheduler logs\nkubectl logs -n airflow deployment/airflow-scheduler -c scheduler | grep -i logging\n</code></pre></p> <p>Documentation: - [ ] Storage provider and bucket name - [ ] Log retention policy - [ ] Access control (IAM roles, service accounts) - [ ] Log encryption configuration</p>"},{"location":"modules/08-kubernetes-executor/exercises/exercise_8_3_production_checklist/#part-4-enable-pgbouncer-connection-pooling","title":"Part 4: Enable PgBouncer (Connection Pooling)","text":"<p>Why: Prevents database connection exhaustion under load.</p> <p>Task: Enable and configure PgBouncer.</p> <pre><code># values.yaml\npgbouncer:\n  enabled: true\n\n  # Pool sizing\n  maxClientConn: 100\n  metadataPoolSize: 10\n  resultBackendPoolSize: 5\n\n  # Authentication\n  auth_type: scram-sha-256\n\n  # Resources\n  resources:\n    requests:\n      cpu: 100m\n      memory: 128Mi\n    limits:\n      cpu: 500m\n      memory: 256Mi\n</code></pre> <p>Verify PgBouncer: <pre><code># Check PgBouncer is running\nkubectl get pods -n airflow | grep pgbouncer\n\n# View PgBouncer stats\nkubectl exec -n airflow deployment/airflow-pgbouncer -- \\\n  psql -h localhost -p 6543 -U pgbouncer pgbouncer -c \"SHOW POOLS\"\n</code></pre></p> <p>Documentation: - [ ] Pool size calculations - [ ] Expected concurrent connections - [ ] Monitoring metrics to watch</p>"},{"location":"modules/08-kubernetes-executor/exercises/exercise_8_3_production_checklist/#part-5-security-secrets-management","title":"Part 5: Security &amp; Secrets Management","text":"<p>Task: Create and document all required secrets.</p> <p>Required Secrets Checklist:</p> <pre><code># 1. Webserver secret key (session security)\nkubectl create secret generic airflow-webserver-secret \\\n  --namespace airflow \\\n  --from-literal=webserver-secret-key=$(openssl rand -hex 32)\n\n# 2. Fernet key (connection encryption)\nFERNET_KEY=$(python -c \"from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())\")\nkubectl create secret generic airflow-fernet-secret \\\n  --namespace airflow \\\n  --from-literal=fernet-key=$FERNET_KEY\n\n# 3. Database connection (from Part 1)\n# Already created: airflow-database-secret\n\n# 4. Git SSH key (from Part 2)\n# Already created: airflow-git-ssh-secret\n\n# 5. Cloud provider credentials (from Part 3)\n# AWS: aws-credentials or IRSA\n# GCP: gcp-credentials or Workload Identity\n</code></pre> <p>Reference secrets in values.yaml: <pre><code>webserverSecretKeySecretName: airflow-webserver-secret\nfernetKeySecretName: airflow-fernet-secret\n</code></pre></p> <p>Documentation: - [ ] List all secrets created - [ ] Rotation policy for each - [ ] Access control (RBAC) - [ ] Consider external secrets (HashiCorp Vault, AWS Secrets Manager)</p>"},{"location":"modules/08-kubernetes-executor/exercises/exercise_8_3_production_checklist/#part-6-high-availability-configuration","title":"Part 6: High Availability Configuration","text":"<p>Task: Configure scheduler and webserver for HA.</p> <pre><code># values.yaml\nscheduler:\n  replicas: 2  # HA - active-active in Airflow 3.x\n\n  # Leader election for deduplication\n  extraEnv:\n    - name: AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK\n      value: \"True\"\n\n  resources:\n    requests:\n      cpu: 500m\n      memory: 1Gi\n    limits:\n      cpu: 2\n      memory: 4Gi\n\n  # Pod disruption budget\n  podDisruptionBudget:\n    enabled: true\n    minAvailable: 1\n\nwebserver:\n  replicas: 2\n\n  resources:\n    requests:\n      cpu: 250m\n      memory: 512Mi\n    limits:\n      cpu: 1\n      memory: 2Gi\n\n  podDisruptionBudget:\n    enabled: true\n    minAvailable: 1\n</code></pre>"},{"location":"modules/08-kubernetes-executor/exercises/exercise_8_3_production_checklist/#part-7-monitoring-setup","title":"Part 7: Monitoring Setup","text":"<p>Task: Configure metrics export.</p> <pre><code># values.yaml\n\n# StatsD metrics\nstatsd:\n  enabled: true\n\nconfig:\n  metrics:\n    statsd_on: \"True\"\n    statsd_host: \"localhost\"\n    statsd_port: \"8125\"\n    statsd_prefix: \"airflow\"\n\n# Or OpenTelemetry (Airflow 3.x)\nextraEnv:\n  - name: AIRFLOW__METRICS__OTEL_ON\n    value: \"True\"\n  - name: AIRFLOW__METRICS__OTEL_HOST\n    value: \"otel-collector.monitoring\"\n  - name: AIRFLOW__METRICS__OTEL_PORT\n    value: \"4318\"\n</code></pre>"},{"location":"modules/08-kubernetes-executor/exercises/exercise_8_3_production_checklist/#deliverables","title":"Deliverables","text":"<p>Create a production deployment package:</p> <pre><code>production-deployment/\n\u251c\u2500\u2500 values-production.yaml      # Complete Helm values\n\u251c\u2500\u2500 secrets/\n\u2502   \u2514\u2500\u2500 secrets-manifest.yaml   # Secret creation commands (sanitized)\n\u251c\u2500\u2500 RUNBOOK.md                  # Operational runbook\n\u2514\u2500\u2500 CHECKLIST.md                # Completed checklist with notes\n</code></pre>"},{"location":"modules/08-kubernetes-executor/exercises/exercise_8_3_production_checklist/#checklistmd-template","title":"CHECKLIST.md Template","text":"<pre><code># Production Deployment Checklist\n\n## Pre-Deployment\n- [ ] Database provisioned and tested\n- [ ] Storage bucket created with correct permissions\n- [ ] Git repository with deploy key configured\n- [ ] All secrets created in Kubernetes\n\n## Database\n- [ ] Provider: _______________\n- [ ] SSL enabled: Yes / No\n- [ ] Backup frequency: _______________\n- [ ] Connection string verified: Yes / No\n\n## DAG Synchronization\n- [ ] Repository: _______________\n- [ ] Branch: _______________\n- [ ] Sync interval: ___ seconds\n- [ ] CI/CD pipeline configured: Yes / No\n\n## Logging\n- [ ] Storage provider: _______________\n- [ ] Bucket: _______________\n- [ ] Retention: ___ days\n- [ ] Encryption: Yes / No\n\n## Connection Pooling\n- [ ] PgBouncer enabled: Yes / No\n- [ ] Max connections: ___\n- [ ] Pool size: ___\n\n## Secrets\n| Secret Name | Purpose | Rotation Policy |\n|-------------|---------|-----------------|\n| airflow-database-secret | DB connection | Quarterly |\n| airflow-webserver-secret | Session security | Annual |\n| airflow-fernet-secret | Encryption | Never (break existing) |\n| airflow-git-ssh-secret | DAG sync | Annual |\n\n## High Availability\n- [ ] Scheduler replicas: ___\n- [ ] Webserver replicas: ___\n- [ ] PDB configured: Yes / No\n\n## Monitoring\n- [ ] Metrics enabled: Yes / No\n- [ ] Exporter: StatsD / OTEL\n- [ ] Dashboards created: Yes / No\n\n## Post-Deployment Verification\n- [ ] All pods healthy\n- [ ] DAGs synced successfully\n- [ ] Test task executed\n- [ ] Logs appearing in remote storage\n- [ ] Metrics visible in monitoring\n</code></pre>"},{"location":"modules/08-kubernetes-executor/exercises/exercise_8_3_production_checklist/#success-criteria","title":"Success Criteria","text":"<ul> <li> Complete values-production.yaml with all configurations</li> <li> All secrets created and referenced correctly</li> <li> Git-sync working with your DAG repository</li> <li> Remote logging verified in cloud storage</li> <li> PgBouncer operational</li> <li> HA configuration applied</li> <li> Monitoring metrics flowing</li> <li> Comprehensive documentation in CHECKLIST.md</li> </ul>"},{"location":"modules/08-kubernetes-executor/exercises/exercise_8_3_production_checklist/#hints","title":"Hints","text":"Hint 1: Testing database connection <pre><code># Test from local machine first\npsql \"postgresql://user:pass@host:5432/airflow?sslmode=require\"\n\n# Or use Kubernetes job\nkubectl run pg-test --rm -it --restart=Never \\\n  --image=postgres:15 -- \\\n  psql \"postgresql://user:pass@host:5432/airflow\"\n</code></pre> Hint 2: Debugging git-sync <pre><code># Check git-sync logs\nkubectl logs -n airflow deployment/airflow-scheduler -c git-sync\n\n# Common issues:\n# - SSH key permissions: must be 0600\n# - Known hosts not configured\n# - Wrong branch name\n</code></pre> Hint 3: Verifying remote logging <pre><code># Trigger a simple DAG\nkubectl exec -n airflow deployment/airflow-scheduler -- \\\n  airflow dags trigger example_bash_operator\n\n# Check cloud storage for logs\naws s3 ls s3://my-bucket/logs/\n# OR\ngsutil ls gs://my-bucket/logs/\n</code></pre>"},{"location":"modules/08-kubernetes-executor/exercises/exercise_8_3_production_checklist/#additional-challenge","title":"Additional Challenge","text":"<p>Implement external secrets management using one of:</p> <ol> <li>HashiCorp Vault: External Secrets Operator with Vault backend</li> <li>AWS Secrets Manager: ESO with AWS provider</li> <li>Azure Key Vault: ESO with Azure provider</li> </ol> <pre><code># Example: External Secrets Operator\napiVersion: external-secrets.io/v1beta1\nkind: ExternalSecret\nmetadata:\n  name: airflow-database-secret\n  namespace: airflow\nspec:\n  refreshInterval: 1h\n  secretStoreRef:\n    name: vault-backend\n    kind: ClusterSecretStore\n  target:\n    name: airflow-database-secret\n  data:\n    - secretKey: connection\n      remoteRef:\n        key: airflow/database\n        property: connection_string\n</code></pre>"},{"location":"modules/08-kubernetes-executor/exercises/exercise_8_3_production_checklist/#references","title":"References","text":"<ul> <li>Apache Airflow Helm Chart Documentation</li> <li>Production Deployment Guide</li> <li>Kubernetes Executor Configuration</li> </ul> <p>_[DAG]: Directed Acyclic Graph - Airflow's core workflow abstraction _[XCom]: Cross-Communication - Airflow's mechanism for task-to-task data passing _[Executor]: The component that runs tasks (Local, Celery, Kubernetes, etc.) _[Scheduler]: The component that triggers DAGs and determines task execution order _[Webserver]: The component that serves the Airflow UI _[Provider]: An Airflow package that integrates with external systems _[TaskFlow]: The modern Python-native API for defining Airflow tasks using decorators _[Asset]: A logical representation of data that DAGs can produce or consume _[SDK]: Software Development Kit - the airflow.sdk module in Airflow 3.x _[K8s]: Kubernetes - container orchestration platform _[uv]: A fast Python package manager by Astral _[ruff]: A fast Python linter and formatter by Astral _[CI]: Continuous Integration _[CD]: Continuous Deployment *[PR]: Pull Request</p>"},{"location":"modules/09-production-patterns/","title":"Module 09: Production Patterns","text":""},{"location":"modules/09-production-patterns/#learning-objectives","title":"\ud83c\udfaf Learning Objectives","text":"<p>By the end of this module, you will:</p> <ul> <li>Design DAGs for reliability and maintainability</li> <li>Implement proper error handling and alerting</li> <li>Set up CI/CD pipelines for DAG deployment</li> <li>Monitor Airflow performance and health</li> <li>Apply organizational patterns for large-scale Airflow</li> </ul>"},{"location":"modules/09-production-patterns/#estimated-time-5-6-hours","title":"\u23f1\ufe0f Estimated Time: 5-6 hours","text":""},{"location":"modules/09-production-patterns/#1-dag-design-patterns","title":"1. DAG Design Patterns","text":""},{"location":"modules/09-production-patterns/#idempotent-tasks","title":"Idempotent Tasks","text":"<p>Tasks should produce the same result when run multiple times:</p> <pre><code>@task\ndef load_data(date: str):\n    \"\"\"Idempotent: overwrites partition for the date\"\"\"\n    # \u2705 Good: Delete before insert\n    db.execute(f\"DELETE FROM table WHERE date = '{date}'\")\n    db.execute(f\"INSERT INTO table SELECT * FROM source WHERE date = '{date}'\")\n\n    # \u274c Bad: Append without checking\n    # db.execute(f\"INSERT INTO table SELECT * FROM source WHERE date = '{date}'\")\n</code></pre>"},{"location":"modules/09-production-patterns/#atomic-operations","title":"Atomic Operations","text":"<p>All-or-nothing execution:</p> <pre><code>@task\ndef atomic_operation():\n    \"\"\"Use transactions for atomicity\"\"\"\n    with db.begin_transaction() as txn:\n        try:\n            txn.execute(\"INSERT INTO table_a ...\")\n            txn.execute(\"UPDATE table_b ...\")\n            txn.execute(\"DELETE FROM table_c ...\")\n            txn.commit()\n        except Exception:\n            txn.rollback()\n            raise\n</code></pre>"},{"location":"modules/09-production-patterns/#small-focused-tasks","title":"Small, Focused Tasks","text":"<pre><code># \u2705 Good: Single responsibility\n@task\ndef extract_users():\n    return fetch_users()\n\n\n@task\ndef validate_users(users):\n    return [u for u in users if u[\"email\"]]\n\n\n@task\ndef transform_users(valid_users):\n    return [enrich(u) for u in valid_users]\n\n\n# \u274c Bad: Monolithic task\n@task\ndef do_everything():\n    users = fetch_users()\n    valid = [u for u in users if u[\"email\"]]\n    enriched = [enrich(u) for u in valid]\n    save(enriched)\n    notify()\n</code></pre>"},{"location":"modules/09-production-patterns/#2-error-handling-retries","title":"2. Error Handling &amp; Retries","text":""},{"location":"modules/09-production-patterns/#retry-configuration","title":"Retry Configuration","text":"<pre><code>from datetime import timedelta\n\n\n@task(\n    retries=3,\n    retry_delay=timedelta(minutes=5),\n    retry_exponential_backoff=True,\n    max_retry_delay=timedelta(hours=1),\n)\ndef flaky_api_call():\n    \"\"\"Retries with exponential backoff\"\"\"\n    response = requests.get(\"https://api.example.com/data\")\n    response.raise_for_status()\n    return response.json()\n</code></pre>"},{"location":"modules/09-production-patterns/#custom-retry-logic","title":"Custom Retry Logic","text":"<pre><code>from airflow.exceptions import AirflowException\n\n\n@task(retries=3)\ndef smart_retry():\n    try:\n        result = call_api()\n    except RateLimitError:\n        # Don't retry rate limits - wait longer\n        raise AirflowException(\"Rate limited, will retry with backoff\")\n    except AuthenticationError:\n        # Don't retry auth failures\n        raise AirflowException(\"Auth failed - check credentials\")\n    except ConnectionError:\n        # Retry connection issues\n        raise\n    return result\n</code></pre>"},{"location":"modules/09-production-patterns/#callbacks-for-notifications","title":"Callbacks for Notifications","text":"<pre><code>from airflow.sdk import DAG\nfrom datetime import datetime\n\ndef on_failure(context):\n    \"\"\"Called when a task fails\"\"\"\n    task_instance = context['ti']\n    dag_id = context['dag'].dag_id\n\n    send_slack_alert(\n        channel=\"#airflow-alerts\",\n        message=f\"Task {task_instance.task_id} failed in {dag_id}\",\n        details=str(context.get('exception'))\n    )\n\ndef on_success(context):\n    \"\"\"Called when DAG succeeds\"\"\"\n    send_slack_message(\"#airflow-status\", \"Pipeline completed successfully\")\n\nwith DAG(\n    dag_id=\"production_pipeline\",\n    default_args={\n        \"on_failure_callback\": on_failure,\n    },\n    on_success_callback=on_success,  # DAG-level callback\n    ...\n):\n    pass\n</code></pre>"},{"location":"modules/09-production-patterns/#3-alerting-notifications","title":"3. Alerting &amp; Notifications","text":""},{"location":"modules/09-production-patterns/#slack-integration","title":"Slack Integration","text":"<pre><code>from airflow.providers.slack.hooks.slack_webhook import SlackWebhookHook\n\n\ndef send_slack_alert(context):\n    hook = SlackWebhookHook(slack_webhook_conn_id=\"slack_webhook\")\n\n    ti = context[\"ti\"]\n    log_url = ti.log_url\n\n    message = {\n        \"blocks\": [\n            {\n                \"type\": \"section\",\n                \"text\": {\"type\": \"mrkdwn\", \"text\": f\":x: *Task Failed*\\n*DAG:* {ti.dag_id}\\n*Task:* {ti.task_id}\"},\n            },\n            {\n                \"type\": \"actions\",\n                \"elements\": [{\"type\": \"button\", \"text\": {\"type\": \"plain_text\", \"text\": \"View Logs\"}, \"url\": log_url}],\n            },\n        ]\n    }\n\n    hook.send_dict(message)\n</code></pre>"},{"location":"modules/09-production-patterns/#deadline-alerts-airflow-3","title":"Deadline Alerts (Airflow 3)","text":"<pre><code>from datetime import timedelta\n\nwith DAG(\n    dag_id=\"sla_dag\",\n    dagrun_timeout=timedelta(hours=2),  # Entire DAG timeout\n    ...\n):\n    @task(execution_timeout=timedelta(minutes=30))\n    def time_bounded_task():\n        \"\"\"Fails if exceeds 30 minutes\"\"\"\n        pass\n</code></pre>"},{"location":"modules/09-production-patterns/#4-cicd-for-dags","title":"4. CI/CD for DAGs","text":""},{"location":"modules/09-production-patterns/#deployment-strategies","title":"Deployment Strategies","text":"Strategy Description Use Case Git-sync Continuous pull from Git Simple, real-time updates Docker Image Bake DAGs into image Immutable, versioned CI/CD Push Deploy on merge Controlled releases"},{"location":"modules/09-production-patterns/#github-actions-pipeline","title":"GitHub Actions Pipeline","text":"<pre><code># .github/workflows/deploy-dags.yml\nname: Deploy DAGs\n\non:\n  push:\n    branches: [main]\n    paths: [\"dags/**\"]\n\njobs:\n  validate:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Set up Python\n        uses: actions/setup-python@v5\n        with:\n          python-version: \"3.11\"\n\n      - name: Install dependencies\n        run: pip install apache-airflow ruff pytest\n\n      - name: Lint\n        run: ruff check dags/ --select AIR\n\n      - name: Test DAG integrity\n        run: |\n          export AIRFLOW_HOME=$(pwd)/.airflow\n          airflow db init\n          python -c \"from airflow.models import DagBag; db=DagBag('dags/'); assert not db.import_errors\"\n\n      - name: Run unit tests\n        run: pytest tests/ -v\n\n  deploy:\n    needs: validate\n    runs-on: ubuntu-latest\n    if: github.ref == 'refs/heads/main'\n    steps:\n      - uses: actions/checkout@v4\n\n      # Option 1: Sync to S3 (if using S3 for DAG storage)\n      - name: Sync to S3\n        uses: aws-actions/configure-aws-credentials@v4\n        with:\n          aws-region: us-east-1\n      - run: aws s3 sync dags/ s3://my-airflow-dags/dags/\n\n      # Option 2: Trigger Helm upgrade with new image\n      # - name: Deploy to K8s\n      #   run: |\n      #     helm upgrade airflow apache-airflow/airflow \\\n      #       --set images.airflow.tag=${{ github.sha }} \\\n      #       -n airflow\n</code></pre>"},{"location":"modules/09-production-patterns/#blue-green-deployments","title":"Blue-Green Deployments","text":"<pre><code># Deploy to staging first\nstaging:\n  runs-on: ubuntu-latest\n  steps:\n    - name: Deploy to staging\n      run: |\n        kubectl config use-context staging\n        helm upgrade airflow-staging ...\n\n# After validation, deploy to production\nproduction:\n  needs: staging\n  runs-on: ubuntu-latest\n  environment: production # Requires approval\n  steps:\n    - name: Deploy to production\n      run: |\n        kubectl config use-context production\n        helm upgrade airflow-production ...\n</code></pre>"},{"location":"modules/09-production-patterns/#5-monitoring-observability","title":"5. Monitoring &amp; Observability","text":""},{"location":"modules/09-production-patterns/#key-metrics-to-monitor","title":"Key Metrics to Monitor","text":"Metric Meaning Alert Threshold <code>scheduler_heartbeat</code> Scheduler is alive Missing for 60s <code>dag_processing.total_parse_time</code> DAG parsing time &gt; 30s <code>executor.queued_tasks</code> Tasks waiting &gt; 100 for 10m <code>pool_running_slots</code> Pool usage &gt; 90% capacity <code>task_instance_failures</code> Failed tasks &gt; 5/hour"},{"location":"modules/09-production-patterns/#prometheus-grafana","title":"Prometheus + Grafana","text":"<pre><code># values.yaml additions\nstatsd:\n  enabled: true\n\nextraEnv:\n  - name: AIRFLOW__METRICS__STATSD_ON\n    value: \"True\"\n  - name: AIRFLOW__METRICS__STATSD_HOST\n    value: \"prometheus-statsd-exporter\"\n</code></pre>"},{"location":"modules/09-production-patterns/#health-checks","title":"Health Checks","text":"<pre><code># Custom health check DAG\nfrom datetime import datetime\n\nfrom airflow.sdk import DAG, task\n\nwith DAG(\n    dag_id=\"health_check\",\n    schedule=\"*/5 * * * *\",  # Every 5 minutes\n    start_date=datetime(2024, 1, 1),\n    max_active_runs=1,\n    catchup=False,\n):\n\n    @task\n    def check_database():\n        from airflow.providers.postgres.hooks.postgres import PostgresHook\n\n        hook = PostgresHook(postgres_conn_id=\"warehouse\")\n        hook.run(\"SELECT 1\")\n        return \"db_ok\"\n\n    @task\n    def check_s3():\n        from airflow.providers.amazon.aws.hooks.s3 import S3Hook\n\n        hook = S3Hook(aws_conn_id=\"aws_default\")\n        hook.check_for_bucket(\"my-bucket\")\n        return \"s3_ok\"\n\n    @task\n    def report_health(results: list):\n        if all(r.endswith(\"_ok\") for r in results):\n            print(\"All systems healthy\")\n        else:\n            raise Exception(\"Health check failed\")\n\n    report_health([check_database(), check_s3()])\n</code></pre>"},{"location":"modules/09-production-patterns/#6-organizational-patterns","title":"6. Organizational Patterns","text":""},{"location":"modules/09-production-patterns/#dag-naming-conventions","title":"DAG Naming Conventions","text":"<pre><code>{domain}_{action}_{target}[_{version}]\n\nExamples:\n- analytics_load_daily_sales\n- marketing_sync_hubspot_contacts\n- finance_generate_monthly_report_v2\n- infra_cleanup_old_logs\n</code></pre>"},{"location":"modules/09-production-patterns/#folder-structure-for-large-teams","title":"Folder Structure for Large Teams","text":"<pre><code>dags/\n\u251c\u2500\u2500 analytics/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 daily_metrics.py\n\u2502   \u2514\u2500\u2500 weekly_reports.py\n\u251c\u2500\u2500 marketing/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2514\u2500\u2500 campaign_sync.py\n\u251c\u2500\u2500 shared/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 operators/\n\u2502   \u2502   \u2514\u2500\u2500 custom_operator.py\n\u2502   \u251c\u2500\u2500 hooks/\n\u2502   \u2502   \u2514\u2500\u2500 custom_hook.py\n\u2502   \u2514\u2500\u2500 utils/\n\u2502       \u2514\u2500\u2500 helpers.py\n\u2514\u2500\u2500 config/\n    \u251c\u2500\u2500 dev.yaml\n    \u251c\u2500\u2500 staging.yaml\n    \u2514\u2500\u2500 prod.yaml\n</code></pre>"},{"location":"modules/09-production-patterns/#configuration-management","title":"Configuration Management","text":"<pre><code># dags/shared/config.py\nimport os\n\nimport yaml\n\n\ndef load_config():\n    env = os.getenv(\"AIRFLOW_ENV\", \"dev\")\n    config_path = f\"/opt/airflow/dags/config/{env}.yaml\"\n\n    with open(config_path) as f:\n        return yaml.safe_load(f)\n\n\nCONFIG = load_config()\n\n# Usage in DAG\nfrom shared.config import CONFIG\n\n\n@task\ndef process():\n    database = CONFIG[\"database\"][\"host\"]\n    api_key = CONFIG[\"api\"][\"key\"]\n</code></pre>"},{"location":"modules/09-production-patterns/#7-performance-optimization","title":"7. Performance Optimization","text":""},{"location":"modules/09-production-patterns/#dag-parsing-performance","title":"DAG Parsing Performance","text":"<pre><code># \u2705 Fast: Import inside functions\n@task\ndef process():\n    pass\n    # ...\n\n\n# \u274c Slow: Top-level imports of heavy libraries\n</code></pre>"},{"location":"modules/09-production-patterns/#reduce-dag-count","title":"Reduce DAG Count","text":"<pre><code># \u274c Bad: One DAG per table (100 DAGs)\nfor table in get_tables():\n    with DAG(f\"sync_{table}\", ...):\n        ...\n\n# \u2705 Better: One DAG with dynamic tasks\nwith DAG(\"sync_all_tables\", ...):\n\n    @task\n    def get_tables():\n        return [\"table1\", \"table2\", ...]\n\n    @task\n    def sync_table(table: str): ...\n\n    sync_table.expand(table=get_tables())\n</code></pre>"},{"location":"modules/09-production-patterns/#pool-management","title":"Pool Management","text":"<pre><code># Create pools via UI or API\n# Admin \u2192 Pools \u2192 Add\n\n# Limit concurrent tasks\n@task(pool=\"api_pool\")  # Max concurrent API calls\ndef call_rate_limited_api():\n    pass\n</code></pre>"},{"location":"modules/09-production-patterns/#exercises","title":"\ud83d\udcdd Exercises","text":""},{"location":"modules/09-production-patterns/#exercise-91-production-ready-dag","title":"Exercise 9.1: Production-Ready DAG","text":"<p>Take any DAG from previous modules and add:</p> <ul> <li>Proper retry configuration</li> <li>Failure callbacks with Slack notification</li> <li>Execution timeouts</li> <li>Logging best practices</li> </ul>"},{"location":"modules/09-production-patterns/#exercise-92-cicd-pipeline","title":"Exercise 9.2: CI/CD Pipeline","text":"<p>Create a GitHub Actions workflow that:</p> <ul> <li>Runs on PR to validate DAGs</li> <li>Runs on merge to main to deploy</li> <li>Includes DAG integrity tests</li> </ul>"},{"location":"modules/09-production-patterns/#exercise-93-monitoring-dashboard","title":"Exercise 9.3: Monitoring Dashboard","text":"<p>Design (and optionally implement):</p> <ul> <li>Key metrics to track</li> <li>Alert thresholds</li> <li>A Grafana dashboard layout</li> </ul>"},{"location":"modules/09-production-patterns/#checkpoint","title":"\u2705 Checkpoint","text":"<p>Before moving to Module 10, ensure you can:</p> <ul> <li> Design idempotent and atomic tasks</li> <li> Configure retries with exponential backoff</li> <li> Set up failure callbacks and alerts</li> <li> Create a CI/CD pipeline for DAG deployment</li> <li> Identify key monitoring metrics</li> <li> Apply organizational patterns for DAG management</li> </ul>"},{"location":"modules/09-production-patterns/#industry-spotlight-spotify","title":"\ud83c\udfed Industry Spotlight: Spotify","text":"<p>How Spotify Orchestrates Production ML Pipelines</p> <p>Spotify runs thousands of ML models powering personalized recommendations, podcast discovery, and audio analysis. Production patterns ensure reliability at scale:</p> Challenge Production Pattern Solution API rate limits Exponential backoff with jitter prevents thundering herd Model freshness Scheduled retraining with staleness alerts Cost visibility Callbacks track compute costs per pipeline Failure isolation Circuit breakers prevent cascade failures <p>Pattern in Use: Spotify-style production ML orchestration:</p> <pre><code>from datetime import timedelta\n\nfrom airflow.sdk import task\n\n\ndef cost_tracking_callback(context):\n    \"\"\"Track pipeline execution costs.\"\"\"\n    task_instance = context[\"ti\"]\n    duration = task_instance.duration\n\n    # Calculate and log costs\n    compute_cost = calculate_compute_cost(duration, context[\"params\"][\"instance_type\"])\n    log_to_cost_dashboard(dag_id=context[\"dag\"].dag_id, cost=compute_cost, timestamp=context[\"logical_date\"])\n\n\n@task(\n    retries=5,\n    retry_delay=timedelta(seconds=30),\n    retry_exponential_backoff=True,\n    max_retry_delay=timedelta(minutes=10),\n    on_success_callback=cost_tracking_callback,\n    execution_timeout=timedelta(hours=2),\n)\ndef train_recommendation_model(user_segment: str):\n    \"\"\"Production ML training with full observability.\"\"\"\n    with circuit_breaker(\"ml-training-service\"):\n        model = train_model(user_segment)\n        validate_model_quality(model, min_accuracy=0.85)\n        return deploy_model(model)\n</code></pre> <p>Key Insight: Spotify's production patterns reduced incident response time by 70% and prevented $2M+ in potential revenue loss through early anomaly detection.</p> <p>\ud83d\udcd6 Related Exercise: Exercise 9.4: LLM Retry Patterns - Apply production patterns to AI/ML workloads</p>"},{"location":"modules/09-production-patterns/#further-reading","title":"\ud83d\udcda Further Reading","text":"<ul> <li>Best Practices Documentation</li> <li>Monitoring &amp; Observability</li> <li>Case Study: Spotify Recommendations</li> </ul> <p>Next: Module 10: Advanced Topics \u2192</p> <p>_[DAG]: Directed Acyclic Graph - Airflow's core workflow abstraction _[XCom]: Cross-Communication - Airflow's mechanism for task-to-task data passing _[Executor]: The component that runs tasks (Local, Celery, Kubernetes, etc.) _[Scheduler]: The component that triggers DAGs and determines task execution order _[Webserver]: The component that serves the Airflow UI _[Provider]: An Airflow package that integrates with external systems _[TaskFlow]: The modern Python-native API for defining Airflow tasks using decorators _[Asset]: A logical representation of data that DAGs can produce or consume _[SDK]: Software Development Kit - the airflow.sdk module in Airflow 3.x _[K8s]: Kubernetes - container orchestration platform _[uv]: A fast Python package manager by Astral _[ruff]: A fast Python linter and formatter by Astral _[CI]: Continuous Integration _[CD]: Continuous Deployment *[PR]: Pull Request</p>"},{"location":"modules/09-production-patterns/exercises/exercise_9_1_production_dag/","title":"Exercise 9.1: Production-Ready DAG","text":""},{"location":"modules/09-production-patterns/exercises/exercise_9_1_production_dag/#objective","title":"Objective","text":"<p>Transform a basic DAG into a production-ready pipeline with proper error handling, retries, timeouts, alerting, and logging best practices.</p>"},{"location":"modules/09-production-patterns/exercises/exercise_9_1_production_dag/#background","title":"Background","text":"<p>Production DAGs require additional considerations beyond basic functionality:</p> Aspect Development Production Retries None or minimal Configured with backoff Timeouts None Task and DAG timeouts Alerts Console logs Slack/PagerDuty/Email Logging Print statements Structured logging Error Handling Basic try/except Comprehensive with context"},{"location":"modules/09-production-patterns/exercises/exercise_9_1_production_dag/#requirements","title":"Requirements","text":"<p>Take the following basic DAG and transform it into a production-ready pipeline:</p>"},{"location":"modules/09-production-patterns/exercises/exercise_9_1_production_dag/#starting-dag-to-transform","title":"Starting DAG (to transform)","text":"<pre><code>\"\"\"Basic ETL DAG - needs production hardening\"\"\"\n\nfrom airflow.sdk import dag, task\nfrom datetime import datetime\nimport requests\n\n@dag(\n    dag_id=\"basic_etl\",\n    start_date=datetime(2024, 1, 1),\n    schedule=\"@daily\",\n    catchup=False,\n)\ndef basic_etl():\n    @task\n    def extract():\n        response = requests.get(\"https://api.example.com/data\")\n        return response.json()\n\n    @task\n    def transform(data):\n        return [item[\"value\"] * 2 for item in data]\n\n    @task\n    def load(transformed_data):\n        print(f\"Loading {len(transformed_data)} records\")\n        # Simulate database insert\n        return {\"records_loaded\": len(transformed_data)}\n\n    data = extract()\n    transformed = transform(data)\n    load(transformed)\n\nbasic_etl()\n</code></pre>"},{"location":"modules/09-production-patterns/exercises/exercise_9_1_production_dag/#production-requirements","title":"Production Requirements","text":"<ol> <li>Retry Configuration</li> <li>Add retries with exponential backoff for the extract task</li> <li>Configure <code>retries=3</code>, <code>retry_delay=timedelta(minutes=1)</code></li> <li>Add <code>retry_exponential_backoff=True</code></li> <li> <p>Set <code>max_retry_delay=timedelta(minutes=30)</code></p> </li> <li> <p>Timeouts</p> </li> <li>Add <code>execution_timeout</code> to each task (appropriate for task type)</li> <li> <p>Add <code>dagrun_timeout</code> to the DAG</p> </li> <li> <p>Failure Callbacks</p> </li> <li>Implement <code>on_failure_callback</code> that sends a Slack notification</li> <li>Include DAG ID, task ID, execution date, and error message</li> <li> <p>Provide a link to the task logs</p> </li> <li> <p>Success Callback</p> </li> <li>Implement <code>on_success_callback</code> at DAG level</li> <li> <p>Report pipeline completion time and records processed</p> </li> <li> <p>Logging Best Practices</p> </li> <li>Use proper logging instead of print statements</li> <li>Include structured information (record counts, timing)</li> <li> <p>Log at appropriate levels (INFO, WARNING, ERROR)</p> </li> <li> <p>Error Handling</p> </li> <li>Add proper exception handling with context</li> <li>Distinguish between retryable and non-retryable errors</li> <li>Include meaningful error messages</li> </ol>"},{"location":"modules/09-production-patterns/exercises/exercise_9_1_production_dag/#starter-code","title":"Starter Code","text":"<p>See <code>exercise_9_1_production_dag_starter.py</code></p>"},{"location":"modules/09-production-patterns/exercises/exercise_9_1_production_dag/#deliverables","title":"Deliverables","text":"<ol> <li><code>solution_9_1_production_dag.py</code> - Complete production DAG</li> <li><code>alerting.py</code> - Reusable callback functions</li> </ol>"},{"location":"modules/09-production-patterns/exercises/exercise_9_1_production_dag/#hints","title":"Hints","text":"Hint 1: Slack callback function <pre><code>def send_slack_alert(context):\n    from airflow.providers.slack.hooks.slack_webhook import SlackWebhookHook\n\n    ti = context.get('ti')\n    dag_id = context.get('dag').dag_id\n    task_id = ti.task_id\n    execution_date = context.get('logical_date')\n    log_url = ti.log_url\n    exception = context.get('exception')\n\n    message = f\"\"\"\n    :x: *Task Failed*\n    *DAG:* {dag_id}\n    *Task:* {task_id}\n    *Execution Date:* {execution_date}\n    *Error:* {str(exception)[:200]}\n    &lt;{log_url}|View Logs&gt;\n    \"\"\"\n\n    hook = SlackWebhookHook(slack_webhook_conn_id='slack_webhook')\n    hook.send(text=message)\n</code></pre> Hint 2: Structured logging <pre><code>import logging\n\nlogger = logging.getLogger(__name__)\n\n@task\ndef my_task():\n    logger.info(\"Starting task\", extra={\n        \"record_count\": 100,\n        \"source\": \"api\"\n    })\n</code></pre> Hint 3: Error categorization <pre><code>from airflow.exceptions import AirflowException, AirflowSkipException\n\n@task(retries=3)\ndef smart_task():\n    try:\n        result = api_call()\n    except RateLimitError as e:\n        # Retryable - let Airflow retry mechanism handle it\n        raise\n    except AuthenticationError as e:\n        # Non-retryable - fail immediately\n        raise AirflowException(f\"Auth failed: {e}\") from None\n    except DataNotFoundError:\n        # Skip this run\n        raise AirflowSkipException(\"No data available\")\n</code></pre>"},{"location":"modules/09-production-patterns/exercises/exercise_9_1_production_dag/#success-criteria","title":"Success Criteria","text":"<ul> <li> DAG has appropriate retry configuration with backoff</li> <li> All tasks have execution timeouts</li> <li> DAG has overall timeout</li> <li> Failure callback sends Slack notification with context</li> <li> Success callback reports completion</li> <li> Uses proper logging instead of print</li> <li> Error handling distinguishes error types</li> <li> Code is clean and well-documented</li> </ul> <p>Next: Exercise 9.2: CI/CD Pipeline \u2192</p> <p>_[DAG]: Directed Acyclic Graph - Airflow's core workflow abstraction _[XCom]: Cross-Communication - Airflow's mechanism for task-to-task data passing _[Executor]: The component that runs tasks (Local, Celery, Kubernetes, etc.) _[Scheduler]: The component that triggers DAGs and determines task execution order _[Webserver]: The component that serves the Airflow UI _[Provider]: An Airflow package that integrates with external systems _[TaskFlow]: The modern Python-native API for defining Airflow tasks using decorators _[Asset]: A logical representation of data that DAGs can produce or consume _[SDK]: Software Development Kit - the airflow.sdk module in Airflow 3.x _[K8s]: Kubernetes - container orchestration platform _[uv]: A fast Python package manager by Astral _[ruff]: A fast Python linter and formatter by Astral _[CI]: Continuous Integration _[CD]: Continuous Deployment *[PR]: Pull Request</p>"},{"location":"modules/09-production-patterns/exercises/exercise_9_2_cicd_pipeline/","title":"Exercise 9.2: CI/CD Pipeline for DAG Deployment","text":""},{"location":"modules/09-production-patterns/exercises/exercise_9_2_cicd_pipeline/#objective","title":"Objective","text":"<p>Create a complete CI/CD pipeline using GitHub Actions that validates, tests, and deploys Airflow DAGs automatically.</p>"},{"location":"modules/09-production-patterns/exercises/exercise_9_2_cicd_pipeline/#background","title":"Background","text":"<p>CI/CD for Airflow DAGs ensures: - DAGs are syntactically correct before deployment - No import errors in DAG files - Unit tests pass - Code quality standards are met - Deployments are automated and consistent</p>"},{"location":"modules/09-production-patterns/exercises/exercise_9_2_cicd_pipeline/#pipeline-stages","title":"Pipeline Stages","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Lint &amp;    \u2502\u2500\u2500\u2500\u25b6\u2502  Validate \u2502\u2500\u2500\u2500\u25b6\u2502  Test   \u2502\u2500\u2500\u2500\u25b6\u2502   Deploy   \u2502\n\u2502   Format    \u2502    \u2502   DAGs    \u2502    \u2502         \u2502    \u2502            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n     \u2502                  \u2502               \u2502               \u2502\n     \u25bc                  \u25bc               \u25bc               \u25bc\n  ruff check       DagBag load     pytest run     sync to prod\n</code></pre>"},{"location":"modules/09-production-patterns/exercises/exercise_9_2_cicd_pipeline/#requirements","title":"Requirements","text":""},{"location":"modules/09-production-patterns/exercises/exercise_9_2_cicd_pipeline/#part-1-create-validation-workflow","title":"Part 1: Create Validation Workflow","text":"<p>Create <code>.github/workflows/validate-dags.yml</code>:</p> <pre><code>name: Validate DAGs\n\non:\n  pull_request:\n    branches: [main]\n    paths:\n      - 'dags/**'\n      - 'tests/**'\n      - 'requirements.txt'\n\njobs:\n  validate:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Set up Python\n        uses: actions/setup-python@v5\n        with:\n          python-version: '3.11'\n\n      - name: Install dependencies\n        run: |\n          pip install apache-airflow==3.0.2 ruff pytest\n\n      - name: Lint with Ruff\n        run: |\n          ruff check dags/ --select AIR\n          ruff format dags/ --check\n\n      - name: Validate DAG integrity\n        run: |\n          export AIRFLOW_HOME=$(pwd)/.airflow\n          airflow db migrate\n          python -c \"\n          from airflow.models import DagBag\n          dag_bag = DagBag(dag_folder='dags/', include_examples=False)\n          if dag_bag.import_errors:\n              for path, error in dag_bag.import_errors.items():\n                  print(f'ERROR in {path}:')\n                  print(error)\n              exit(1)\n          print(f'Successfully loaded {len(dag_bag.dags)} DAGs')\n          \"\n\n      - name: Run unit tests\n        run: pytest tests/ -v --tb=short\n</code></pre>"},{"location":"modules/09-production-patterns/exercises/exercise_9_2_cicd_pipeline/#part-2-create-deployment-workflow","title":"Part 2: Create Deployment Workflow","text":"<p>Create <code>.github/workflows/deploy-dags.yml</code>:</p> <pre><code>name: Deploy DAGs\n\non:\n  push:\n    branches: [main]\n    paths:\n      - 'dags/**'\n\njobs:\n  # TODO: Implement the deployment job\n  # Should:\n  # 1. Run validation first\n  # 2. Deploy to staging environment\n  # 3. Run smoke tests\n  # 4. Deploy to production (with approval)\n</code></pre>"},{"location":"modules/09-production-patterns/exercises/exercise_9_2_cicd_pipeline/#part-3-dag-integrity-test-script","title":"Part 3: DAG Integrity Test Script","text":"<p>Create <code>tests/test_dag_integrity.py</code>:</p> <pre><code>\"\"\"\nDAG Integrity Tests\n\nThese tests ensure all DAGs are valid and follow best practices.\nRun during CI/CD to catch issues before deployment.\n\"\"\"\n\nimport pytest\nfrom datetime import datetime, timedelta\nfrom airflow.models import DagBag\n\n\n@pytest.fixture(scope=\"session\")\ndef dag_bag():\n    \"\"\"Load all DAGs once for the test session.\"\"\"\n    return DagBag(dag_folder=\"dags/\", include_examples=False)\n\n\nclass TestDagIntegrity:\n    \"\"\"Test suite for DAG integrity.\"\"\"\n\n    def test_no_import_errors(self, dag_bag):\n        \"\"\"Verify all DAGs load without import errors.\"\"\"\n        # TODO: Implement\n        pass\n\n    def test_dag_ids_unique(self, dag_bag):\n        \"\"\"Verify all DAG IDs are unique.\"\"\"\n        # TODO: Implement\n        pass\n\n    def test_all_dags_have_description(self, dag_bag):\n        \"\"\"Verify all DAGs have descriptions.\"\"\"\n        # TODO: Implement\n        pass\n\n    def test_all_dags_have_owner(self, dag_bag):\n        \"\"\"Verify all DAGs have an owner defined.\"\"\"\n        # TODO: Implement\n        pass\n\n    def test_no_cycles(self, dag_bag):\n        \"\"\"Verify no DAGs have circular dependencies.\"\"\"\n        # TODO: Implement\n        pass\n\n    def test_start_dates_not_dynamic(self, dag_bag):\n        \"\"\"Verify start_date is not datetime.now().\"\"\"\n        # TODO: Implement\n        pass\n</code></pre>"},{"location":"modules/09-production-patterns/exercises/exercise_9_2_cicd_pipeline/#part-4-deployment-strategy-configuration","title":"Part 4: Deployment Strategy Configuration","text":"<p>Choose and implement a deployment strategy:</p> <p>Option A: Git-Sync (Continuous) - DAGs sync automatically from Git - Configure in Helm values.yaml</p> <p>Option B: S3/GCS Sync (On-Demand) - DAGs copied to cloud storage - Airflow reads from storage</p> <p>Option C: Container Image (Immutable) - DAGs baked into Airflow image - Full control over versions</p>"},{"location":"modules/09-production-patterns/exercises/exercise_9_2_cicd_pipeline/#deliverables","title":"Deliverables","text":"<ol> <li><code>.github/workflows/validate-dags.yml</code> - PR validation workflow</li> <li><code>.github/workflows/deploy-dags.yml</code> - Deployment workflow</li> <li><code>tests/test_dag_integrity.py</code> - Complete integrity test suite</li> <li><code>DEPLOYMENT.md</code> - Documentation of deployment strategy</li> </ol>"},{"location":"modules/09-production-patterns/exercises/exercise_9_2_cicd_pipeline/#starter-files","title":"Starter Files","text":"<p>See <code>exercise_9_2_cicd_starter/</code> directory for: - Workflow templates - Test file skeleton - Sample configuration</p>"},{"location":"modules/09-production-patterns/exercises/exercise_9_2_cicd_pipeline/#hints","title":"Hints","text":"Hint 1: Ruff AIR rules  Ruff has Airflow-specific rules: <pre><code># Check for Airflow best practices\nruff check dags/ --select AIR\n\n# AIR001: Task variable name different from task_id\n# AIR002: Task has no owner set\n# etc.\n</code></pre> Hint 2: Test for dynamic start_date <pre><code>def test_start_dates_not_dynamic(self, dag_bag):\n    \"\"\"Dynamic start_date causes issues with catchup.\"\"\"\n    for dag_id, dag in dag_bag.dags.items():\n        assert dag.start_date is not None, f\"{dag_id} has no start_date\"\n        # Start date should be in the past\n        assert dag.start_date &lt; datetime.now(), \\\n            f\"{dag_id} has future start_date\"\n</code></pre> Hint 3: GitHub environments for approval <pre><code>deploy-production:\n  needs: deploy-staging\n  environment: production  # Requires approval\n  steps:\n    - name: Deploy to production\n      run: ...\n</code></pre>"},{"location":"modules/09-production-patterns/exercises/exercise_9_2_cicd_pipeline/#success-criteria","title":"Success Criteria","text":"<ul> <li> PR workflow validates DAG syntax and imports</li> <li> PR workflow runs linting with Airflow rules</li> <li> PR workflow executes unit tests</li> <li> Deploy workflow syncs DAGs to target environment</li> <li> Staging deployment happens automatically</li> <li> Production deployment requires approval</li> <li> DAG integrity tests cover all requirements</li> <li> DEPLOYMENT.md documents the chosen strategy</li> </ul> <p>Next: Exercise 9.3: Monitoring Dashboard \u2192</p> <p>_[DAG]: Directed Acyclic Graph - Airflow's core workflow abstraction _[XCom]: Cross-Communication - Airflow's mechanism for task-to-task data passing _[Executor]: The component that runs tasks (Local, Celery, Kubernetes, etc.) _[Scheduler]: The component that triggers DAGs and determines task execution order _[Webserver]: The component that serves the Airflow UI _[Provider]: An Airflow package that integrates with external systems _[TaskFlow]: The modern Python-native API for defining Airflow tasks using decorators _[Asset]: A logical representation of data that DAGs can produce or consume _[SDK]: Software Development Kit - the airflow.sdk module in Airflow 3.x _[K8s]: Kubernetes - container orchestration platform _[uv]: A fast Python package manager by Astral _[ruff]: A fast Python linter and formatter by Astral _[CI]: Continuous Integration _[CD]: Continuous Deployment *[PR]: Pull Request</p>"},{"location":"modules/09-production-patterns/exercises/exercise_9_3_monitoring/","title":"Exercise 9.3: Monitoring Dashboard","text":""},{"location":"modules/09-production-patterns/exercises/exercise_9_3_monitoring/#objective","title":"Objective","text":"<p>Design and document a comprehensive monitoring strategy for Airflow, including key metrics, alert thresholds, and dashboard layouts.</p>"},{"location":"modules/09-production-patterns/exercises/exercise_9_3_monitoring/#background","title":"Background","text":"<p>Effective Airflow monitoring covers:</p> Category Focus Area Health Scheduler heartbeat, database connections Performance DAG parsing time, task execution latency Capacity Queue depth, pool usage, concurrent tasks Reliability Task failure rate, retry counts Business DAG completion SLAs, data freshness"},{"location":"modules/09-production-patterns/exercises/exercise_9_3_monitoring/#requirements","title":"Requirements","text":""},{"location":"modules/09-production-patterns/exercises/exercise_9_3_monitoring/#part-1-identify-key-metrics","title":"Part 1: Identify Key Metrics","text":"<p>Document the following metrics and their significance:</p>"},{"location":"modules/09-production-patterns/exercises/exercise_9_3_monitoring/#scheduler-metrics","title":"Scheduler Metrics","text":"Metric Description Alert Threshold <code>scheduler_heartbeat</code> <code>dag_processing.total_parse_time</code> <code>scheduler.tasks.running</code> <code>scheduler.tasks.pending</code>"},{"location":"modules/09-production-patterns/exercises/exercise_9_3_monitoring/#executor-metrics","title":"Executor Metrics","text":"Metric Description Alert Threshold <code>executor.queued_tasks</code> <code>executor.running_tasks</code> <code>executor.open_slots</code>"},{"location":"modules/09-production-patterns/exercises/exercise_9_3_monitoring/#task-metrics","title":"Task Metrics","text":"Metric Description Alert Threshold <code>ti.finish.&lt;dag_id&gt;.&lt;task_id&gt;.&lt;state&gt;</code> <code>ti.duration.&lt;dag_id&gt;.&lt;task_id&gt;</code> <code>task_instance_created-&lt;state&gt;</code>"},{"location":"modules/09-production-patterns/exercises/exercise_9_3_monitoring/#pool-metrics","title":"Pool Metrics","text":"Metric Description Alert Threshold <code>pool.open_slots.&lt;pool_name&gt;</code> <code>pool.used_slots.&lt;pool_name&gt;</code> <code>pool.queued_slots.&lt;pool_name&gt;</code>"},{"location":"modules/09-production-patterns/exercises/exercise_9_3_monitoring/#part-2-design-dashboard-layout","title":"Part 2: Design Dashboard Layout","text":"<p>Create a Grafana dashboard design with these panels:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                        AIRFLOW MONITORING DASHBOARD                      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502  Scheduler   \u2502  \u2502    Tasks     \u2502  \u2502   Queued     \u2502  \u2502   Failed     \u2502 \u2502\n\u2502  \u2502  Heartbeat   \u2502  \u2502   Running    \u2502  \u2502    Tasks     \u2502  \u2502    Tasks     \u2502 \u2502\n\u2502  \u2502     \u2705       \u2502  \u2502     12       \u2502  \u2502      3       \u2502  \u2502      0       \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502\n\u2502  \u2502     Task Execution Trend        \u2502  \u2502      Pool Usage                 \u2502\u2502\n\u2502  \u2502   (Line chart over time)        \u2502  \u2502   (Stacked bar chart)          \u2502\u2502\n\u2502  \u2502                                 \u2502  \u2502                                 \u2502\u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502                    DAG Run Duration by DAG                          \u2502 \u2502\n\u2502  \u2502                    (Heatmap or bar chart)                           \u2502 \u2502\n\u2502  \u2502                                                                     \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502\n\u2502  \u2502     DAG Parsing Time            \u2502  \u2502     Task Failure Rate          \u2502\u2502\n\u2502  \u2502   (Gauge: target &lt; 30s)         \u2502  \u2502   (Line chart with threshold)  \u2502\u2502\n\u2502  \u2502                                 \u2502  \u2502                                 \u2502\u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"modules/09-production-patterns/exercises/exercise_9_3_monitoring/#part-3-create-alert-rules","title":"Part 3: Create Alert Rules","text":"<p>Define alerting rules in Prometheus AlertManager format:</p> <pre><code># alerts.yaml\ngroups:\n  - name: airflow-critical\n    rules:\n      - alert: AirflowSchedulerDown\n        expr: # TODO\n        for: 2m\n        labels:\n          severity: critical\n        annotations:\n          summary: \"Airflow scheduler is down\"\n          description: \"No heartbeat for 2 minutes\"\n\n      - alert: AirflowHighTaskFailureRate\n        expr: # TODO\n        for: 5m\n        labels:\n          severity: warning\n        annotations:\n          summary: \"High task failure rate\"\n          description: \"More than 10% of tasks failing\"\n\n      # TODO: Add more alert rules\n</code></pre>"},{"location":"modules/09-production-patterns/exercises/exercise_9_3_monitoring/#part-4-implement-health-check-dag","title":"Part 4: Implement Health Check DAG","text":"<p>Create a DAG that monitors Airflow health:</p> <pre><code>\"\"\"Health check DAG that verifies system connectivity.\"\"\"\n\nfrom airflow.sdk import dag, task\nfrom datetime import datetime\n\n@dag(\n    dag_id=\"system_health_check\",\n    schedule=\"*/5 * * * *\",  # Every 5 minutes\n    start_date=datetime(2024, 1, 1),\n    catchup=False,\n    tags=[\"monitoring\"],\n)\ndef health_check():\n    @task\n    def check_database():\n        \"\"\"Verify database connectivity.\"\"\"\n        # TODO: Implement database health check\n        pass\n\n    @task\n    def check_connections():\n        \"\"\"Verify critical connections are working.\"\"\"\n        # TODO: Check important Airflow connections\n        pass\n\n    @task\n    def check_variables():\n        \"\"\"Verify critical variables exist.\"\"\"\n        # TODO: Check important Airflow variables\n        pass\n\n    @task\n    def report_health(db_ok, conn_ok, vars_ok):\n        \"\"\"Report overall health status.\"\"\"\n        # TODO: Aggregate and report\n        pass\n\nhealth_check()\n</code></pre>"},{"location":"modules/09-production-patterns/exercises/exercise_9_3_monitoring/#deliverables","title":"Deliverables","text":"<ol> <li><code>metrics_specification.md</code> - Complete metric documentation</li> <li><code>dashboard_design.json</code> - Grafana dashboard JSON (optional)</li> <li><code>alerts.yaml</code> - Prometheus alert rules</li> <li><code>health_check_dag.py</code> - Health monitoring DAG</li> </ol>"},{"location":"modules/09-production-patterns/exercises/exercise_9_3_monitoring/#hints","title":"Hints","text":"Hint 1: StatsD metric names  Airflow uses StatsD for metrics. Common metric patterns: <pre><code># Task metrics\nti.finish.&lt;dag_id&gt;.&lt;task_id&gt;.success\nti.finish.&lt;dag_id&gt;.&lt;task_id&gt;.failed\n\n# Scheduler metrics\nscheduler.heartbeat\nscheduler.critical_section_duration\n\n# Executor metrics\nexecutor.queued_tasks\nexecutor.running_tasks\n</code></pre> Hint 2: Grafana panel query <pre><code># Tasks completed in last hour by state\nsum(rate(airflow_ti_finish_total[1h])) by (state)\n\n# Average DAG parse time\navg(airflow_dag_processing_total_parse_time)\n</code></pre> Hint 3: PagerDuty integration <pre><code>alertmanager.yml:\nreceivers:\n  - name: pagerduty\n    pagerduty_configs:\n      - service_key: &lt;your-key&gt;\n        severity: '{{ .CommonLabels.severity }}'\n</code></pre>"},{"location":"modules/09-production-patterns/exercises/exercise_9_3_monitoring/#success-criteria","title":"Success Criteria","text":"<ul> <li> All key metrics documented with descriptions</li> <li> Alert thresholds defined with rationale</li> <li> Dashboard design covers all monitoring categories</li> <li> Prometheus alert rules are syntactically correct</li> <li> Health check DAG verifies critical components</li> <li> Escalation paths defined for critical alerts</li> </ul>"},{"location":"modules/09-production-patterns/exercises/exercise_9_3_monitoring/#bonus-challenges","title":"Bonus Challenges","text":"<ol> <li>Create actual Grafana dashboard using the design</li> <li>Implement custom metrics for business KPIs</li> <li>Set up log-based alerts using Loki/CloudWatch</li> <li>Create runbook for each alert type</li> </ol>"},{"location":"modules/09-production-patterns/exercises/exercise_9_3_monitoring/#references","title":"References","text":"<ul> <li>Airflow Metrics</li> <li>Prometheus AlertManager</li> <li>Grafana Dashboard Design</li> </ul> <p>_[DAG]: Directed Acyclic Graph - Airflow's core workflow abstraction _[XCom]: Cross-Communication - Airflow's mechanism for task-to-task data passing _[Executor]: The component that runs tasks (Local, Celery, Kubernetes, etc.) _[Scheduler]: The component that triggers DAGs and determines task execution order _[Webserver]: The component that serves the Airflow UI _[Provider]: An Airflow package that integrates with external systems _[TaskFlow]: The modern Python-native API for defining Airflow tasks using decorators _[Asset]: A logical representation of data that DAGs can produce or consume _[SDK]: Software Development Kit - the airflow.sdk module in Airflow 3.x _[K8s]: Kubernetes - container orchestration platform _[uv]: A fast Python package manager by Astral _[ruff]: A fast Python linter and formatter by Astral _[CI]: Continuous Integration _[CD]: Continuous Deployment *[PR]: Pull Request</p>"},{"location":"modules/09-production-patterns/exercises/exercise_9_4_llm_retry_patterns/","title":"Exercise 9.4: LLM Retry Patterns","text":""},{"location":"modules/09-production-patterns/exercises/exercise_9_4_llm_retry_patterns/#objective","title":"Objective","text":"<p>Build production-grade retry and rate limiting patterns for LLM API calls, demonstrating exponential backoff, cost tracking, fallback routing, and circuit breaker patterns.</p>"},{"location":"modules/09-production-patterns/exercises/exercise_9_4_llm_retry_patterns/#background","title":"Background","text":"<p>LLM APIs present unique operational challenges that require specialized patterns:</p> Challenge Impact Solution Pattern Rate Limits 429 errors, blocked requests Token bucket rate limiting API Costs Unexpected bills, budget overruns Cost tracking with alerts Transient Failures Timeouts, 5xx errors Exponential backoff with jitter Provider Outages Complete workflow failures Multi-provider fallback chains Token Limits Context window exceeded Request chunking and validation"},{"location":"modules/09-production-patterns/exercises/exercise_9_4_llm_retry_patterns/#requirements","title":"Requirements","text":""},{"location":"modules/09-production-patterns/exercises/exercise_9_4_llm_retry_patterns/#task-1-intelligent-rate-limiting","title":"Task 1: Intelligent Rate Limiting","text":"<p>Implement a token bucket rate limiter that:</p> <ul> <li>Tracks tokens per minute (TPM) and requests per minute (RPM)</li> <li>Waits automatically when approaching limits</li> <li>Provides backpressure to upstream tasks</li> <li>Logs rate limit status for monitoring</li> </ul>"},{"location":"modules/09-production-patterns/exercises/exercise_9_4_llm_retry_patterns/#task-2-exponential-backoff-with-jitter","title":"Task 2: Exponential Backoff with Jitter","text":"<p>Configure retry behavior that:</p> <ul> <li>Uses exponential backoff (1s, 2s, 4s, 8s...)</li> <li>Adds random jitter to prevent thundering herd</li> <li>Distinguishes retryable vs non-retryable errors</li> <li>Caps maximum retry delay</li> </ul>"},{"location":"modules/09-production-patterns/exercises/exercise_9_4_llm_retry_patterns/#task-3-cost-tracking-callbacks","title":"Task 3: Cost Tracking Callbacks","text":"<p>Implement cost monitoring that:</p> <ul> <li>Tracks token usage per request</li> <li>Calculates estimated costs using model pricing</li> <li>Alerts when approaching budget thresholds</li> <li>Provides per-task and per-DAG cost aggregation</li> </ul>"},{"location":"modules/09-production-patterns/exercises/exercise_9_4_llm_retry_patterns/#task-4-multi-provider-fallback","title":"Task 4: Multi-Provider Fallback","text":"<p>Create a fallback chain that:</p> <ul> <li>Tries primary provider first</li> <li>Falls back to secondary on failure</li> <li>Logs which provider succeeded</li> <li>Maintains consistent response format</li> </ul>"},{"location":"modules/09-production-patterns/exercises/exercise_9_4_llm_retry_patterns/#task-5-circuit-breaker-pattern","title":"Task 5: Circuit Breaker Pattern","text":"<p>Implement circuit breaker logic that:</p> <ul> <li>Opens circuit after N consecutive failures</li> <li>Prevents further API calls while open</li> <li>Resets after cooldown period</li> <li>Provides health status for monitoring</li> </ul>"},{"location":"modules/09-production-patterns/exercises/exercise_9_4_llm_retry_patterns/#starter-code","title":"Starter Code","text":"<p>See <code>exercise_9_4_llm_retry_patterns_starter.py</code></p>"},{"location":"modules/09-production-patterns/exercises/exercise_9_4_llm_retry_patterns/#hints","title":"Hints","text":"Hint 1: Token Bucket Implementation <pre><code>import time\nfrom dataclasses import dataclass\n\n\n@dataclass\nclass TokenBucket:\n    capacity: int\n    tokens: float\n    refill_rate: float  # tokens per second\n    last_refill: float\n\n    def consume(self, tokens: int) -&gt; bool:\n        self._refill()\n        if self.tokens &gt;= tokens:\n            self.tokens -= tokens\n            return True\n        return False\n\n    def _refill(self):\n        now = time.time()\n        elapsed = now - self.last_refill\n        self.tokens = min(self.capacity, self.tokens + elapsed * self.refill_rate)\n        self.last_refill = now\n\n    def wait_time(self, tokens: int) -&gt; float:\n        self._refill()\n        if self.tokens &gt;= tokens:\n            return 0\n        return (tokens - self.tokens) / self.refill_rate\n</code></pre> Hint 2: Exponential Backoff with Jitter <pre><code>import random\nfrom datetime import timedelta\n\n\ndef calculate_backoff(attempt: int, base_delay: float = 1.0, max_delay: float = 60.0) -&gt; float:\n    \"\"\"Calculate delay with exponential backoff and jitter.\"\"\"\n    delay = min(base_delay * (2**attempt), max_delay)\n    jitter = random.uniform(0, delay * 0.1)  # 10% jitter\n    return delay + jitter\n\n\n# In task decorator:\n@task(\n    retries=5,\n    retry_delay=timedelta(seconds=1),\n    retry_exponential_backoff=True,\n    max_retry_delay=timedelta(minutes=2),\n)\ndef llm_task(): ...\n</code></pre> Hint 3: Cost Tracking Callback <pre><code>def track_llm_cost(context, response_data: dict):\n    \"\"\"Track LLM costs and alert on threshold.\"\"\"\n    from airflow.models import Variable\n\n    # Model pricing (per 1K tokens)\n    PRICING = {\n        \"gpt-4\": {\"input\": 0.03, \"output\": 0.06},\n        \"gpt-3.5-turbo\": {\"input\": 0.0015, \"output\": 0.002},\n    }\n\n    model = response_data.get(\"model\", \"gpt-3.5-turbo\")\n    input_tokens = response_data.get(\"usage\", {}).get(\"prompt_tokens\", 0)\n    output_tokens = response_data.get(\"usage\", {}).get(\"completion_tokens\", 0)\n\n    pricing = PRICING.get(model, PRICING[\"gpt-3.5-turbo\"])\n    cost = (input_tokens * pricing[\"input\"] + output_tokens * pricing[\"output\"]) / 1000\n\n    # Accumulate and check threshold\n    total_cost = float(Variable.get(\"llm_total_cost\", default_var=\"0\"))\n    total_cost += cost\n    Variable.set(\"llm_total_cost\", str(total_cost))\n\n    budget = float(Variable.get(\"llm_budget\", default_var=\"100\"))\n    if total_cost &gt; budget * 0.8:\n        logger.warning(f\"LLM costs at {total_cost / budget:.0%} of budget!\")\n</code></pre> Hint 4: Fallback Chain Pattern <pre><code>def call_with_fallback(prompt: str, providers: list[dict]) -&gt; dict:\n    \"\"\"Try providers in order until one succeeds.\"\"\"\n    last_error = None\n\n    for provider in providers:\n        try:\n            logger.info(f\"Trying provider: {provider['name']}\")\n            response = provider[\"call_fn\"](prompt)\n            logger.info(f\"Success with provider: {provider['name']}\")\n            return {\n                \"provider\": provider[\"name\"],\n                \"response\": response,\n                \"fallback_used\": provider != providers[0],\n            }\n        except Exception as e:\n            logger.warning(f\"Provider {provider['name']} failed: {e}\")\n            last_error = e\n            continue\n\n    raise Exception(f\"All providers failed. Last error: {last_error}\")\n</code></pre> Hint 5: Circuit Breaker <pre><code>from dataclasses import dataclass\nfrom datetime import datetime, timedelta\n\n\n@dataclass\nclass CircuitBreaker:\n    failure_threshold: int = 5\n    reset_timeout: timedelta = timedelta(minutes=5)\n    failure_count: int = 0\n    last_failure_time: datetime | None = None\n    state: str = \"closed\"  # closed, open, half-open\n\n    def can_execute(self) -&gt; bool:\n        if self.state == \"closed\":\n            return True\n        if self.state == \"open\":\n            if datetime.now() - self.last_failure_time &gt; self.reset_timeout:\n                self.state = \"half-open\"\n                return True\n            return False\n        return True  # half-open allows one attempt\n\n    def record_success(self):\n        self.failure_count = 0\n        self.state = \"closed\"\n\n    def record_failure(self):\n        self.failure_count += 1\n        self.last_failure_time = datetime.now()\n        if self.failure_count &gt;= self.failure_threshold:\n            self.state = \"open\"\n</code></pre>"},{"location":"modules/09-production-patterns/exercises/exercise_9_4_llm_retry_patterns/#success-criteria","title":"Success Criteria","text":"<ul> <li> Rate limiter prevents exceeding API limits</li> <li> Retries use exponential backoff with jitter</li> <li> Cost tracking logs usage and alerts on threshold</li> <li> Fallback chain tries multiple providers</li> <li> Circuit breaker prevents cascading failures</li> <li> All patterns are production-ready with logging</li> <li> Error handling distinguishes retryable vs non-retryable errors</li> </ul>"},{"location":"modules/09-production-patterns/exercises/exercise_9_4_llm_retry_patterns/#files","title":"Files","text":"<ul> <li>Starter: <code>exercise_9_4_llm_retry_patterns_starter.py</code></li> <li>Solution: <code>../solutions/solution_9_4_llm_retry_patterns.py</code></li> </ul>"},{"location":"modules/09-production-patterns/exercises/exercise_9_4_llm_retry_patterns/#estimated-time","title":"Estimated Time","text":"<p>60-90 minutes</p> <p>Next: Exercise 9.5 \u2192 | Module 11: Sensors \u2192</p> <p>_[DAG]: Directed Acyclic Graph - Airflow's core workflow abstraction _[XCom]: Cross-Communication - Airflow's mechanism for task-to-task data passing _[Executor]: The component that runs tasks (Local, Celery, Kubernetes, etc.) _[Scheduler]: The component that triggers DAGs and determines task execution order _[Webserver]: The component that serves the Airflow UI _[Provider]: An Airflow package that integrates with external systems _[TaskFlow]: The modern Python-native API for defining Airflow tasks using decorators _[Asset]: A logical representation of data that DAGs can produce or consume _[SDK]: Software Development Kit - the airflow.sdk module in Airflow 3.x _[K8s]: Kubernetes - container orchestration platform _[uv]: A fast Python package manager by Astral _[ruff]: A fast Python linter and formatter by Astral _[CI]: Continuous Integration _[CD]: Continuous Deployment *[PR]: Pull Request</p>"},{"location":"modules/10-advanced-topics/","title":"Module 10: Advanced Topics","text":""},{"location":"modules/10-advanced-topics/#learning-objectives","title":"\ud83c\udfaf Learning Objectives","text":"<p>By the end of this module, you will: - Understand the Task SDK and Task Execution Interface - Use the Edge Executor for hybrid deployments - Configure multiple executors - Build plugins and custom extensions - Work with the new Airflow 3 API</p>"},{"location":"modules/10-advanced-topics/#estimated-time-4-6-hours","title":"\u23f1\ufe0f Estimated Time: 4-6 hours","text":""},{"location":"modules/10-advanced-topics/#1-task-sdk-deep-dive","title":"1. Task SDK Deep Dive","text":"<p>Airflow 3's Task SDK is the new runtime for task execution.</p>"},{"location":"modules/10-advanced-topics/#why-task-sdk","title":"Why Task SDK?","text":"Old Model (Airflow 2) New Model (Airflow 3) Workers access database Workers use REST API Full Airflow install on workers Lightweight Task SDK Shared credentials Scoped tokens Python only Multi-language potential"},{"location":"modules/10-advanced-topics/#task-sdk-usage","title":"Task SDK Usage","text":"<pre><code># The SDK is used automatically with @task\nfrom airflow.sdk import task\n\n@task\ndef my_task():\n    # Inside here, you're running in the Task SDK runtime\n    # No direct database access - use provided APIs\n    pass\n</code></pre>"},{"location":"modules/10-advanced-topics/#accessing-metadata-via-sdk","title":"Accessing Metadata via SDK","text":"<pre><code>from airflow.sdk import task\nfrom airflow.sdk.execution_time.context import get_current_context\n\n@task\ndef access_metadata():\n    context = get_current_context()\n\n    # Get task instance info\n    ti = context[\"ti\"]\n\n    # Pull XCom (goes through Task Execution API)\n    value = ti.xcom_pull(task_ids=\"upstream\", key=\"result\")\n\n    # Push XCom\n    ti.xcom_push(key=\"my_result\", value={\"status\": \"done\"})\n</code></pre>"},{"location":"modules/10-advanced-topics/#task-sdk-versioning","title":"Task SDK Versioning","text":"<p>The Task SDK is versioned separately from Airflow: - Airflow 3.0.x includes Task SDK 1.0.x - Allows independent SDK updates</p> <pre><code># Check installed version\npip show apache-airflow-task-sdk\n</code></pre>"},{"location":"modules/10-advanced-topics/#2-task-execution-interface-aip-72","title":"2. Task Execution Interface (AIP-72)","text":"<p>The API that workers use to communicate with the scheduler.</p>"},{"location":"modules/10-advanced-topics/#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Worker    \u2502 \u25c4\u2500\u2500\u2500\u2500\u2500\u25ba \u2502   API Server    \u2502\n\u2502  (Task SDK) \u2502  REST   \u2502 (Task Execution \u2502\n\u2502             \u2502   API   \u2502   Interface)    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                 \u2502\n                                 \u25bc\n                        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                        \u2502    Database     \u2502\n                        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"modules/10-advanced-topics/#key-endpoints","title":"Key Endpoints","text":"Endpoint Purpose <code>GET /task/{id}/state</code> Get task state <code>PATCH /task/{id}/state</code> Update task state <code>POST /task/{id}/xcom</code> Push XCom <code>GET /task/{id}/xcom/{key}</code> Pull XCom <code>GET /task/{id}/connection/{conn_id}</code> Get connection"},{"location":"modules/10-advanced-topics/#benefits","title":"Benefits","text":"<ol> <li>Security: Workers don't need DB credentials</li> <li>Scalability: API can be scaled independently</li> <li>Isolation: Tasks can run in untrusted environments</li> <li>Multi-language: Any language can implement the SDK</li> </ol>"},{"location":"modules/10-advanced-topics/#3-edge-executor-aip-69","title":"3. Edge Executor (AIP-69)","text":"<p>Run tasks on edge devices, remote data centers, or hybrid clouds.</p>"},{"location":"modules/10-advanced-topics/#use-cases","title":"Use Cases","text":"<ul> <li>Process data locally before sending to cloud</li> <li>Run tasks in air-gapped environments</li> <li>Utilize on-premise GPU resources</li> <li>Comply with data locality requirements</li> </ul>"},{"location":"modules/10-advanced-topics/#setup","title":"Setup","text":"<pre><code># Install edge provider\npip install apache-airflow-providers-edge3\n</code></pre>"},{"location":"modules/10-advanced-topics/#configuration","title":"Configuration","text":"<pre><code># In airflow.cfg or environment\n[edge]\napi_url = https://airflow.example.com/api/v2\njob_poll_interval = 5\nheartbeat_interval = 30\n</code></pre>"},{"location":"modules/10-advanced-topics/#edge-worker-registration","title":"Edge Worker Registration","text":"<pre><code># On the edge device\nairflow edge worker \\\n  --edge-hostname \"edge-worker-1\" \\\n  --api-url \"https://airflow.example.com/api/v2\" \\\n  --api-token \"$EDGE_TOKEN\"\n</code></pre>"},{"location":"modules/10-advanced-topics/#targeting-edge-workers","title":"Targeting Edge Workers","text":"<pre><code>from airflow.sdk import DAG, task\n\nwith DAG(...):\n    @task(queue=\"edge-datacenter-1\")  # Route to specific edge\n    def process_on_edge():\n        \"\"\"Runs on edge worker\"\"\"\n        pass\n</code></pre>"},{"location":"modules/10-advanced-topics/#4-multiple-executor-configuration","title":"4. Multiple Executor Configuration","text":"<p>Airflow 3 deprecates hybrid executors in favor of multiple executor config.</p>"},{"location":"modules/10-advanced-topics/#configuration_1","title":"Configuration","text":"<pre><code># airflow.cfg\n[core]\nexecutor = airflow.executors.executor_loader.ExecutorLoader\n\n[executors]\ndefault = KubernetesExecutor\ncelery = CeleryExecutor\nlocal = LocalExecutor\n</code></pre>"},{"location":"modules/10-advanced-topics/#task-level-executor-selection","title":"Task-Level Executor Selection","text":"<pre><code>@task(executor=\"celery\")\ndef high_throughput_task():\n    \"\"\"Uses CeleryExecutor for speed\"\"\"\n    pass\n\n@task(executor=\"default\")  # KubernetesExecutor\ndef isolated_task():\n    \"\"\"Uses K8s for isolation\"\"\"\n    pass\n</code></pre>"},{"location":"modules/10-advanced-topics/#5-building-plugins","title":"5. Building Plugins","text":"<p>Extend Airflow functionality with plugins.</p>"},{"location":"modules/10-advanced-topics/#plugin-structure","title":"Plugin Structure","text":"<pre><code># plugins/my_plugin.py\nfrom airflow.plugins_manager import AirflowPlugin\nfrom flask import Blueprint\n\n# Custom view\nmy_views_blueprint = Blueprint(\n    \"my_views\",\n    __name__,\n    url_prefix=\"/myplugin\",\n)\n\n@my_views_blueprint.route(\"/\")\ndef my_view():\n    return \"Hello from my plugin!\"\n\n# Custom operator (if not using providers)\nclass MyCustomOperator(BaseOperator):\n    def execute(self, context):\n        pass\n\n# Register plugin\nclass MyPlugin(AirflowPlugin):\n    name = \"my_plugin\"\n    flask_blueprints = [my_views_blueprint]\n    operators = [MyCustomOperator]\n</code></pre>"},{"location":"modules/10-advanced-topics/#plugin-location","title":"Plugin Location","text":"<pre><code>$AIRFLOW_HOME/\n\u2514\u2500\u2500 plugins/\n    \u251c\u2500\u2500 __init__.py\n    \u2514\u2500\u2500 my_plugin.py\n</code></pre>"},{"location":"modules/10-advanced-topics/#custom-macros","title":"Custom Macros","text":"<pre><code># plugins/macros.py\nfrom airflow.plugins_manager import AirflowPlugin\n\ndef custom_macro(date_str):\n    \"\"\"Custom Jinja macro available in templates\"\"\"\n    return date_str.replace(\"-\", \"\")\n\nclass MacroPlugin(AirflowPlugin):\n    name = \"macro_plugin\"\n    macros = [custom_macro]\n</code></pre> <p>Usage in DAG: <pre><code>bash_command=\"echo {{ custom_macro(ds) }}\"  # 20240101\n</code></pre></p>"},{"location":"modules/10-advanced-topics/#6-rest-api-v2","title":"6. REST API v2","text":"<p>Airflow 3 uses FastAPI-based REST API v2.</p>"},{"location":"modules/10-advanced-topics/#authentication","title":"Authentication","text":"<pre><code>import requests\n\n# Get auth token (depends on auth backend)\nheaders = {\"Authorization\": \"Bearer &lt;token&gt;\"}\n\n# Or basic auth\nfrom requests.auth import HTTPBasicAuth\nauth = HTTPBasicAuth(\"username\", \"password\")\n</code></pre>"},{"location":"modules/10-advanced-topics/#common-operations","title":"Common Operations","text":"<pre><code>import requests\n\nBASE = \"https://airflow.example.com/api/v2\"\n\n# List DAGs\nr = requests.get(f\"{BASE}/dags\", auth=auth)\ndags = r.json()[\"dags\"]\n\n# Trigger DAG run\nr = requests.post(\n    f\"{BASE}/dags/my_dag/dagRuns\",\n    json={\"conf\": {\"key\": \"value\"}},\n    auth=auth\n)\n\n# Get task instances\nr = requests.get(\n    f\"{BASE}/dags/my_dag/dagRuns/run_id/taskInstances\",\n    auth=auth\n)\n\n# Get XCom\nr = requests.get(\n    f\"{BASE}/dags/my_dag/dagRuns/run_id/taskInstances/task_id/xcomEntries/key\",\n    auth=auth\n)\n</code></pre>"},{"location":"modules/10-advanced-topics/#using-the-python-client","title":"Using the Python Client","text":"<pre><code>from airflow.api_connexion.client import Client\n\nclient = Client(\n    host=\"https://airflow.example.com\",\n    username=\"admin\",\n    password=\"admin\"\n)\n\n# List DAGs\ndags = client.dag_api.get_dags()\n\n# Trigger run\nclient.dag_run_api.post_dag_run(\n    dag_id=\"my_dag\",\n    body={\"conf\": {}}\n)\n</code></pre>"},{"location":"modules/10-advanced-topics/#7-dag-bundles-versioning","title":"7. DAG Bundles &amp; Versioning","text":""},{"location":"modules/10-advanced-topics/#dag-bundles","title":"DAG Bundles","text":"<p>Bundle configuration for DAG deployment:</p> <pre><code># In airflow.cfg\n[dag_bundles]\nmy_bundle = {\n    \"type\": \"git\",\n    \"repo\": \"git@github.com:org/dags.git\",\n    \"branch\": \"main\",\n    \"subdir\": \"dags/\"\n}\n</code></pre>"},{"location":"modules/10-advanced-topics/#version-aware-execution","title":"Version-Aware Execution","text":"<pre><code>from airflow.sdk import DAG, task\n\nwith DAG(\n    dag_id=\"versioned_dag\",\n    version=\"1.0.0\",  # Explicit versioning\n    ...\n):\n    @task\n    def my_task():\n        pass\n</code></pre> <p>Key behavior: - DAG runs use the version at trigger time - Mid-execution changes don't affect running DAGs - UI shows version for each run</p>"},{"location":"modules/10-advanced-topics/#8-custom-timetables","title":"8. Custom Timetables","text":"<p>For schedules cron can't express:</p> <pre><code>from airflow.timetables.base import DagRunInfo, DataInterval, Timetable\nfrom pendulum import DateTime\n\nclass TradingDaysTimetable(Timetable):\n    \"\"\"Run only on trading days (excludes weekends and holidays)\"\"\"\n\n    HOLIDAYS = {\n        DateTime(2024, 12, 25),\n        DateTime(2024, 1, 1),\n        # ... more holidays\n    }\n\n    def is_trading_day(self, dt: DateTime) -&gt; bool:\n        return dt.weekday() &lt; 5 and dt not in self.HOLIDAYS\n\n    def next_dagrun_info(\n        self,\n        *,\n        last_automated_data_interval,\n        restriction,\n    ):\n        if last_automated_data_interval is None:\n            next_start = restriction.earliest\n        else:\n            next_start = last_automated_data_interval.end\n\n        # Find next trading day\n        while not self.is_trading_day(next_start):\n            next_start = next_start.add(days=1)\n\n        next_end = next_start.add(days=1)\n\n        return DagRunInfo(\n            run_after=next_end,\n            data_interval=DataInterval(next_start, next_end)\n        )\n</code></pre>"},{"location":"modules/10-advanced-topics/#9-deferrable-operators-advanced-sensors","title":"9. Deferrable Operators (Advanced Sensors)","text":"<p>Operators that release worker slots while waiting:</p> <pre><code>from airflow.sensors.base import BaseSensorOperator\nfrom airflow.triggers.temporal import TimeDeltaTrigger\n\nclass DeferrableSensor(BaseSensorOperator):\n    def execute(self, context):\n        if not self.poke(context):\n            # Defer instead of sleeping\n            self.defer(\n                trigger=TimeDeltaTrigger(timedelta(minutes=5)),\n                method_name=\"execute_complete\"\n            )\n\n    def execute_complete(self, context, event=None):\n        # Called when trigger fires\n        if self.poke(context):\n            return True\n        # Defer again\n        self.defer(...)\n</code></pre>"},{"location":"modules/10-advanced-topics/#exercises","title":"\ud83d\udcdd Exercises","text":""},{"location":"modules/10-advanced-topics/#exercise-101-api-automation","title":"Exercise 10.1: API Automation","text":"<p>Create a Python script that: 1. Lists all DAGs via API 2. Finds DAGs that haven't run in 7 days 3. Triggers each one with a maintenance flag</p>"},{"location":"modules/10-advanced-topics/#exercise-102-custom-timetable","title":"Exercise 10.2: Custom Timetable","text":"<p>Create a timetable for: - Every weekday at 9 AM - But skip the first Monday of each month - And run an extra time on the last Friday</p>"},{"location":"modules/10-advanced-topics/#exercise-103-plugin-development","title":"Exercise 10.3: Plugin Development","text":"<p>Build a plugin that: 1. Adds a custom view showing task duration statistics 2. Provides a custom macro for date formatting 3. Includes a utility operator</p>"},{"location":"modules/10-advanced-topics/#final-checkpoint","title":"\u2705 Final Checkpoint","text":"<p>Congratulations on completing the curriculum! Verify you can:</p> <ul> <li> Explain the Task SDK architecture</li> <li> Describe when to use Edge Executor</li> <li> Configure multiple executors</li> <li> Build Airflow plugins</li> <li> Use the REST API v2</li> <li> Understand DAG versioning</li> </ul>"},{"location":"modules/10-advanced-topics/#whats-next","title":"\ud83c\udf93 What's Next?","text":"<ol> <li>Build a portfolio project: Create a production-grade pipeline</li> <li>Contribute to Airflow: Submit bug fixes or documentation</li> <li>Explore providers: Deep-dive into cloud-specific operators</li> <li>Join the community: Slack, mailing lists, Airflow Summit</li> </ol>"},{"location":"modules/10-advanced-topics/#additional-resources","title":"\ud83d\udcda Additional Resources","text":"<ul> <li>Airflow 3 Release Blog</li> <li>AIP Proposals</li> <li>Provider Packages</li> <li>Airflow Summit Talks</li> </ul> <p>You've completed the Airflow Mastery curriculum! \ud83c\udf89</p> <p>_[DAG]: Directed Acyclic Graph - Airflow's core workflow abstraction _[XCom]: Cross-Communication - Airflow's mechanism for task-to-task data passing _[Executor]: The component that runs tasks (Local, Celery, Kubernetes, etc.) _[Scheduler]: The component that triggers DAGs and determines task execution order _[Webserver]: The component that serves the Airflow UI _[Provider]: An Airflow package that integrates with external systems _[TaskFlow]: The modern Python-native API for defining Airflow tasks using decorators _[Asset]: A logical representation of data that DAGs can produce or consume _[SDK]: Software Development Kit - the airflow.sdk module in Airflow 3.x _[K8s]: Kubernetes - container orchestration platform _[uv]: A fast Python package manager by Astral _[ruff]: A fast Python linter and formatter by Astral _[CI]: Continuous Integration _[CD]: Continuous Deployment *[PR]: Pull Request</p>"},{"location":"modules/10-advanced-topics/exercises/exercise_10_1_api_automation/","title":"Exercise 10.1: REST API Automation","text":""},{"location":"modules/10-advanced-topics/exercises/exercise_10_1_api_automation/#objective","title":"Objective","text":"<p>Build a Python automation script that uses Airflow's REST API v2 to manage DAGs, including listing DAGs, monitoring execution, and triggering maintenance runs.</p>"},{"location":"modules/10-advanced-topics/exercises/exercise_10_1_api_automation/#background","title":"Background","text":"<p>Airflow 3's REST API v2 provides programmatic access to: - DAG management (list, trigger, pause) - DAG Run monitoring - Task Instance inspection - XCom access - Connection and Variable management</p>"},{"location":"modules/10-advanced-topics/exercises/exercise_10_1_api_automation/#api-base-url","title":"API Base URL","text":"<pre><code>https://your-airflow-host/api/v2/\n</code></pre>"},{"location":"modules/10-advanced-topics/exercises/exercise_10_1_api_automation/#authentication-methods","title":"Authentication Methods","text":"<ol> <li>Basic Auth: Username/password</li> <li>Session Auth: Login cookie</li> <li>Token Auth: API tokens (if configured)</li> </ol>"},{"location":"modules/10-advanced-topics/exercises/exercise_10_1_api_automation/#requirements","title":"Requirements","text":"<p>Create a Python script that:</p>"},{"location":"modules/10-advanced-topics/exercises/exercise_10_1_api_automation/#part-1-list-and-analyze-dags","title":"Part 1: List and Analyze DAGs","text":"<ol> <li>Connect to Airflow API</li> <li>List all DAGs with their properties</li> <li>Filter to find DAGs that:</li> <li>Haven't run in the last 7 days</li> <li>Are not paused</li> <li>Have a schedule defined</li> </ol>"},{"location":"modules/10-advanced-topics/exercises/exercise_10_1_api_automation/#part-2-dag-run-analysis","title":"Part 2: DAG Run Analysis","text":"<ol> <li>For each inactive DAG, get the last run details</li> <li>Calculate time since last successful run</li> <li>Check for any failed recent runs</li> </ol>"},{"location":"modules/10-advanced-topics/exercises/exercise_10_1_api_automation/#part-3-automated-trigger","title":"Part 3: Automated Trigger","text":"<ol> <li>Trigger each stale DAG with a configuration flag</li> <li>Monitor the run status until completion</li> <li>Report results</li> </ol>"},{"location":"modules/10-advanced-topics/exercises/exercise_10_1_api_automation/#part-4-health-report","title":"Part 4: Health Report","text":"<p>Generate a report showing: - Total DAGs - Active vs paused DAGs - DAGs with recent failures - Stale DAGs that were triggered</p>"},{"location":"modules/10-advanced-topics/exercises/exercise_10_1_api_automation/#starter-code","title":"Starter Code","text":"<p>See <code>exercise_10_1_api_automation_starter.py</code></p>"},{"location":"modules/10-advanced-topics/exercises/exercise_10_1_api_automation/#example-output","title":"Example Output","text":"<pre><code>=== Airflow DAG Maintenance Report ===\nGenerated: 2024-01-15 10:00:00\n\nDAG Summary:\n- Total DAGs: 45\n- Active: 38\n- Paused: 7\n\nStale DAGs (no run in 7 days):\n1. data_archive_monthly - Last run: 2024-01-01\n2. legacy_report_sync - Last run: 2024-01-05\n3. cleanup_temp_files - Last run: 2024-01-08\n\nTriggered Runs:\n\u2705 data_archive_monthly - run_id: manual__2024-01-15T10:00:00\n\u2705 legacy_report_sync - run_id: manual__2024-01-15T10:00:01\n\u274c cleanup_temp_files - FAILED: DAG has import errors\n\nRecent Failures (last 24h):\n- daily_metrics: 2 failures\n- api_sync: 1 failure\n</code></pre>"},{"location":"modules/10-advanced-topics/exercises/exercise_10_1_api_automation/#hints","title":"Hints","text":"Hint 1: API Client Setup <pre><code>import requests\nfrom datetime import datetime, timedelta\n\nclass AirflowClient:\n    def __init__(self, base_url, username, password):\n        self.base_url = base_url.rstrip('/')\n        self.session = requests.Session()\n        self.session.auth = (username, password)\n        self.session.headers.update({\n            'Content-Type': 'application/json',\n            'Accept': 'application/json'\n        })\n\n    def get(self, endpoint):\n        response = self.session.get(f\"{self.base_url}{endpoint}\")\n        response.raise_for_status()\n        return response.json()\n</code></pre> Hint 2: Pagination handling <pre><code>def get_all_dags(self):\n    \"\"\"Handle API pagination.\"\"\"\n    dags = []\n    offset = 0\n    limit = 100\n\n    while True:\n        response = self.get(f\"/dags?offset={offset}&amp;limit={limit}\")\n        dags.extend(response['dags'])\n\n        if len(response['dags']) &lt; limit:\n            break\n        offset += limit\n\n    return dags\n</code></pre> Hint 3: Triggering DAG runs <pre><code>def trigger_dag(self, dag_id, conf=None):\n    \"\"\"Trigger a DAG run.\"\"\"\n    payload = {\n        \"conf\": conf or {},\n        \"note\": \"Triggered by maintenance script\"\n    }\n    response = self.session.post(\n        f\"{self.base_url}/dags/{dag_id}/dagRuns\",\n        json=payload\n    )\n    response.raise_for_status()\n    return response.json()\n</code></pre>"},{"location":"modules/10-advanced-topics/exercises/exercise_10_1_api_automation/#success-criteria","title":"Success Criteria","text":"<ul> <li> Successfully connects to Airflow API</li> <li> Lists all DAGs with relevant properties</li> <li> Correctly identifies stale DAGs (no run in 7 days)</li> <li> Triggers DAG runs with maintenance flag</li> <li> Monitors run status until completion</li> <li> Generates comprehensive report</li> <li> Handles errors gracefully (import errors, API errors)</li> <li> Code is well-documented and reusable</li> </ul> <p>Next: Exercise 10.2: Custom Timetable \u2192</p> <p>_[DAG]: Directed Acyclic Graph - Airflow's core workflow abstraction _[XCom]: Cross-Communication - Airflow's mechanism for task-to-task data passing _[Executor]: The component that runs tasks (Local, Celery, Kubernetes, etc.) _[Scheduler]: The component that triggers DAGs and determines task execution order _[Webserver]: The component that serves the Airflow UI _[Provider]: An Airflow package that integrates with external systems _[TaskFlow]: The modern Python-native API for defining Airflow tasks using decorators _[Asset]: A logical representation of data that DAGs can produce or consume _[SDK]: Software Development Kit - the airflow.sdk module in Airflow 3.x _[K8s]: Kubernetes - container orchestration platform _[uv]: A fast Python package manager by Astral _[ruff]: A fast Python linter and formatter by Astral _[CI]: Continuous Integration _[CD]: Continuous Deployment *[PR]: Pull Request</p>"},{"location":"modules/10-advanced-topics/exercises/exercise_10_2_custom_timetable/","title":"Exercise 10.2: Custom Timetable","text":""},{"location":"modules/10-advanced-topics/exercises/exercise_10_2_custom_timetable/#objective","title":"Objective","text":"<p>Create a custom timetable that implements complex business scheduling logic beyond what cron expressions can handle.</p>"},{"location":"modules/10-advanced-topics/exercises/exercise_10_2_custom_timetable/#background","title":"Background","text":"<p>Airflow 3's timetable system allows custom scheduling logic for scenarios like: - Business day calculations - Holiday-aware scheduling - Variable intervals based on data conditions - Month-relative scheduling (first Monday, last Friday)</p>"},{"location":"modules/10-advanced-topics/exercises/exercise_10_2_custom_timetable/#timetable-architecture","title":"Timetable Architecture","text":"<pre><code>from airflow.timetables.base import DagRunInfo, DataInterval, Timetable\nfrom pendulum import DateTime\n\nclass CustomTimetable(Timetable):\n    \"\"\"Custom scheduling logic.\"\"\"\n\n    def next_dagrun_info(\n        self,\n        *,\n        last_automated_data_interval: DataInterval | None,\n        restriction: TimeRestriction,\n    ) -&gt; DagRunInfo | None:\n        \"\"\"Calculate next DAG run timing.\"\"\"\n        pass\n\n    def infer_manual_data_interval(\n        self,\n        *,\n        run_after: DateTime,\n    ) -&gt; DataInterval:\n        \"\"\"Handle manual triggers.\"\"\"\n        pass\n</code></pre>"},{"location":"modules/10-advanced-topics/exercises/exercise_10_2_custom_timetable/#requirements","title":"Requirements","text":"<p>Create a timetable implementing this business schedule:</p>"},{"location":"modules/10-advanced-topics/exercises/exercise_10_2_custom_timetable/#part-1-basic-weekday-schedule","title":"Part 1: Basic Weekday Schedule","text":"<ol> <li>Run every weekday (Monday-Friday) at 9:00 AM</li> <li>Skip weekends entirely</li> <li>Handle timezone correctly (use business timezone)</li> </ol>"},{"location":"modules/10-advanced-topics/exercises/exercise_10_2_custom_timetable/#part-2-month-relative-exceptions","title":"Part 2: Month-Relative Exceptions","text":"<ol> <li>Skip the first Monday of each month (monthly planning meetings)</li> <li>Add an extra run on the last Friday of each month at 5:00 PM (month-end processing)</li> </ol>"},{"location":"modules/10-advanced-topics/exercises/exercise_10_2_custom_timetable/#part-3-holiday-awareness-bonus","title":"Part 3: Holiday Awareness (Bonus)","text":"<ol> <li>Define a list of company holidays</li> <li>Skip runs on holidays</li> <li>Optionally reschedule to next business day</li> </ol>"},{"location":"modules/10-advanced-topics/exercises/exercise_10_2_custom_timetable/#expected-schedule-examples","title":"Expected Schedule Examples","text":"<pre><code>January 2024:\n- Mon Jan 1  - SKIP (holiday: New Year)\n- Tue Jan 2  - RUN 9:00 AM\n- Wed Jan 3  - RUN 9:00 AM\n- ...\n- Mon Jan 8  - SKIP (first Monday of month)\n- Tue Jan 9  - RUN 9:00 AM\n- ...\n- Fri Jan 26 - RUN 9:00 AM AND 5:00 PM (last Friday)\n- Mon Jan 29 - RUN 9:00 AM\n- ...\n</code></pre>"},{"location":"modules/10-advanced-topics/exercises/exercise_10_2_custom_timetable/#starter-code","title":"Starter Code","text":"<p>See <code>exercise_10_2_custom_timetable_starter.py</code></p>"},{"location":"modules/10-advanced-topics/exercises/exercise_10_2_custom_timetable/#hints","title":"Hints","text":"Hint 1: Finding month-relative days <pre><code>import pendulum\nfrom pendulum import DateTime\n\ndef get_first_monday(year: int, month: int) -&gt; DateTime:\n    \"\"\"Get the first Monday of a month.\"\"\"\n    first_day = pendulum.datetime(year, month, 1)\n    # Monday is weekday 0\n    days_until_monday = (7 - first_day.weekday()) % 7\n    if first_day.weekday() == 0:  # Already Monday\n        return first_day\n    return first_day.add(days=days_until_monday)\n\ndef get_last_friday(year: int, month: int) -&gt; DateTime:\n    \"\"\"Get the last Friday of a month.\"\"\"\n    # Go to last day of month\n    last_day = pendulum.datetime(year, month, 1).end_of(\"month\")\n    # Friday is weekday 4\n    days_since_friday = (last_day.weekday() - 4) % 7\n    return last_day.subtract(days=days_since_friday)\n</code></pre> Hint 2: Timetable serialization <pre><code>class BusinessDayTimetable(Timetable):\n    \"\"\"Timetables must be serializable for the database.\"\"\"\n\n    def __init__(self, timezone: str = \"America/New_York\"):\n        self.timezone = timezone\n\n    def serialize(self) -&gt; dict:\n        \"\"\"Serialize for storage.\"\"\"\n        return {\"timezone\": self.timezone}\n\n    @classmethod\n    def deserialize(cls, data: dict) -&gt; \"BusinessDayTimetable\":\n        \"\"\"Deserialize from storage.\"\"\"\n        return cls(timezone=data[\"timezone\"])\n</code></pre> Hint 3: Next run calculation <pre><code>def _get_next_run(self, after: DateTime) -&gt; DateTime | None:\n    \"\"\"Calculate next valid run time.\"\"\"\n    candidate = after.add(days=1).set(hour=9, minute=0, second=0)\n\n    # Skip to Monday if weekend\n    if candidate.weekday() == 5:  # Saturday\n        candidate = candidate.add(days=2)\n    elif candidate.weekday() == 6:  # Sunday\n        candidate = candidate.add(days=1)\n\n    # Check for first Monday skip\n    first_monday = self._get_first_monday(candidate.year, candidate.month)\n    if candidate.date() == first_monday.date():\n        candidate = candidate.add(days=1)\n\n    return candidate\n</code></pre>"},{"location":"modules/10-advanced-topics/exercises/exercise_10_2_custom_timetable/#verification","title":"Verification","text":"<p>Test your timetable:</p> <pre><code>from pendulum import datetime\nfrom your_timetable import BusinessDayTimetable\n\ntt = BusinessDayTimetable(timezone=\"America/New_York\")\n\n# Test next run calculation\ntest_date = datetime(2024, 1, 5, 10, 0, 0)  # Friday after run\nnext_run = tt._get_next_run(test_date)\nprint(f\"Next run after {test_date}: {next_run}\")\n# Should be Monday Jan 8 (skip weekend) but wait - that's first Monday!\n# Should actually be Tuesday Jan 9\n\n# Test last Friday detection\nassert tt._is_last_friday(datetime(2024, 1, 26)) == True\nassert tt._is_last_friday(datetime(2024, 1, 19)) == False\n</code></pre>"},{"location":"modules/10-advanced-topics/exercises/exercise_10_2_custom_timetable/#success-criteria","title":"Success Criteria","text":"<ul> <li> Runs every weekday at 9:00 AM</li> <li> Correctly skips weekends</li> <li> Skips first Monday of each month</li> <li> Adds extra 5:00 PM run on last Friday</li> <li> Handles timezone correctly</li> <li> Properly serializes/deserializes</li> <li> Works with manual triggers</li> <li> Integrates with a test DAG</li> </ul>"},{"location":"modules/10-advanced-topics/exercises/exercise_10_2_custom_timetable/#bonus-challenges","title":"Bonus Challenges","text":"<ol> <li>Holiday Calendar Integration: Use <code>holidays</code> library for automatic holiday detection</li> <li>Configurable Skip Days: Make the skip rules configurable per-DAG</li> <li>Catchup Behavior: Handle catchup runs correctly for historical data intervals</li> <li>UI Display: Implement <code>summary</code> property for nice display in Airflow UI</li> </ol> <p>Next: Exercise 10.3: Plugin Development \u2192</p> <p>_[DAG]: Directed Acyclic Graph - Airflow's core workflow abstraction _[XCom]: Cross-Communication - Airflow's mechanism for task-to-task data passing _[Executor]: The component that runs tasks (Local, Celery, Kubernetes, etc.) _[Scheduler]: The component that triggers DAGs and determines task execution order _[Webserver]: The component that serves the Airflow UI _[Provider]: An Airflow package that integrates with external systems _[TaskFlow]: The modern Python-native API for defining Airflow tasks using decorators _[Asset]: A logical representation of data that DAGs can produce or consume _[SDK]: Software Development Kit - the airflow.sdk module in Airflow 3.x _[K8s]: Kubernetes - container orchestration platform _[uv]: A fast Python package manager by Astral _[ruff]: A fast Python linter and formatter by Astral _[CI]: Continuous Integration _[CD]: Continuous Deployment *[PR]: Pull Request</p>"},{"location":"modules/10-advanced-topics/exercises/exercise_10_3_plugin_development/","title":"Exercise 10.3: Plugin Development","text":""},{"location":"modules/10-advanced-topics/exercises/exercise_10_3_plugin_development/#objective","title":"Objective","text":"<p>Build an Airflow plugin that extends the platform with custom views, macros, and operators to demonstrate the plugin architecture.</p>"},{"location":"modules/10-advanced-topics/exercises/exercise_10_3_plugin_development/#background","title":"Background","text":"<p>Airflow's plugin system allows extending the platform with: - Custom Views: Flask blueprints adding web UI pages - Custom Operators: Reusable task types - Custom Hooks: Connection handlers - Custom Macros: Jinja template functions - Menu Links: Navigation items in the Airflow UI</p>"},{"location":"modules/10-advanced-topics/exercises/exercise_10_3_plugin_development/#plugin-architecture","title":"Plugin Architecture","text":"<pre><code>from airflow.plugins_manager import AirflowPlugin\nfrom flask import Blueprint\n\nclass MyPlugin(AirflowPlugin):\n    name = \"my_plugin\"\n\n    # Flask blueprints for custom views\n    flask_blueprints = [my_blueprint]\n\n    # Custom operators\n    operators = [MyOperator]\n\n    # Custom hooks\n    hooks = [MyHook]\n\n    # Jinja template macros\n    macros = [my_macro_function]\n\n    # Menu items\n    appbuilder_menu_items = [\n        {\n            \"name\": \"My View\",\n            \"category\": \"Custom\",\n            \"href\": \"/myview\",\n        }\n    ]\n</code></pre>"},{"location":"modules/10-advanced-topics/exercises/exercise_10_3_plugin_development/#requirements","title":"Requirements","text":"<p>Create a plugin with three components:</p>"},{"location":"modules/10-advanced-topics/exercises/exercise_10_3_plugin_development/#part-1-task-duration-statistics-view","title":"Part 1: Task Duration Statistics View","text":"<p>Build a custom view that displays: 1. Average task duration by DAG 2. Slowest tasks across all DAGs 3. Duration trends over time 4. Filterable by date range</p>"},{"location":"modules/10-advanced-topics/exercises/exercise_10_3_plugin_development/#part-2-custom-date-formatting-macro","title":"Part 2: Custom Date Formatting Macro","text":"<p>Create a Jinja macro for date formatting: 1. <code>{{ format_business_date(ds) }}</code> \u2192 \"Monday, January 15, 2024\" 2. <code>{{ days_until_month_end(ds) }}</code> \u2192 Number of days remaining 3. <code>{{ is_business_day(ds) }}</code> \u2192 True/False</p>"},{"location":"modules/10-advanced-topics/exercises/exercise_10_3_plugin_development/#part-3-utility-operator","title":"Part 3: Utility Operator","text":"<p>Build a <code>SlackSummaryOperator</code> that: 1. Collects DAG run statistics 2. Formats a summary message 3. Sends to a Slack webhook (mock for exercise)</p>"},{"location":"modules/10-advanced-topics/exercises/exercise_10_3_plugin_development/#starter-code","title":"Starter Code","text":"<p>See <code>exercise_10_3_plugin_development_starter.py</code></p>"},{"location":"modules/10-advanced-topics/exercises/exercise_10_3_plugin_development/#expected-plugin-structure","title":"Expected Plugin Structure","text":"<pre><code>plugins/\n\u2514\u2500\u2500 task_analytics/\n    \u251c\u2500\u2500 __init__.py\n    \u251c\u2500\u2500 views/\n    \u2502   \u251c\u2500\u2500 __init__.py\n    \u2502   \u2514\u2500\u2500 duration_view.py\n    \u251c\u2500\u2500 operators/\n    \u2502   \u251c\u2500\u2500 __init__.py\n    \u2502   \u2514\u2500\u2500 slack_summary.py\n    \u251c\u2500\u2500 macros/\n    \u2502   \u251c\u2500\u2500 __init__.py\n    \u2502   \u2514\u2500\u2500 date_macros.py\n    \u2514\u2500\u2500 plugin.py\n</code></pre>"},{"location":"modules/10-advanced-topics/exercises/exercise_10_3_plugin_development/#hints","title":"Hints","text":"Hint 1: Flask Blueprint for Views <pre><code>from flask import Blueprint, render_template_string\nfrom airflow.www.app import csrf\n\nbp = Blueprint(\n    \"task_analytics\",\n    __name__,\n    template_folder=\"templates\",\n    static_folder=\"static\",\n    url_prefix=\"/task-analytics\",\n)\n\n@bp.route(\"/\")\n@csrf.exempt  # For GET requests\ndef duration_stats():\n    # Query task instances\n    from airflow.models import TaskInstance\n    from airflow.settings import Session\n\n    session = Session()\n    # Your query logic here\n    session.close()\n\n    return render_template_string(TEMPLATE, data=data)\n</code></pre> Hint 2: Querying Task Duration Statistics <pre><code>from sqlalchemy import func\nfrom airflow.models import TaskInstance, DagRun\nfrom airflow.utils.state import State\n\ndef get_duration_stats(days_back=7):\n    from airflow.settings import Session\n    from datetime import datetime, timedelta\n\n    session = Session()\n    cutoff = datetime.utcnow() - timedelta(days=days_back)\n\n    # Average duration by DAG\n    results = (\n        session.query(\n            TaskInstance.dag_id,\n            func.avg(TaskInstance.duration).label(\"avg_duration\"),\n            func.count(TaskInstance.task_id).label(\"task_count\"),\n        )\n        .filter(TaskInstance.start_date &gt; cutoff)\n        .filter(TaskInstance.state == State.SUCCESS)\n        .group_by(TaskInstance.dag_id)\n        .all()\n    )\n\n    session.close()\n    return results\n</code></pre> Hint 3: Custom Macro Registration <pre><code>from pendulum import parse\n\ndef format_business_date(ds: str) -&gt; str:\n    \"\"\"Format date as business-friendly string.\"\"\"\n    dt = parse(ds)\n    return dt.format(\"dddd, MMMM D, YYYY\")\n\ndef days_until_month_end(ds: str) -&gt; int:\n    \"\"\"Calculate days remaining in month.\"\"\"\n    dt = parse(ds)\n    end_of_month = dt.end_of(\"month\")\n    return (end_of_month - dt).days\n\ndef is_business_day(ds: str) -&gt; bool:\n    \"\"\"Check if date is a business day (Mon-Fri).\"\"\"\n    dt = parse(ds)\n    return dt.weekday() &lt; 5  # 0-4 are Mon-Fri\n\n# Register in plugin\nclass MyPlugin(AirflowPlugin):\n    name = \"my_plugin\"\n    macros = [format_business_date, days_until_month_end, is_business_day]\n</code></pre> Hint 4: Custom Operator Structure <pre><code>from airflow.models import BaseOperator\nfrom airflow.utils.context import Context\n\nclass SlackSummaryOperator(BaseOperator):\n    template_fields = (\"message\", \"webhook_url\")\n\n    def __init__(\n        self,\n        *,\n        webhook_url: str,\n        dag_ids: list[str] | None = None,\n        lookback_hours: int = 24,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n        self.webhook_url = webhook_url\n        self.dag_ids = dag_ids\n        self.lookback_hours = lookback_hours\n\n    def execute(self, context: Context):\n        stats = self._collect_stats()\n        message = self._format_message(stats)\n        self._send_to_slack(message)\n        return stats\n\n    def _collect_stats(self) -&gt; dict:\n        \"\"\"Collect DAG run statistics.\"\"\"\n        # Query logic here\n        pass\n</code></pre>"},{"location":"modules/10-advanced-topics/exercises/exercise_10_3_plugin_development/#verification","title":"Verification","text":""},{"location":"modules/10-advanced-topics/exercises/exercise_10_3_plugin_development/#test-your-plugin","title":"Test Your Plugin","text":"<pre><code># Test macros\nfrom your_plugin.macros.date_macros import (\n    format_business_date,\n    days_until_month_end,\n    is_business_day,\n)\n\nassert format_business_date(\"2024-01-15\") == \"Monday, January 15, 2024\"\nassert is_business_day(\"2024-01-15\") == True  # Monday\nassert is_business_day(\"2024-01-13\") == False  # Saturday\n</code></pre>"},{"location":"modules/10-advanced-topics/exercises/exercise_10_3_plugin_development/#test-view","title":"Test View","text":"<pre><code># Start Airflow webserver and navigate to:\nhttp://localhost:8080/task-analytics/\n</code></pre>"},{"location":"modules/10-advanced-topics/exercises/exercise_10_3_plugin_development/#test-operator","title":"Test Operator","text":"<pre><code>from your_plugin.operators.slack_summary import SlackSummaryOperator\n\nop = SlackSummaryOperator(\n    task_id=\"send_summary\",\n    webhook_url=\"https://hooks.slack.com/...\",\n    dag_ids=[\"my_dag\"],\n    lookback_hours=24,\n)\n\n# Dry run\nstats = op._collect_stats()\nprint(op._format_message(stats))\n</code></pre>"},{"location":"modules/10-advanced-topics/exercises/exercise_10_3_plugin_development/#success-criteria","title":"Success Criteria","text":"<ul> <li> Plugin loads without errors</li> <li> Custom view accessible at <code>/task-analytics/</code></li> <li> View displays task duration statistics</li> <li> All three macros work correctly</li> <li> <code>SlackSummaryOperator</code> collects and formats stats</li> <li> Plugin appears in Airflow plugin list</li> <li> Menu item appears in Airflow UI</li> <li> Code follows Airflow plugin best practices</li> </ul>"},{"location":"modules/10-advanced-topics/exercises/exercise_10_3_plugin_development/#bonus-challenges","title":"Bonus Challenges","text":"<ol> <li>Add Charts: Use Chart.js or similar to visualize duration trends</li> <li>Add Filters: Allow filtering by DAG, date range, task state</li> <li>Export Feature: Add CSV/JSON export for statistics</li> <li>Real Slack Integration: Connect to actual Slack webhook</li> <li>Unit Tests: Write pytest tests for all plugin components</li> </ol>"},{"location":"modules/10-advanced-topics/exercises/exercise_10_3_plugin_development/#references","title":"References","text":"<ul> <li>Airflow Plugins Documentation</li> <li>Flask Blueprints</li> <li>Creating Custom Operators</li> </ul> <p>Congratulations! You've completed the Advanced Topics module!</p> <p>_[DAG]: Directed Acyclic Graph - Airflow's core workflow abstraction _[XCom]: Cross-Communication - Airflow's mechanism for task-to-task data passing _[Executor]: The component that runs tasks (Local, Celery, Kubernetes, etc.) _[Scheduler]: The component that triggers DAGs and determines task execution order _[Webserver]: The component that serves the Airflow UI _[Provider]: An Airflow package that integrates with external systems _[TaskFlow]: The modern Python-native API for defining Airflow tasks using decorators _[Asset]: A logical representation of data that DAGs can produce or consume _[SDK]: Software Development Kit - the airflow.sdk module in Airflow 3.x _[K8s]: Kubernetes - container orchestration platform _[uv]: A fast Python package manager by Astral _[ruff]: A fast Python linter and formatter by Astral _[CI]: Continuous Integration _[CD]: Continuous Deployment *[PR]: Pull Request</p>"},{"location":"modules/11-sensors-deferrable/","title":"Module 11: Sensors &amp; Deferrable Operators","text":""},{"location":"modules/11-sensors-deferrable/#overview","title":"Overview","text":"<p>This module covers Airflow's sensor patterns and deferrable operators - essential tools for building efficient workflows that wait for external conditions without consuming worker resources.</p> <p>Learning Time: 4-5 hours</p>"},{"location":"modules/11-sensors-deferrable/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this module, you will be able to:</p> <ol> <li>Understand sensor modes (poke vs reschedule) and when to use each</li> <li>Configure built-in sensors for files, HTTP, SQL, and external systems</li> <li>Convert traditional sensors to deferrable operators</li> <li>Create custom triggers for specialized waiting patterns</li> <li>Optimize resource usage with async patterns</li> </ol>"},{"location":"modules/11-sensors-deferrable/#prerequisites","title":"Prerequisites","text":"<ul> <li>Completed Modules 01-06</li> <li>Understanding of async/await in Python</li> <li>Familiarity with Airflow task states</li> </ul>"},{"location":"modules/11-sensors-deferrable/#key-concepts","title":"Key Concepts","text":""},{"location":"modules/11-sensors-deferrable/#1-sensor-fundamentals","title":"1. Sensor Fundamentals","text":"<p>Sensors are special operators that wait for a condition to be met before continuing:</p> <pre><code>from airflow.sensors.filesystem import FileSensor\n\n# Wait for a file to appear\nwait_for_file = FileSensor(\n    task_id=\"wait_for_data\",\n    filepath=\"/data/incoming/daily_export.csv\",\n    poke_interval=60,  # Check every 60 seconds\n    timeout=3600,      # Fail after 1 hour\n)\n</code></pre>"},{"location":"modules/11-sensors-deferrable/#2-sensor-modes","title":"2. Sensor Modes","text":"Mode Worker Usage Best For poke Holds worker slot Short waits (&lt;5 min) reschedule Releases worker Long waits (&gt;5 min) <pre><code># Poke mode (default) - keeps worker occupied\nsensor_poke = FileSensor(\n    task_id=\"quick_check\",\n    mode=\"poke\",\n    poke_interval=30,\n    timeout=300,\n)\n\n# Reschedule mode - releases worker between checks\nsensor_reschedule = FileSensor(\n    task_id=\"long_wait\",\n    mode=\"reschedule\",\n    poke_interval=300,  # 5 minutes\n    timeout=86400,      # 24 hours\n)\n</code></pre>"},{"location":"modules/11-sensors-deferrable/#3-built-in-sensors","title":"3. Built-in Sensors","text":"<p>Airflow provides sensors for common patterns:</p> Sensor Purpose <code>FileSensor</code> Wait for file existence <code>ExternalTaskSensor</code> Wait for another DAG/task <code>HttpSensor</code> Wait for HTTP endpoint condition <code>SqlSensor</code> Wait for SQL query result <code>DateTimeSensor</code> Wait until specific time <code>TimeDeltaSensor</code> Wait for duration after start <code>S3KeySensor</code> Wait for S3 object <code>GCSObjectExistenceSensor</code> Wait for GCS object"},{"location":"modules/11-sensors-deferrable/#4-deferrable-operators","title":"4. Deferrable Operators","text":"<p>Deferrable operators use async triggers to release worker slots entirely:</p> <pre><code>from airflow.sensors.filesystem import FileSensor\n\n# Traditional sensor (uses worker during poke/reschedule)\ntraditional = FileSensor(\n    task_id=\"traditional_wait\",\n    filepath=\"/data/file.csv\",\n    mode=\"reschedule\",\n)\n\n# Deferrable sensor (releases worker completely)\ndeferrable = FileSensor(\n    task_id=\"async_wait\",\n    filepath=\"/data/file.csv\",\n    deferrable=True,  # Enable async mode\n)\n</code></pre>"},{"location":"modules/11-sensors-deferrable/#5-how-deferrable-works","title":"5. How Deferrable Works","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    Deferrable Flow                          \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                             \u2502\n\u2502  1. Task starts on Worker                                   \u2502\n\u2502     \u2193                                                       \u2502\n\u2502  2. Task defers itself, creates Trigger                     \u2502\n\u2502     \u2193                                                       \u2502\n\u2502  3. Worker slot released (available for other tasks)        \u2502\n\u2502     \u2193                                                       \u2502\n\u2502  4. Triggerer process monitors condition (lightweight)      \u2502\n\u2502     \u2193                                                       \u2502\n\u2502  5. Condition met \u2192 Trigger fires event                     \u2502\n\u2502     \u2193                                                       \u2502\n\u2502  6. Task resumes on Worker with result                      \u2502\n\u2502                                                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"modules/11-sensors-deferrable/#6-custom-triggers","title":"6. Custom Triggers","text":"<p>Create custom triggers for specialized waiting patterns:</p> <pre><code>from airflow.triggers.base import BaseTrigger, TriggerEvent\nfrom typing import AsyncIterator\nimport asyncio\n\nclass CustomFileTrigger(BaseTrigger):\n    \"\"\"Trigger that fires when file appears.\"\"\"\n\n    def __init__(self, filepath: str, poll_interval: float = 5.0):\n        super().__init__()\n        self.filepath = filepath\n        self.poll_interval = poll_interval\n\n    def serialize(self) -&gt; tuple[str, dict]:\n        \"\"\"Serialize trigger for storage.\"\"\"\n        return (\n            \"path.to.CustomFileTrigger\",\n            {\"filepath\": self.filepath, \"poll_interval\": self.poll_interval},\n        )\n\n    async def run(self) -&gt; AsyncIterator[TriggerEvent]:\n        \"\"\"Async generator that yields when condition is met.\"\"\"\n        import aiofiles.os\n\n        while True:\n            if await aiofiles.os.path.exists(self.filepath):\n                yield TriggerEvent({\"filepath\": self.filepath, \"found\": True})\n                return\n            await asyncio.sleep(self.poll_interval)\n</code></pre>"},{"location":"modules/11-sensors-deferrable/#7-triggerer-component","title":"7. Triggerer Component","text":"<p>The triggerer is a separate Airflow component that runs triggers:</p> <pre><code># Start the triggerer (required for deferrable operators)\nairflow triggerer\n\n# In docker-compose\ntriggerer:\n  command: triggerer\n  depends_on:\n    - scheduler\n</code></pre>"},{"location":"modules/11-sensors-deferrable/#resource-comparison","title":"Resource Comparison","text":"Approach Worker Usage Scheduler Load Best For Poke mode 100% during wait Low Short waits Reschedule mode ~5% (periodic) Medium Medium waits Deferrable 0% during wait Very Low Long waits, scale"},{"location":"modules/11-sensors-deferrable/#exercises","title":"Exercises","text":""},{"location":"modules/11-sensors-deferrable/#exercise-111-sensor-modes","title":"Exercise 11.1: Sensor Modes","text":"<p>Compare poke vs reschedule modes: - Create sensors with both modes - Measure worker slot usage - Understand timeout behavior</p>"},{"location":"modules/11-sensors-deferrable/#exercise-112-deferrable-conversion","title":"Exercise 11.2: Deferrable Conversion","text":"<p>Convert traditional sensors to deferrable: - Migrate FileSensor to deferrable mode - Configure triggerer - Verify resource savings</p>"},{"location":"modules/11-sensors-deferrable/#exercise-113-custom-trigger","title":"Exercise 11.3: Custom Trigger","text":"<p>Build a custom trigger: - Create async trigger for API polling - Implement serialization - Test with deferrable operator</p>"},{"location":"modules/11-sensors-deferrable/#solutions","title":"Solutions","text":"<p>Complete solutions are in the <code>solutions/</code> directory.</p>"},{"location":"modules/11-sensors-deferrable/#common-patterns","title":"Common Patterns","text":""},{"location":"modules/11-sensors-deferrable/#pattern-1-file-based-data-pipeline","title":"Pattern 1: File-Based Data Pipeline","text":"<pre><code>from airflow.sdk import dag, task\nfrom airflow.sensors.filesystem import FileSensor\nfrom datetime import datetime\n\n@dag(start_date=datetime(2024, 1, 1))\ndef file_pipeline():\n    wait_for_input = FileSensor(\n        task_id=\"wait_for_input\",\n        filepath=\"/data/input/{{ ds }}/data.csv\",\n        deferrable=True,\n        poke_interval=60,\n        timeout=3600,\n    )\n\n    @task\n    def process_file():\n        # Process the file\n        pass\n\n    wait_for_input &gt;&gt; process_file()\n</code></pre>"},{"location":"modules/11-sensors-deferrable/#pattern-2-cross-dag-dependencies","title":"Pattern 2: Cross-DAG Dependencies","text":"<pre><code>from airflow.sensors.external_task import ExternalTaskSensor\n\nwait_for_upstream = ExternalTaskSensor(\n    task_id=\"wait_for_upstream\",\n    external_dag_id=\"upstream_dag\",\n    external_task_id=\"final_task\",\n    deferrable=True,\n    allowed_states=[\"success\"],\n    failed_states=[\"failed\", \"skipped\"],\n)\n</code></pre>"},{"location":"modules/11-sensors-deferrable/#pattern-3-http-api-readiness","title":"Pattern 3: HTTP API Readiness","text":"<pre><code>from airflow.sensors.http import HttpSensor\n\nwait_for_api = HttpSensor(\n    task_id=\"wait_for_api\",\n    http_conn_id=\"api_connection\",\n    endpoint=\"/health\",\n    response_check=lambda response: response.json()[\"status\"] == \"ready\",\n    deferrable=True,\n    poke_interval=30,\n)\n</code></pre>"},{"location":"modules/11-sensors-deferrable/#best-practices","title":"Best Practices","text":"<ol> <li>Choose the right mode:</li> <li>Poke: waits &lt; 5 minutes</li> <li>Reschedule: waits 5-60 minutes</li> <li> <p>Deferrable: waits &gt; 60 minutes or at scale</p> </li> <li> <p>Set appropriate timeouts:    <pre><code>sensor = FileSensor(\n    timeout=3600,           # Max wait time\n    soft_fail=True,         # Mark as skipped instead of failed\n    poke_interval=60,       # Balance between responsiveness and load\n)\n</code></pre></p> </li> <li> <p>Use exponential backoff for flaky conditions:    <pre><code>sensor = HttpSensor(\n    exponential_backoff=True,\n    poke_interval=10,       # Starts at 10s\n    # Increases: 10, 20, 40, 80, ... up to max\n)\n</code></pre></p> </li> <li> <p>Enable deferrable when available:    <pre><code># Check if sensor supports deferrable\nsensor = FileSensor(\n    deferrable=True,  # Airflow 2.6+ / 3.x\n)\n</code></pre></p> </li> </ol>"},{"location":"modules/11-sensors-deferrable/#troubleshooting","title":"Troubleshooting","text":""},{"location":"modules/11-sensors-deferrable/#sensor-stuck-in-running-state","title":"Sensor Stuck in Running State","text":"<pre><code># Check sensor logs\nairflow tasks logs &lt;dag_id&gt; &lt;task_id&gt; &lt;execution_date&gt;\n\n# Common causes:\n# 1. Condition never met \u2192 Check file/endpoint existence\n# 2. Timeout too long \u2192 Reduce timeout value\n# 3. poke_interval too short \u2192 Increase to reduce DB load\n</code></pre>"},{"location":"modules/11-sensors-deferrable/#deferrable-not-working","title":"Deferrable Not Working","text":"<pre><code># 1. Verify triggerer is running\nairflow triggerer\n\n# 2. Check trigger registration\nairflow triggers list\n\n# 3. Verify async dependencies\npip install aiohttp aiofiles\n</code></pre>"},{"location":"modules/11-sensors-deferrable/#high-database-load","title":"High Database Load","text":"<pre><code># Reduce sensor frequency\nsensor = FileSensor(\n    poke_interval=300,  # 5 minutes instead of default 60s\n    mode=\"reschedule\",   # Release worker between checks\n)\n</code></pre>"},{"location":"modules/11-sensors-deferrable/#next-steps","title":"Next Steps","text":"<p>After completing this module: 1. Review Module 12: REST API for external integrations 2. Explore Module 13: Connections &amp; Secrets for secure configuration 3. Study Module 14: Resource Management for scaling</p>"},{"location":"modules/11-sensors-deferrable/#references","title":"References","text":"<ul> <li>Airflow Sensors Documentation</li> <li>Deferrable Operators Guide</li> <li>Writing Custom Triggers</li> </ul> <p>_[DAG]: Directed Acyclic Graph - Airflow's core workflow abstraction _[XCom]: Cross-Communication - Airflow's mechanism for task-to-task data passing _[Executor]: The component that runs tasks (Local, Celery, Kubernetes, etc.) _[Scheduler]: The component that triggers DAGs and determines task execution order _[Webserver]: The component that serves the Airflow UI _[Provider]: An Airflow package that integrates with external systems _[TaskFlow]: The modern Python-native API for defining Airflow tasks using decorators _[Asset]: A logical representation of data that DAGs can produce or consume _[SDK]: Software Development Kit - the airflow.sdk module in Airflow 3.x _[K8s]: Kubernetes - container orchestration platform _[uv]: A fast Python package manager by Astral _[ruff]: A fast Python linter and formatter by Astral _[CI]: Continuous Integration _[CD]: Continuous Deployment *[PR]: Pull Request</p>"},{"location":"modules/11-sensors-deferrable/exercises/exercise_11_1_sensor_modes/","title":"Exercise 11.1: Sensor Modes","text":""},{"location":"modules/11-sensors-deferrable/exercises/exercise_11_1_sensor_modes/#objective","title":"Objective","text":"<p>Understand and compare Airflow sensor modes (poke vs reschedule) by building a file processing DAG that demonstrates worker resource usage patterns.</p>"},{"location":"modules/11-sensors-deferrable/exercises/exercise_11_1_sensor_modes/#background","title":"Background","text":"<p>Sensors can operate in two modes:</p> Mode Behavior Worker Usage poke Holds worker slot, sleeps between checks 100% during entire wait reschedule Releases worker, reschedules task ~0% between checks"},{"location":"modules/11-sensors-deferrable/exercises/exercise_11_1_sensor_modes/#requirements","title":"Requirements","text":""},{"location":"modules/11-sensors-deferrable/exercises/exercise_11_1_sensor_modes/#part-1-basic-sensor-setup","title":"Part 1: Basic Sensor Setup","text":"<p>Create two FileSensors with different modes:</p> <ol> <li>Poke mode sensor:</li> <li>Check for file every 10 seconds</li> <li>Timeout after 5 minutes</li> <li> <p>Log when checking</p> </li> <li> <p>Reschedule mode sensor:</p> </li> <li>Check for file every 30 seconds</li> <li>Timeout after 1 hour</li> <li>Log when checking</li> </ol>"},{"location":"modules/11-sensors-deferrable/exercises/exercise_11_1_sensor_modes/#part-2-sensor-configuration","title":"Part 2: Sensor Configuration","text":"<p>Configure sensors with: - <code>soft_fail=True</code> - Skip instead of fail on timeout - <code>exponential_backoff=True</code> - Increase interval on consecutive failures - Custom <code>poke_interval</code> values</p>"},{"location":"modules/11-sensors-deferrable/exercises/exercise_11_1_sensor_modes/#part-3-multiple-sensor-types","title":"Part 3: Multiple Sensor Types","text":"<p>Add additional sensors: - <code>DateTimeSensor</code> - Wait until specific time - <code>TimeDeltaSensor</code> - Wait for duration after start - <code>HttpSensor</code> - Wait for API endpoint (mock)</p>"},{"location":"modules/11-sensors-deferrable/exercises/exercise_11_1_sensor_modes/#part-4-monitoring","title":"Part 4: Monitoring","text":"<p>Implement logging to observe: - When each sensor starts - Each poke attempt - Worker slot status - Final outcome</p>"},{"location":"modules/11-sensors-deferrable/exercises/exercise_11_1_sensor_modes/#expected-file-structure","title":"Expected File Structure","text":"<pre><code>/tmp/airflow_sensor_exercise/\n\u251c\u2500\u2500 input/\n\u2502   \u2514\u2500\u2500 data_{{ ds }}.csv     # File to wait for\n\u2514\u2500\u2500 processed/\n    \u2514\u2500\u2500 result_{{ ds }}.json  # Processed output\n</code></pre>"},{"location":"modules/11-sensors-deferrable/exercises/exercise_11_1_sensor_modes/#starter-code","title":"Starter Code","text":"<p>See <code>exercise_11_1_sensor_modes_starter.py</code></p>"},{"location":"modules/11-sensors-deferrable/exercises/exercise_11_1_sensor_modes/#verification","title":"Verification","text":"<p>Test your implementation:</p> <pre><code># Create test file to trigger sensors\nmkdir -p /tmp/airflow_sensor_exercise/input\necho \"data\" &gt; /tmp/airflow_sensor_exercise/input/data_2024-01-15.csv\n\n# Trigger DAG and observe behavior\nairflow dags trigger exercise_11_1_sensor_modes\n\n# Monitor task states\nairflow tasks states-for-dag-run exercise_11_1_sensor_modes &lt;run_id&gt;\n</code></pre>"},{"location":"modules/11-sensors-deferrable/exercises/exercise_11_1_sensor_modes/#hints","title":"Hints","text":"Hint 1: FileSensor configuration <pre><code>from airflow.sensors.filesystem import FileSensor\n\nwait_poke = FileSensor(\n    task_id=\"wait_poke_mode\",\n    filepath=\"/tmp/data/{{ ds }}/file.csv\",\n    mode=\"poke\",\n    poke_interval=10,\n    timeout=300,\n)\n</code></pre> Hint 2: Reschedule mode setup <pre><code>wait_reschedule = FileSensor(\n    task_id=\"wait_reschedule_mode\",\n    filepath=\"/tmp/data/{{ ds }}/file.csv\",\n    mode=\"reschedule\",\n    poke_interval=30,\n    timeout=3600,\n    soft_fail=True,\n)\n</code></pre> Hint 3: DateTimeSensor usage <pre><code>from airflow.sensors.date_time import DateTimeSensor\nfrom pendulum import datetime\n\nwait_until = DateTimeSensor(\n    task_id=\"wait_until_time\",\n    target_time=\"{{ execution_date.add(minutes=5) }}\",\n)\n</code></pre>"},{"location":"modules/11-sensors-deferrable/exercises/exercise_11_1_sensor_modes/#success-criteria","title":"Success Criteria","text":"<ul> <li> Both sensor modes implemented correctly</li> <li> Sensors use templated file paths</li> <li> Appropriate poke_interval for each mode</li> <li> Timeout and soft_fail configured</li> <li> Multiple sensor types demonstrated</li> <li> Logging shows poke attempts</li> <li> DAG completes when file exists</li> <li> Resource usage difference observable</li> </ul> <p>Next: Exercise 11.2: Deferrable Conversion \u2192</p> <p>_[DAG]: Directed Acyclic Graph - Airflow's core workflow abstraction _[XCom]: Cross-Communication - Airflow's mechanism for task-to-task data passing _[Executor]: The component that runs tasks (Local, Celery, Kubernetes, etc.) _[Scheduler]: The component that triggers DAGs and determines task execution order _[Webserver]: The component that serves the Airflow UI _[Provider]: An Airflow package that integrates with external systems _[TaskFlow]: The modern Python-native API for defining Airflow tasks using decorators _[Asset]: A logical representation of data that DAGs can produce or consume _[SDK]: Software Development Kit - the airflow.sdk module in Airflow 3.x _[K8s]: Kubernetes - container orchestration platform _[uv]: A fast Python package manager by Astral _[ruff]: A fast Python linter and formatter by Astral _[CI]: Continuous Integration _[CD]: Continuous Deployment *[PR]: Pull Request</p>"},{"location":"modules/11-sensors-deferrable/exercises/exercise_11_2_deferrable_conversion/","title":"Exercise 11.2: Deferrable Conversion","text":""},{"location":"modules/11-sensors-deferrable/exercises/exercise_11_2_deferrable_conversion/#objective","title":"Objective","text":"<p>Convert traditional sensors to deferrable operators to achieve maximum resource efficiency with zero worker usage during waits.</p>"},{"location":"modules/11-sensors-deferrable/exercises/exercise_11_2_deferrable_conversion/#background","title":"Background","text":"<p>Deferrable operators release worker slots completely by using async triggers:</p> <pre><code>Traditional Sensor (reschedule):\n  [Worker] \u2192 sleep \u2192 [Worker] \u2192 sleep \u2192 [Worker] \u2192 done\n             \u2191 releases \u2191        \u2191 releases \u2191\n\nDeferrable Operator:\n  [Worker] \u2192 defer \u2192 [Triggerer async] \u2192 [Worker] \u2192 done\n             \u2191 0% worker usage \u2191\n</code></pre>"},{"location":"modules/11-sensors-deferrable/exercises/exercise_11_2_deferrable_conversion/#key-components","title":"Key Components","text":"<ol> <li>Deferrable Operator: Task that can defer itself</li> <li>Trigger: Async class that monitors the condition</li> <li>Triggerer: Separate process that runs triggers</li> </ol>"},{"location":"modules/11-sensors-deferrable/exercises/exercise_11_2_deferrable_conversion/#requirements","title":"Requirements","text":""},{"location":"modules/11-sensors-deferrable/exercises/exercise_11_2_deferrable_conversion/#part-1-enable-deferrable-mode","title":"Part 1: Enable Deferrable Mode","text":"<p>Convert existing sensors to deferrable:</p> <pre><code># Before (traditional)\nsensor = FileSensor(\n    task_id=\"wait\",\n    filepath=\"/path/to/file\",\n    mode=\"reschedule\",\n)\n\n# After (deferrable)\nsensor = FileSensor(\n    task_id=\"wait\",\n    filepath=\"/path/to/file\",\n    deferrable=True,  # Key change!\n)\n</code></pre>"},{"location":"modules/11-sensors-deferrable/exercises/exercise_11_2_deferrable_conversion/#part-2-multiple-deferrable-sensors","title":"Part 2: Multiple Deferrable Sensors","text":"<p>Create deferrable versions of: - FileSensor - DateTimeSensor - TimeDeltaSensor</p>"},{"location":"modules/11-sensors-deferrable/exercises/exercise_11_2_deferrable_conversion/#part-3-resource-monitoring","title":"Part 3: Resource Monitoring","text":"<p>Implement monitoring to verify: - Worker slots are released during defer - Trigger is running in triggerer process - Task resumes correctly after condition met</p>"},{"location":"modules/11-sensors-deferrable/exercises/exercise_11_2_deferrable_conversion/#part-4-error-handling","title":"Part 4: Error Handling","text":"<p>Handle deferrable-specific scenarios: - Triggerer not running - Trigger timeout - Trigger errors</p>"},{"location":"modules/11-sensors-deferrable/exercises/exercise_11_2_deferrable_conversion/#starter-code","title":"Starter Code","text":"<p>See <code>exercise_11_2_deferrable_conversion_starter.py</code></p>"},{"location":"modules/11-sensors-deferrable/exercises/exercise_11_2_deferrable_conversion/#verification","title":"Verification","text":"<pre><code># Ensure triggerer is running\nairflow triggerer\n\n# Check active triggers\nairflow triggers list\n\n# Monitor trigger events\ntail -f $AIRFLOW_HOME/logs/triggerer/*.log\n</code></pre>"},{"location":"modules/11-sensors-deferrable/exercises/exercise_11_2_deferrable_conversion/#hints","title":"Hints","text":"Hint 1: Enabling deferrable mode <pre><code>from airflow.sensors.filesystem import FileSensor\n\n# Simply add deferrable=True\nwait_for_file = FileSensor(\n    task_id=\"wait_for_file\",\n    filepath=\"/data/{{ ds }}/input.csv\",\n    deferrable=True,\n    poke_interval=60,  # Still used as trigger poll interval\n    timeout=3600,\n)\n</code></pre> Hint 2: Checking if deferrable is supported <pre><code># Not all sensors support deferrable mode\n# Check the documentation or source code\n\n# Sensors with deferrable support in Airflow 3.x:\n# - FileSensor\n# - DateTimeSensor\n# - TimeDeltaSensor\n# - HttpSensor (with deferrable=True)\n# - ExternalTaskSensor\n# - S3KeySensor (in AWS provider)\n</code></pre> Hint 3: Triggerer configuration <pre><code># docker-compose.yml\ntriggerer:\n  image: apache/airflow:3.1.5\n  command: triggerer\n  depends_on:\n    - scheduler\n  environment:\n    AIRFLOW__CORE__EXECUTOR: LocalExecutor\n\n# Or start manually:\n# airflow triggerer\n</code></pre>"},{"location":"modules/11-sensors-deferrable/exercises/exercise_11_2_deferrable_conversion/#success-criteria","title":"Success Criteria","text":"<ul> <li> All sensors converted to deferrable mode</li> <li> Triggerer process is running</li> <li> Worker slots released during defer (verify in logs)</li> <li> Tasks resume correctly after trigger fires</li> <li> Timeout handling works correctly</li> <li> Error scenarios handled gracefully</li> </ul> <p>Next: Exercise 11.3: Custom Trigger \u2192</p> <p>_[DAG]: Directed Acyclic Graph - Airflow's core workflow abstraction _[XCom]: Cross-Communication - Airflow's mechanism for task-to-task data passing _[Executor]: The component that runs tasks (Local, Celery, Kubernetes, etc.) _[Scheduler]: The component that triggers DAGs and determines task execution order _[Webserver]: The component that serves the Airflow UI _[Provider]: An Airflow package that integrates with external systems _[TaskFlow]: The modern Python-native API for defining Airflow tasks using decorators _[Asset]: A logical representation of data that DAGs can produce or consume _[SDK]: Software Development Kit - the airflow.sdk module in Airflow 3.x _[K8s]: Kubernetes - container orchestration platform _[uv]: A fast Python package manager by Astral _[ruff]: A fast Python linter and formatter by Astral _[CI]: Continuous Integration _[CD]: Continuous Deployment *[PR]: Pull Request</p>"},{"location":"modules/11-sensors-deferrable/exercises/exercise_11_3_custom_trigger/","title":"Exercise 11.3: Custom Trigger","text":""},{"location":"modules/11-sensors-deferrable/exercises/exercise_11_3_custom_trigger/#objective","title":"Objective","text":"<p>Build a custom async trigger for specialized waiting patterns that aren't covered by built-in sensors.</p>"},{"location":"modules/11-sensors-deferrable/exercises/exercise_11_3_custom_trigger/#background","title":"Background","text":"<p>Custom triggers enable deferrable operators for any waiting scenario:</p> <pre><code>from airflow.triggers.base import BaseTrigger, TriggerEvent\nfrom typing import AsyncIterator\n\nclass MyTrigger(BaseTrigger):\n    \"\"\"Custom trigger for specialized waiting.\"\"\"\n\n    def serialize(self) -&gt; tuple[str, dict]:\n        \"\"\"Required: Serialize for database storage.\"\"\"\n        pass\n\n    async def run(self) -&gt; AsyncIterator[TriggerEvent]:\n        \"\"\"Required: Async generator that yields when done.\"\"\"\n        pass\n</code></pre>"},{"location":"modules/11-sensors-deferrable/exercises/exercise_11_3_custom_trigger/#trigger-lifecycle","title":"Trigger Lifecycle","text":"<pre><code>1. Operator calls self.defer(trigger=MyTrigger(...))\n2. Trigger serialized and stored in database\n3. Triggerer process picks up trigger\n4. Triggerer runs trigger.run() async\n5. Trigger yields TriggerEvent when condition met\n6. Operator.execute_complete() receives event\n</code></pre>"},{"location":"modules/11-sensors-deferrable/exercises/exercise_11_3_custom_trigger/#requirements","title":"Requirements","text":""},{"location":"modules/11-sensors-deferrable/exercises/exercise_11_3_custom_trigger/#part-1-api-polling-trigger","title":"Part 1: API Polling Trigger","text":"<p>Create a trigger that polls an HTTP endpoint:</p> <pre><code>class HttpPollingTrigger(BaseTrigger):\n    \"\"\"Poll HTTP endpoint until condition is met.\"\"\"\n\n    def __init__(\n        self,\n        endpoint: str,\n        headers: dict = None,\n        response_check: str = \"status\",  # JSON path to check\n        expected_value: str = \"ready\",\n        poll_interval: float = 30.0,\n        timeout: float = 3600.0,\n    ):\n        ...\n</code></pre>"},{"location":"modules/11-sensors-deferrable/exercises/exercise_11_3_custom_trigger/#part-2-deferrable-operator","title":"Part 2: Deferrable Operator","text":"<p>Create an operator that uses your trigger:</p> <pre><code>class HttpPollingOperator(BaseDeferrableOperator):\n    \"\"\"Wait for HTTP endpoint to return expected value.\"\"\"\n\n    def execute(self, context):\n        # Check immediately, defer if not ready\n        ...\n\n    def execute_complete(self, context, event):\n        # Called when trigger fires\n        ...\n</code></pre>"},{"location":"modules/11-sensors-deferrable/exercises/exercise_11_3_custom_trigger/#part-3-file-pattern-trigger-alternative","title":"Part 3: File Pattern Trigger (Alternative)","text":"<p>If HTTP is complex, create a simpler trigger:</p> <pre><code>class FilePatternTrigger(BaseTrigger):\n    \"\"\"Wait for files matching a glob pattern.\"\"\"\n\n    def __init__(\n        self,\n        pattern: str,          # e.g., \"/data/*.csv\"\n        min_files: int = 1,    # Minimum files to find\n        poll_interval: float = 10.0,\n    ):\n        ...\n</code></pre>"},{"location":"modules/11-sensors-deferrable/exercises/exercise_11_3_custom_trigger/#starter-code","title":"Starter Code","text":"<p>See <code>exercise_11_3_custom_trigger_starter.py</code></p>"},{"location":"modules/11-sensors-deferrable/exercises/exercise_11_3_custom_trigger/#hints","title":"Hints","text":"Hint 1: Trigger serialization <pre><code>def serialize(self) -&gt; tuple[str, dict]:\n    \"\"\"\n    Return (classpath, kwargs) for reconstruction.\n\n    The classpath must be importable by the triggerer.\n    \"\"\"\n    return (\n        \"mymodule.triggers.HttpPollingTrigger\",\n        {\n            \"endpoint\": self.endpoint,\n            \"headers\": self.headers,\n            \"poll_interval\": self.poll_interval,\n        },\n    )\n</code></pre> Hint 2: Async HTTP requests <pre><code>import aiohttp\n\nasync def _check_endpoint(self) -&gt; bool:\n    \"\"\"Make async HTTP request.\"\"\"\n    async with aiohttp.ClientSession() as session:\n        async with session.get(\n            self.endpoint,\n            headers=self.headers,\n        ) as response:\n            if response.status == 200:\n                data = await response.json()\n                return data.get(self.response_check) == self.expected_value\n    return False\n</code></pre> Hint 3: Trigger run method <pre><code>async def run(self) -&gt; AsyncIterator[TriggerEvent]:\n    \"\"\"Async generator that polls until condition met.\"\"\"\n    import asyncio\n    from datetime import datetime, timedelta\n\n    start_time = datetime.utcnow()\n    timeout_at = start_time + timedelta(seconds=self.timeout)\n\n    while datetime.utcnow() &lt; timeout_at:\n        try:\n            if await self._check_condition():\n                yield TriggerEvent({\"status\": \"success\"})\n                return\n        except Exception as e:\n            self.log.warning(f\"Check failed: {e}\")\n\n        await asyncio.sleep(self.poll_interval)\n\n    # Timeout\n    yield TriggerEvent({\"status\": \"timeout\"})\n</code></pre> Hint 4: Deferrable operator pattern <pre><code>from airflow.models import BaseDeferrableOperator\n\nclass MyDeferrableOperator(BaseDeferrableOperator):\n    def execute(self, context):\n        # Check condition immediately\n        if self._condition_met():\n            return {\"status\": \"immediate\"}\n\n        # Defer to trigger\n        self.defer(\n            trigger=MyTrigger(\n                param1=self.param1,\n                poll_interval=self.poll_interval,\n            ),\n            method_name=\"execute_complete\",\n        )\n\n    def execute_complete(self, context, event):\n        \"\"\"Called when trigger fires.\"\"\"\n        if event[\"status\"] == \"timeout\":\n            raise AirflowException(\"Trigger timed out\")\n        return event\n</code></pre>"},{"location":"modules/11-sensors-deferrable/exercises/exercise_11_3_custom_trigger/#success-criteria","title":"Success Criteria","text":"<ul> <li> Custom trigger implements serialize() correctly</li> <li> Trigger run() is an async generator</li> <li> Trigger handles timeout gracefully</li> <li> Deferrable operator uses the trigger</li> <li> execute_complete handles trigger events</li> <li> Trigger works with triggerer process</li> <li> Error scenarios handled properly</li> </ul> <p>Congratulations! You've completed the Sensors &amp; Deferrable module!</p> <p>_[DAG]: Directed Acyclic Graph - Airflow's core workflow abstraction _[XCom]: Cross-Communication - Airflow's mechanism for task-to-task data passing _[Executor]: The component that runs tasks (Local, Celery, Kubernetes, etc.) _[Scheduler]: The component that triggers DAGs and determines task execution order _[Webserver]: The component that serves the Airflow UI _[Provider]: An Airflow package that integrates with external systems _[TaskFlow]: The modern Python-native API for defining Airflow tasks using decorators _[Asset]: A logical representation of data that DAGs can produce or consume _[SDK]: Software Development Kit - the airflow.sdk module in Airflow 3.x _[K8s]: Kubernetes - container orchestration platform _[uv]: A fast Python package manager by Astral _[ruff]: A fast Python linter and formatter by Astral _[CI]: Continuous Integration _[CD]: Continuous Deployment *[PR]: Pull Request</p>"},{"location":"modules/11-sensors-deferrable/exercises/exercise_11_4_vector_store_sensor/","title":"Exercise 11.4: Vector Store Sensor","text":""},{"location":"modules/11-sensors-deferrable/exercises/exercise_11_4_vector_store_sensor/#objective","title":"Objective","text":"<p>Build a deferrable sensor that waits for vector store indexing operations to complete, demonstrating async patterns for AI/ML pipelines.</p>"},{"location":"modules/11-sensors-deferrable/exercises/exercise_11_4_vector_store_sensor/#background","title":"Background","text":"<p>Vector store operations in AI/ML pipelines often involve:</p> Operation Duration Pattern Embedding ingestion Minutes to hours Async polling Index building Minutes to hours Deferrable sensor Collection creation Seconds Quick check Health checks Milliseconds Sync validation <p>Traditional sensors holding worker slots for hours is wasteful. Deferrable sensors release the worker and use the triggerer service for efficient async waiting.</p>"},{"location":"modules/11-sensors-deferrable/exercises/exercise_11_4_vector_store_sensor/#requirements","title":"Requirements","text":""},{"location":"modules/11-sensors-deferrable/exercises/exercise_11_4_vector_store_sensor/#task-1-vector-store-status-trigger","title":"Task 1: Vector Store Status Trigger","text":"<p>Create a custom trigger that:</p> <ul> <li>Polls vector store indexing status asynchronously</li> <li>Supports multiple vector store backends (simulated)</li> <li>Handles various status states (indexing, ready, failed)</li> <li>Times out gracefully with informative errors</li> </ul>"},{"location":"modules/11-sensors-deferrable/exercises/exercise_11_4_vector_store_sensor/#task-2-deferrable-vector-store-sensor","title":"Task 2: Deferrable Vector Store Sensor","text":"<p>Implement a sensor that:</p> <ul> <li>Defers to custom trigger for async waiting</li> <li>Validates collection exists before waiting</li> <li>Provides progress information when available</li> <li>Handles different failure modes</li> </ul>"},{"location":"modules/11-sensors-deferrable/exercises/exercise_11_4_vector_store_sensor/#task-3-index-health-check","title":"Task 3: Index Health Check","text":"<p>Add validation that:</p> <ul> <li>Verifies index is queryable after creation</li> <li>Checks document count matches expected</li> <li>Reports index statistics (dimensions, document count)</li> </ul>"},{"location":"modules/11-sensors-deferrable/exercises/exercise_11_4_vector_store_sensor/#task-4-pipeline-integration","title":"Task 4: Pipeline Integration","text":"<p>Create a DAG that:</p> <ul> <li>Ingests documents to vector store</li> <li>Uses sensor to wait for indexing</li> <li>Validates index health</li> <li>Runs sample queries to verify</li> </ul>"},{"location":"modules/11-sensors-deferrable/exercises/exercise_11_4_vector_store_sensor/#starter-code","title":"Starter Code","text":"<p>See <code>exercise_11_4_vector_store_sensor_starter.py</code></p>"},{"location":"modules/11-sensors-deferrable/exercises/exercise_11_4_vector_store_sensor/#hints","title":"Hints","text":"Hint 1: Custom Trigger Structure <pre><code>from airflow.triggers.base import BaseTrigger, TriggerEvent\n\n\nclass VectorStoreIndexTrigger(BaseTrigger):\n    \"\"\"Trigger that waits for vector store indexing to complete.\"\"\"\n\n    def __init__(\n        self,\n        collection_name: str,\n        expected_count: int,\n        poll_interval: float = 30.0,\n        timeout: float = 3600.0,\n    ):\n        super().__init__()\n        self.collection_name = collection_name\n        self.expected_count = expected_count\n        self.poll_interval = poll_interval\n        self.timeout = timeout\n\n    def serialize(self) -&gt; tuple[str, dict]:\n        return (\n            f\"{self.__class__.__module__}.{self.__class__.__name__}\",\n            {\n                \"collection_name\": self.collection_name,\n                \"expected_count\": self.expected_count,\n                \"poll_interval\": self.poll_interval,\n                \"timeout\": self.timeout,\n            },\n        )\n\n    async def run(self) -&gt; AsyncIterator[TriggerEvent]:\n        # Poll for indexing completion\n        ...\n</code></pre> Hint 2: Deferrable Sensor Pattern <pre><code>from airflow.sensors.base import BaseSensorOperator\n\n\nclass VectorStoreSensor(BaseSensorOperator):\n    \"\"\"Sensor that waits for vector store to be ready.\"\"\"\n\n    def __init__(\n        self,\n        collection_name: str,\n        expected_count: int = 0,\n        deferrable: bool = True,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n        self.collection_name = collection_name\n        self.expected_count = expected_count\n        self.deferrable = deferrable\n\n    def execute(self, context: Context) -&gt; None:\n        # Quick check first\n        if self._check_ready():\n            return\n\n        # Defer to trigger for async waiting\n        if self.deferrable:\n            self.defer(\n                trigger=VectorStoreIndexTrigger(\n                    collection_name=self.collection_name,\n                    expected_count=self.expected_count,\n                ),\n                method_name=\"execute_complete\",\n            )\n        else:\n            # Fallback to poke mode\n            super().execute(context)\n\n    def execute_complete(self, context: Context, event: dict) -&gt; None:\n        \"\"\"Called when trigger fires.\"\"\"\n        if event.get(\"status\") == \"success\":\n            self.log.info(f\"Vector store ready: {event}\")\n        else:\n            raise AirflowException(f\"Vector store error: {event}\")\n</code></pre> Hint 3: Async Status Polling <pre><code>import asyncio\nfrom datetime import datetime, timedelta\n\n\nasync def run(self) -&gt; AsyncIterator[TriggerEvent]:\n    start_time = datetime.utcnow()\n    timeout_at = start_time + timedelta(seconds=self.timeout)\n\n    while datetime.utcnow() &lt; timeout_at:\n        # Check status asynchronously\n        status = await self._check_status()\n\n        if status[\"state\"] == \"ready\":\n            yield TriggerEvent(\n                {\n                    \"status\": \"success\",\n                    \"collection\": self.collection_name,\n                    \"document_count\": status[\"count\"],\n                }\n            )\n            return\n\n        if status[\"state\"] == \"failed\":\n            yield TriggerEvent(\n                {\n                    \"status\": \"error\",\n                    \"message\": status.get(\"error\", \"Indexing failed\"),\n                }\n            )\n            return\n\n        # Log progress\n        self.log.info(f\"Indexing in progress: {status.get('progress', 'unknown')}\")\n\n        # Wait before next poll\n        await asyncio.sleep(self.poll_interval)\n\n    # Timeout\n    yield TriggerEvent(\n        {\n            \"status\": \"timeout\",\n            \"message\": f\"Indexing did not complete within {self.timeout}s\",\n        }\n    )\n</code></pre> Hint 4: Health Check Validation <pre><code>@task\ndef validate_index_health(collection_name: str, expected_count: int) -&gt; dict:\n    \"\"\"Validate vector store index is healthy and queryable.\"\"\"\n    # Get collection stats\n    stats = get_collection_stats(collection_name)\n\n    # Validate document count\n    if stats[\"document_count\"] &lt; expected_count:\n        raise AirflowException(f\"Document count {stats['document_count']} &lt; expected {expected_count}\")\n\n    # Run test query\n    test_embedding = generate_test_embedding()\n    results = query_collection(collection_name, test_embedding, k=3)\n\n    if not results:\n        raise AirflowException(\"Test query returned no results\")\n\n    return {\n        \"collection\": collection_name,\n        \"document_count\": stats[\"document_count\"],\n        \"dimensions\": stats[\"dimensions\"],\n        \"test_query_results\": len(results),\n        \"healthy\": True,\n    }\n</code></pre>"},{"location":"modules/11-sensors-deferrable/exercises/exercise_11_4_vector_store_sensor/#success-criteria","title":"Success Criteria","text":"<ul> <li> Custom trigger polls asynchronously without blocking workers</li> <li> Sensor defers properly to triggerer service</li> <li> Timeout and error handling work correctly</li> <li> Health check validates index is queryable</li> <li> Progress logging provides visibility during long waits</li> <li> Integration test runs end-to-end successfully</li> </ul>"},{"location":"modules/11-sensors-deferrable/exercises/exercise_11_4_vector_store_sensor/#files","title":"Files","text":"<ul> <li>Starter: <code>exercise_11_4_vector_store_sensor_starter.py</code></li> <li>Solution: <code>../solutions/solution_11_4_vector_store_sensor.py</code></li> </ul>"},{"location":"modules/11-sensors-deferrable/exercises/exercise_11_4_vector_store_sensor/#estimated-time","title":"Estimated Time","text":"<p>60-90 minutes</p>"},{"location":"modules/11-sensors-deferrable/exercises/exercise_11_4_vector_store_sensor/#prerequisites","title":"Prerequisites","text":"<ul> <li>Understanding of async/await patterns</li> <li>Familiarity with Airflow triggers (Exercise 11.3)</li> <li>Basic knowledge of vector databases</li> </ul> <p>\u2190 Exercise 11.3 | Module 15: AI/ML Orchestration \u2192</p> <p>_[DAG]: Directed Acyclic Graph - Airflow's core workflow abstraction _[XCom]: Cross-Communication - Airflow's mechanism for task-to-task data passing _[Executor]: The component that runs tasks (Local, Celery, Kubernetes, etc.) _[Scheduler]: The component that triggers DAGs and determines task execution order _[Webserver]: The component that serves the Airflow UI _[Provider]: An Airflow package that integrates with external systems _[TaskFlow]: The modern Python-native API for defining Airflow tasks using decorators _[Asset]: A logical representation of data that DAGs can produce or consume _[SDK]: Software Development Kit - the airflow.sdk module in Airflow 3.x _[K8s]: Kubernetes - container orchestration platform _[uv]: A fast Python package manager by Astral _[ruff]: A fast Python linter and formatter by Astral _[CI]: Continuous Integration _[CD]: Continuous Deployment *[PR]: Pull Request</p>"},{"location":"modules/12-rest-api/","title":"Module 12: REST API","text":""},{"location":"modules/12-rest-api/#overview","title":"Overview","text":"<p>This module covers Airflow's REST API v2 for programmatic access to DAGs, runs, tasks, and system information. Learn to build integrations, automation scripts, and monitoring tools.</p> <p>Learning Time: 3-4 hours</p>"},{"location":"modules/12-rest-api/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this module, you will be able to:</p> <ol> <li>Understand REST API v2 architecture and authentication</li> <li>Query and manage DAGs programmatically</li> <li>Trigger and monitor DAG runs via API</li> <li>Access task instance details and XCom values</li> <li>Build automation scripts and integrations</li> <li>Implement proper error handling and pagination</li> </ol>"},{"location":"modules/12-rest-api/#prerequisites","title":"Prerequisites","text":"<ul> <li>Completed Modules 01-06</li> <li>Basic understanding of REST APIs</li> <li>Familiarity with Python <code>requests</code> library</li> </ul>"},{"location":"modules/12-rest-api/#key-concepts","title":"Key Concepts","text":""},{"location":"modules/12-rest-api/#1-api-overview","title":"1. API Overview","text":"<p>Airflow 3.x provides a comprehensive REST API:</p> <pre><code>Base URL: http://&lt;host&gt;:&lt;port&gt;/api/v2/\n\nMain Resources:\n\u251c\u2500\u2500 /dags                    # DAG management\n\u251c\u2500\u2500 /dags/{dag_id}/dagRuns   # DAG run operations\n\u251c\u2500\u2500 /dags/{dag_id}/tasks     # Task definitions\n\u251c\u2500\u2500 /taskInstances           # Task execution details\n\u251c\u2500\u2500 /variables               # Variable management\n\u251c\u2500\u2500 /connections             # Connection management\n\u251c\u2500\u2500 /pools                   # Pool management\n\u251c\u2500\u2500 /config                  # Configuration info\n\u2514\u2500\u2500 /health                  # Health check endpoint\n</code></pre>"},{"location":"modules/12-rest-api/#2-authentication-methods","title":"2. Authentication Methods","text":"<pre><code>import requests\n\n# Method 1: Basic Authentication (default)\nsession = requests.Session()\nsession.auth = (\"admin\", \"admin\")\n\n# Method 2: Token Authentication\nsession.headers[\"Authorization\"] = \"Bearer &lt;token&gt;\"\n\n# Method 3: Session Authentication (after login)\nlogin_response = session.post(\n    \"http://localhost:8080/api/v2/auth/login\",\n    json={\"username\": \"admin\", \"password\": \"admin\"}\n)\n</code></pre>"},{"location":"modules/12-rest-api/#3-common-operations","title":"3. Common Operations","text":"<pre><code>BASE_URL = \"http://localhost:8080/api/v2\"\n\n# List all DAGs\nresponse = session.get(f\"{BASE_URL}/dags\")\ndags = response.json()[\"dags\"]\n\n# Get specific DAG\nresponse = session.get(f\"{BASE_URL}/dags/my_dag\")\ndag_details = response.json()\n\n# Trigger DAG run\nresponse = session.post(\n    f\"{BASE_URL}/dags/my_dag/dagRuns\",\n    json={\"conf\": {\"key\": \"value\"}}\n)\nrun_id = response.json()[\"dag_run_id\"]\n\n# Get DAG run status\nresponse = session.get(\n    f\"{BASE_URL}/dags/my_dag/dagRuns/{run_id}\"\n)\nstate = response.json()[\"state\"]\n</code></pre>"},{"location":"modules/12-rest-api/#4-pagination","title":"4. Pagination","text":"<p>API responses may be paginated:</p> <pre><code>def get_all_dags(session):\n    \"\"\"Handle pagination to get all DAGs.\"\"\"\n    dags = []\n    offset = 0\n    limit = 100\n\n    while True:\n        response = session.get(\n            f\"{BASE_URL}/dags\",\n            params={\"offset\": offset, \"limit\": limit}\n        )\n        data = response.json()\n        dags.extend(data[\"dags\"])\n\n        if len(data[\"dags\"]) &lt; limit:\n            break\n        offset += limit\n\n    return dags\n</code></pre>"},{"location":"modules/12-rest-api/#5-error-handling","title":"5. Error Handling","text":"<pre><code>def safe_api_call(session, url, method=\"GET\", **kwargs):\n    \"\"\"Make API call with proper error handling.\"\"\"\n    try:\n        response = session.request(method, url, **kwargs)\n        response.raise_for_status()\n        return response.json()\n\n    except requests.exceptions.HTTPError as e:\n        if response.status_code == 404:\n            return None  # Resource not found\n        elif response.status_code == 401:\n            raise AuthenticationError(\"Invalid credentials\")\n        elif response.status_code == 403:\n            raise PermissionError(\"Insufficient permissions\")\n        else:\n            raise APIError(f\"HTTP {response.status_code}: {e}\")\n\n    except requests.exceptions.ConnectionError:\n        raise ConnectionError(\"Cannot connect to Airflow API\")\n</code></pre>"},{"location":"modules/12-rest-api/#api-reference-quick-guide","title":"API Reference Quick Guide","text":""},{"location":"modules/12-rest-api/#dag-operations","title":"DAG Operations","text":"Endpoint Method Description <code>/dags</code> GET List all DAGs <code>/dags/{dag_id}</code> GET Get DAG details <code>/dags/{dag_id}</code> PATCH Update DAG (pause/unpause) <code>/dags/{dag_id}/dagRuns</code> GET List DAG runs <code>/dags/{dag_id}/dagRuns</code> POST Trigger new run <code>/dags/{dag_id}/dagRuns/{run_id}</code> GET Get run details <code>/dags/{dag_id}/dagRuns/{run_id}</code> DELETE Delete run"},{"location":"modules/12-rest-api/#task-operations","title":"Task Operations","text":"Endpoint Method Description <code>/dags/{dag_id}/tasks</code> GET List tasks in DAG <code>/dags/{dag_id}/tasks/{task_id}</code> GET Get task details <code>/taskInstances</code> GET List task instances <code>/dags/{dag_id}/dagRuns/{run_id}/taskInstances/{task_id}</code> GET Get specific task instance"},{"location":"modules/12-rest-api/#system-operations","title":"System Operations","text":"Endpoint Method Description <code>/health</code> GET Health check <code>/version</code> GET Airflow version <code>/variables</code> GET/POST List/create variables <code>/variables/{key}</code> GET/PATCH/DELETE Variable CRUD <code>/connections</code> GET/POST List/create connections <code>/pools</code> GET/POST List/create pools"},{"location":"modules/12-rest-api/#exercises","title":"Exercises","text":""},{"location":"modules/12-rest-api/#exercise-121-api-basics","title":"Exercise 12.1: API Basics","text":"<p>Learn API authentication and basic operations: - Set up authenticated session - List and query DAGs - Understand response structure</p>"},{"location":"modules/12-rest-api/#exercise-122-dag-management","title":"Exercise 12.2: DAG Management","text":"<p>Build DAG management operations: - Trigger DAG runs with configuration - Monitor run progress - Handle task instances</p>"},{"location":"modules/12-rest-api/#exercise-123-automation-script","title":"Exercise 12.3: Automation Script","text":"<p>Create a comprehensive automation tool: - Find stale DAGs - Trigger maintenance runs - Generate health reports</p>"},{"location":"modules/12-rest-api/#solutions","title":"Solutions","text":"<p>Complete solutions are in the <code>solutions/</code> directory.</p>"},{"location":"modules/12-rest-api/#common-patterns","title":"Common Patterns","text":""},{"location":"modules/12-rest-api/#pattern-1-dag-health-checker","title":"Pattern 1: DAG Health Checker","text":"<pre><code>def check_dag_health(session, dag_id, lookback_hours=24):\n    \"\"\"Check recent DAG run health.\"\"\"\n    from datetime import datetime, timedelta\n\n    cutoff = datetime.utcnow() - timedelta(hours=lookback_hours)\n\n    response = session.get(\n        f\"{BASE_URL}/dags/{dag_id}/dagRuns\",\n        params={\n            \"start_date_gte\": cutoff.isoformat(),\n            \"order_by\": \"-start_date\",\n            \"limit\": 100,\n        }\n    )\n\n    runs = response.json()[\"dag_runs\"]\n\n    stats = {\n        \"total\": len(runs),\n        \"success\": sum(1 for r in runs if r[\"state\"] == \"success\"),\n        \"failed\": sum(1 for r in runs if r[\"state\"] == \"failed\"),\n        \"running\": sum(1 for r in runs if r[\"state\"] == \"running\"),\n    }\n    stats[\"success_rate\"] = (\n        stats[\"success\"] / stats[\"total\"] * 100\n        if stats[\"total\"] &gt; 0 else 0\n    )\n\n    return stats\n</code></pre>"},{"location":"modules/12-rest-api/#pattern-2-bulk-dag-operations","title":"Pattern 2: Bulk DAG Operations","text":"<pre><code>def pause_dags_by_tag(session, tag, pause=True):\n    \"\"\"Pause/unpause all DAGs with a specific tag.\"\"\"\n    # Get DAGs with tag\n    response = session.get(\n        f\"{BASE_URL}/dags\",\n        params={\"tags\": tag, \"limit\": 1000}\n    )\n    dags = response.json()[\"dags\"]\n\n    results = []\n    for dag in dags:\n        dag_id = dag[\"dag_id\"]\n        response = session.patch(\n            f\"{BASE_URL}/dags/{dag_id}\",\n            json={\"is_paused\": pause}\n        )\n        results.append({\n            \"dag_id\": dag_id,\n            \"status\": \"paused\" if pause else \"unpaused\",\n            \"success\": response.status_code == 200,\n        })\n\n    return results\n</code></pre>"},{"location":"modules/12-rest-api/#pattern-3-trigger-and-wait","title":"Pattern 3: Trigger and Wait","text":"<pre><code>def trigger_and_wait(session, dag_id, conf=None, timeout=3600):\n    \"\"\"Trigger DAG and wait for completion.\"\"\"\n    import time\n\n    # Trigger\n    response = session.post(\n        f\"{BASE_URL}/dags/{dag_id}/dagRuns\",\n        json={\"conf\": conf or {}}\n    )\n    response.raise_for_status()\n    run_id = response.json()[\"dag_run_id\"]\n\n    # Poll for completion\n    start = time.time()\n    while time.time() - start &lt; timeout:\n        response = session.get(\n            f\"{BASE_URL}/dags/{dag_id}/dagRuns/{run_id}\"\n        )\n        state = response.json()[\"state\"]\n\n        if state in (\"success\", \"failed\"):\n            return {\"run_id\": run_id, \"state\": state}\n\n        time.sleep(10)\n\n    return {\"run_id\": run_id, \"state\": \"timeout\"}\n</code></pre>"},{"location":"modules/12-rest-api/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Use sessions for connection pooling:    <pre><code>session = requests.Session()\nsession.auth = (username, password)\n# Reuse session for multiple requests\n</code></pre></p> </li> <li> <p>Handle pagination properly:    <pre><code># Always check if more pages exist\nwhile len(results) &lt; total_entries:\n    # Fetch next page\n</code></pre></p> </li> <li> <p>Implement retry logic:    <pre><code>from requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\n\nretry = Retry(total=3, backoff_factor=0.5)\nadapter = HTTPAdapter(max_retries=retry)\nsession.mount(\"http://\", adapter)\n</code></pre></p> </li> <li> <p>Use appropriate timeouts:    <pre><code>response = session.get(url, timeout=(5, 30))  # (connect, read)\n</code></pre></p> </li> <li> <p>Log API interactions:    <pre><code>logging.debug(f\"API call: {method} {url}\")\nlogging.debug(f\"Response: {response.status_code}\")\n</code></pre></p> </li> </ol>"},{"location":"modules/12-rest-api/#troubleshooting","title":"Troubleshooting","text":""},{"location":"modules/12-rest-api/#authentication-issues","title":"Authentication Issues","text":"<pre><code># Check API is accessible\nresponse = session.get(f\"{BASE_URL}/health\")\nprint(f\"Health check: {response.status_code}\")\n\n# Verify credentials\nresponse = session.get(f\"{BASE_URL}/dags?limit=1\")\nif response.status_code == 401:\n    print(\"Invalid credentials\")\nelif response.status_code == 403:\n    print(\"Insufficient permissions\")\n</code></pre>"},{"location":"modules/12-rest-api/#rate-limiting","title":"Rate Limiting","text":"<pre><code># Implement exponential backoff\nimport time\n\ndef api_call_with_backoff(session, url, max_retries=5):\n    for attempt in range(max_retries):\n        response = session.get(url)\n        if response.status_code == 429:  # Too Many Requests\n            wait = 2 ** attempt\n            time.sleep(wait)\n            continue\n        return response\n    raise Exception(\"Max retries exceeded\")\n</code></pre>"},{"location":"modules/12-rest-api/#next-steps","title":"Next Steps","text":"<p>After completing this module: 1. Review Module 13: Connections &amp; Secrets 2. Explore Module 14: Resource Management 3. Build your own automation tools</p>"},{"location":"modules/12-rest-api/#references","title":"References","text":"<ul> <li>Airflow REST API Reference</li> <li>API Authentication Guide</li> <li>Python Requests Library</li> </ul> <p>_[DAG]: Directed Acyclic Graph - Airflow's core workflow abstraction _[XCom]: Cross-Communication - Airflow's mechanism for task-to-task data passing _[Executor]: The component that runs tasks (Local, Celery, Kubernetes, etc.) _[Scheduler]: The component that triggers DAGs and determines task execution order _[Webserver]: The component that serves the Airflow UI _[Provider]: An Airflow package that integrates with external systems _[TaskFlow]: The modern Python-native API for defining Airflow tasks using decorators _[Asset]: A logical representation of data that DAGs can produce or consume _[SDK]: Software Development Kit - the airflow.sdk module in Airflow 3.x _[K8s]: Kubernetes - container orchestration platform _[uv]: A fast Python package manager by Astral _[ruff]: A fast Python linter and formatter by Astral _[CI]: Continuous Integration _[CD]: Continuous Deployment *[PR]: Pull Request</p>"},{"location":"modules/12-rest-api/exercises/exercise_12_1_api_basics/","title":"Exercise 12.1: API Basics","text":""},{"location":"modules/12-rest-api/exercises/exercise_12_1_api_basics/#objective","title":"Objective","text":"<p>Learn Airflow REST API v2 fundamentals including authentication, request/response handling, and basic operations.</p>"},{"location":"modules/12-rest-api/exercises/exercise_12_1_api_basics/#background","title":"Background","text":"<p>The REST API v2 provides programmatic access to Airflow:</p> <pre><code>http://&lt;airflow-host&gt;/api/v2/\n</code></pre> <p>Key endpoints: - <code>/health</code> - System health - <code>/dags</code> - DAG listing and management - <code>/version</code> - Airflow version info</p>"},{"location":"modules/12-rest-api/exercises/exercise_12_1_api_basics/#requirements","title":"Requirements","text":""},{"location":"modules/12-rest-api/exercises/exercise_12_1_api_basics/#part-1-authentication-setup","title":"Part 1: Authentication Setup","text":"<p>Create an API client with proper authentication:</p> <pre><code>class AirflowAPIClient:\n    def __init__(self, base_url, username, password):\n        # Set up authenticated session\n        pass\n\n    def health_check(self):\n        # GET /health\n        pass\n</code></pre>"},{"location":"modules/12-rest-api/exercises/exercise_12_1_api_basics/#part-2-basic-queries","title":"Part 2: Basic Queries","text":"<p>Implement these operations: 1. List all DAGs (handle pagination) 2. Get specific DAG details 3. Get Airflow version</p>"},{"location":"modules/12-rest-api/exercises/exercise_12_1_api_basics/#part-3-response-handling","title":"Part 3: Response Handling","text":"<p>Handle API responses properly: - Parse JSON responses - Handle errors (401, 403, 404, 500) - Implement retry logic</p>"},{"location":"modules/12-rest-api/exercises/exercise_12_1_api_basics/#part-4-request-headers","title":"Part 4: Request Headers","text":"<p>Configure proper headers: - Content-Type: application/json - Accept: application/json</p>"},{"location":"modules/12-rest-api/exercises/exercise_12_1_api_basics/#starter-code","title":"Starter Code","text":"<p>See <code>exercise_12_1_api_basics_starter.py</code></p>"},{"location":"modules/12-rest-api/exercises/exercise_12_1_api_basics/#hints","title":"Hints","text":"Hint 1: Session setup <pre><code>import requests\n\nsession = requests.Session()\nsession.auth = (username, password)\nsession.headers.update({\n    \"Content-Type\": \"application/json\",\n    \"Accept\": \"application/json\",\n})\n</code></pre> Hint 2: Error handling <pre><code>try:\n    response = session.get(url)\n    response.raise_for_status()\n    return response.json()\nexcept requests.exceptions.HTTPError as e:\n    if response.status_code == 404:\n        return None\n    raise\n</code></pre>"},{"location":"modules/12-rest-api/exercises/exercise_12_1_api_basics/#success-criteria","title":"Success Criteria","text":"<ul> <li> Client authenticates successfully</li> <li> Health check endpoint works</li> <li> DAG listing handles pagination</li> <li> Errors are handled gracefully</li> <li> Proper headers configured</li> </ul> <p>Next: Exercise 12.2: DAG Management \u2192</p> <p>_[DAG]: Directed Acyclic Graph - Airflow's core workflow abstraction _[XCom]: Cross-Communication - Airflow's mechanism for task-to-task data passing _[Executor]: The component that runs tasks (Local, Celery, Kubernetes, etc.) _[Scheduler]: The component that triggers DAGs and determines task execution order _[Webserver]: The component that serves the Airflow UI _[Provider]: An Airflow package that integrates with external systems _[TaskFlow]: The modern Python-native API for defining Airflow tasks using decorators _[Asset]: A logical representation of data that DAGs can produce or consume _[SDK]: Software Development Kit - the airflow.sdk module in Airflow 3.x _[K8s]: Kubernetes - container orchestration platform _[uv]: A fast Python package manager by Astral _[ruff]: A fast Python linter and formatter by Astral _[CI]: Continuous Integration _[CD]: Continuous Deployment *[PR]: Pull Request</p>"},{"location":"modules/12-rest-api/exercises/exercise_12_2_dag_management/","title":"Exercise 12.2: DAG Management","text":""},{"location":"modules/12-rest-api/exercises/exercise_12_2_dag_management/#objective","title":"Objective","text":"<p>Build DAG management operations using the REST API including triggering runs, monitoring status, and handling task instances.</p>"},{"location":"modules/12-rest-api/exercises/exercise_12_2_dag_management/#requirements","title":"Requirements","text":""},{"location":"modules/12-rest-api/exercises/exercise_12_2_dag_management/#part-1-trigger-dag-runs","title":"Part 1: Trigger DAG Runs","text":"<ul> <li>Trigger with configuration</li> <li>Handle run_id generation</li> <li>Validate trigger response</li> </ul>"},{"location":"modules/12-rest-api/exercises/exercise_12_2_dag_management/#part-2-monitor-run-progress","title":"Part 2: Monitor Run Progress","text":"<ul> <li>Poll run status until completion</li> <li>Track task instance states</li> <li>Handle timeout scenarios</li> </ul>"},{"location":"modules/12-rest-api/exercises/exercise_12_2_dag_management/#part-3-bulk-operations","title":"Part 3: Bulk Operations","text":"<ul> <li>Pause/unpause multiple DAGs</li> <li>Clear failed runs</li> <li>Retry failed tasks</li> </ul>"},{"location":"modules/12-rest-api/exercises/exercise_12_2_dag_management/#starter-code","title":"Starter Code","text":"<p>See <code>exercise_12_2_dag_management_starter.py</code></p>"},{"location":"modules/12-rest-api/exercises/exercise_12_2_dag_management/#success-criteria","title":"Success Criteria","text":"<ul> <li> Trigger DAG runs with conf</li> <li> Wait for run completion</li> <li> Track task states</li> <li> Handle errors gracefully</li> </ul> <p>Next: Exercise 12.3: Automation Script \u2192</p> <p>_[DAG]: Directed Acyclic Graph - Airflow's core workflow abstraction _[XCom]: Cross-Communication - Airflow's mechanism for task-to-task data passing _[Executor]: The component that runs tasks (Local, Celery, Kubernetes, etc.) _[Scheduler]: The component that triggers DAGs and determines task execution order _[Webserver]: The component that serves the Airflow UI _[Provider]: An Airflow package that integrates with external systems _[TaskFlow]: The modern Python-native API for defining Airflow tasks using decorators _[Asset]: A logical representation of data that DAGs can produce or consume _[SDK]: Software Development Kit - the airflow.sdk module in Airflow 3.x _[K8s]: Kubernetes - container orchestration platform _[uv]: A fast Python package manager by Astral _[ruff]: A fast Python linter and formatter by Astral _[CI]: Continuous Integration _[CD]: Continuous Deployment *[PR]: Pull Request</p>"},{"location":"modules/12-rest-api/exercises/exercise_12_3_automation_script/","title":"Exercise 12.3: Automation Script","text":""},{"location":"modules/12-rest-api/exercises/exercise_12_3_automation_script/#objective","title":"Objective","text":"<p>Build a comprehensive automation script that orchestrates Airflow operations, including deployment validation, batch triggering, health monitoring, and reporting.</p>"},{"location":"modules/12-rest-api/exercises/exercise_12_3_automation_script/#background","title":"Background","text":"<p>Production Airflow environments require automation for: - Deployment validation: Verify DAGs after deployment - Batch operations: Trigger multiple DAGs in sequence or parallel - Health monitoring: Continuous health checks with alerting - Reporting: Generate operational reports</p>"},{"location":"modules/12-rest-api/exercises/exercise_12_3_automation_script/#requirements","title":"Requirements","text":""},{"location":"modules/12-rest-api/exercises/exercise_12_3_automation_script/#part-1-deployment-validator","title":"Part 1: Deployment Validator","text":"<p>Create a validator that checks DAG health after deployments:</p> <pre><code>class DeploymentValidator:\n    def validate_dags(self, expected_dags: list) -&gt; ValidationResult:\n        \"\"\"\n        Validate that expected DAGs exist and are healthy.\n\n        Checks:\n        - DAG exists in Airflow\n        - DAG is not in error state\n        - DAG has expected tasks\n        - DAG can be parsed without errors\n        \"\"\"\n        pass\n\n    def run_smoke_test(self, dag_id: str) -&gt; bool:\n        \"\"\"\n        Run a quick test of the DAG.\n        Trigger with test config and verify completion.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"modules/12-rest-api/exercises/exercise_12_3_automation_script/#part-2-batch-orchestrator","title":"Part 2: Batch Orchestrator","text":"<p>Create an orchestrator for running multiple DAGs:</p> <pre><code>class BatchOrchestrator:\n    def run_sequence(self, dag_configs: list) -&gt; BatchResult:\n        \"\"\"\n        Run DAGs in sequence, waiting for each to complete.\n\n        dag_configs = [\n            {\"dag_id\": \"extract\", \"conf\": {...}},\n            {\"dag_id\": \"transform\", \"conf\": {...}},\n            {\"dag_id\": \"load\", \"conf\": {...}},\n        ]\n        \"\"\"\n        pass\n\n    def run_parallel(self, dag_configs: list, max_concurrent: int) -&gt; BatchResult:\n        \"\"\"\n        Run DAGs in parallel with concurrency limit.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"modules/12-rest-api/exercises/exercise_12_3_automation_script/#part-3-health-monitor","title":"Part 3: Health Monitor","text":"<p>Create a continuous health monitoring system:</p> <pre><code>class HealthMonitor:\n    def check_system_health(self) -&gt; HealthReport:\n        \"\"\"\n        Comprehensive health check:\n        - Scheduler status\n        - Worker connectivity\n        - Database health\n        - Recent failure rate\n        \"\"\"\n        pass\n\n    def get_failure_summary(self, hours: int = 24) -&gt; dict:\n        \"\"\"\n        Get summary of failures in the last N hours.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"modules/12-rest-api/exercises/exercise_12_3_automation_script/#part-4-operations-reporter","title":"Part 4: Operations Reporter","text":"<p>Generate operational reports:</p> <pre><code>class OperationsReporter:\n    def daily_summary(self) -&gt; str:\n        \"\"\"\n        Generate daily operations summary.\n\n        Includes:\n        - Total runs by state\n        - Failed DAGs with details\n        - Long-running tasks\n        - Resource utilization\n        \"\"\"\n        pass\n\n    def export_metrics(self, format: str = \"json\") -&gt; str:\n        \"\"\"\n        Export metrics in specified format.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"modules/12-rest-api/exercises/exercise_12_3_automation_script/#starter-code","title":"Starter Code","text":"<p>See <code>exercise_12_3_automation_script_starter.py</code></p>"},{"location":"modules/12-rest-api/exercises/exercise_12_3_automation_script/#hints","title":"Hints","text":"Hint 1: Parallel execution <pre><code>from concurrent.futures import ThreadPoolExecutor, as_completed\n\ndef run_parallel(self, dag_configs, max_concurrent):\n    results = []\n    with ThreadPoolExecutor(max_workers=max_concurrent) as executor:\n        futures = {\n            executor.submit(self._run_single, cfg): cfg\n            for cfg in dag_configs\n        }\n        for future in as_completed(futures):\n            config = futures[future]\n            try:\n                result = future.result()\n                results.append(result)\n            except Exception as e:\n                results.append({\"error\": str(e)})\n    return results\n</code></pre> Hint 2: Health check aggregation <pre><code>def check_system_health(self):\n    checks = {\n        \"api\": self._check_api(),\n        \"scheduler\": self._check_scheduler(),\n        \"database\": self._check_database(),\n    }\n    overall = all(c[\"healthy\"] for c in checks.values())\n    return {\"healthy\": overall, \"checks\": checks}\n</code></pre> Hint 3: Failure rate calculation <pre><code>def _get_failure_rate(self, dag_id, hours):\n    runs = self._get_recent_runs(dag_id, hours)\n    if not runs:\n        return 0.0\n    failed = sum(1 for r in runs if r[\"state\"] == \"failed\")\n    return failed / len(runs)\n</code></pre>"},{"location":"modules/12-rest-api/exercises/exercise_12_3_automation_script/#success-criteria","title":"Success Criteria","text":"<ul> <li> Deployment validator checks DAG health</li> <li> Smoke tests can be triggered</li> <li> Batch orchestrator handles sequences</li> <li> Parallel execution respects concurrency</li> <li> Health monitor provides comprehensive status</li> <li> Reporter generates useful summaries</li> <li> Error handling is robust</li> </ul>"},{"location":"modules/12-rest-api/exercises/exercise_12_3_automation_script/#testing","title":"Testing","text":"<pre><code># Run the automation script\ncd modules/12-rest-api/solutions\npython solution_12_3_automation_script.py\n\n# Expected output:\n# - Health check results\n# - Validation results\n# - Sample batch execution\n# - Operations report\n</code></pre> <p>Next: Module 13: Connections &amp; Secrets \u2192</p> <p>_[DAG]: Directed Acyclic Graph - Airflow's core workflow abstraction _[XCom]: Cross-Communication - Airflow's mechanism for task-to-task data passing _[Executor]: The component that runs tasks (Local, Celery, Kubernetes, etc.) _[Scheduler]: The component that triggers DAGs and determines task execution order _[Webserver]: The component that serves the Airflow UI _[Provider]: An Airflow package that integrates with external systems _[TaskFlow]: The modern Python-native API for defining Airflow tasks using decorators _[Asset]: A logical representation of data that DAGs can produce or consume _[SDK]: Software Development Kit - the airflow.sdk module in Airflow 3.x _[K8s]: Kubernetes - container orchestration platform _[uv]: A fast Python package manager by Astral _[ruff]: A fast Python linter and formatter by Astral _[CI]: Continuous Integration _[CD]: Continuous Deployment *[PR]: Pull Request</p>"},{"location":"modules/13-connections-secrets/","title":"Module 13: Connections &amp; Secrets","text":"<p>Master secure credential management in Airflow 3.x including connections, variables, and secrets backends.</p>"},{"location":"modules/13-connections-secrets/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this module, you will: - Understand Airflow's connection architecture - Configure connections programmatically and via UI - Implement secrets backends for production - Manage variables securely - Use connection URI format - Apply best practices for credential management</p>"},{"location":"modules/13-connections-secrets/#prerequisites","title":"Prerequisites","text":"<ul> <li>Module 01: Foundations</li> <li>Module 03: Operators &amp; Hooks</li> <li>Understanding of environment variables</li> </ul>"},{"location":"modules/13-connections-secrets/#connection-fundamentals","title":"Connection Fundamentals","text":""},{"location":"modules/13-connections-secrets/#what-are-connections","title":"What Are Connections?","text":"<p>Connections store credentials and settings for external systems:</p> <pre><code>from airflow.hooks.base import BaseHook\n\n# Get connection from Airflow's metadata database\nconn = BaseHook.get_connection(\"my_postgres\")\n\n# Access connection properties\nprint(conn.host)      # Database host\nprint(conn.login)     # Username\nprint(conn.password)  # Password (decrypted)\nprint(conn.schema)    # Database name\nprint(conn.port)      # Port number\nprint(conn.extra)     # JSON string of extra parameters\n</code></pre>"},{"location":"modules/13-connections-secrets/#connection-properties","title":"Connection Properties","text":"Property Description Example <code>conn_id</code> Unique identifier <code>\"my_postgres\"</code> <code>conn_type</code> Type of connection <code>\"postgres\"</code>, <code>\"aws\"</code> <code>host</code> Server hostname <code>\"db.example.com\"</code> <code>schema</code> Database/namespace <code>\"airflow_db\"</code> <code>login</code> Username <code>\"admin\"</code> <code>password</code> Password <code>\"secret123\"</code> <code>port</code> Port number <code>5432</code> <code>extra</code> JSON extra config <code>'{\"timeout\": 30}'</code>"},{"location":"modules/13-connections-secrets/#connection-uri-format","title":"Connection URI Format","text":"<p>Connections can be defined as URIs:</p> <pre><code>&lt;conn_type&gt;://&lt;login&gt;:&lt;password&gt;@&lt;host&gt;:&lt;port&gt;/&lt;schema&gt;?&lt;extra_params&gt;\n</code></pre> <p>Examples:</p> <pre><code># PostgreSQL\npostgresql://user:password@localhost:5432/mydb\n\n# MySQL with SSL\nmysql://user:password@host:3306/db?ssl_mode=required\n\n# S3 (with extra JSON)\naws://access_key:secret_key@?region_name=us-east-1\n\n# HTTP with headers\nhttp://@api.example.com?headers={\"Authorization\":\"Bearer token\"}\n</code></pre>"},{"location":"modules/13-connections-secrets/#connection-types","title":"Connection Types","text":""},{"location":"modules/13-connections-secrets/#database-connections","title":"Database Connections","text":"<pre><code># PostgreSQL\nfrom airflow.providers.postgres.hooks.postgres import PostgresHook\n\nhook = PostgresHook(postgres_conn_id=\"my_postgres\")\ndf = hook.get_pandas_df(\"SELECT * FROM users\")\n</code></pre>"},{"location":"modules/13-connections-secrets/#cloud-provider-connections","title":"Cloud Provider Connections","text":"<pre><code># AWS\nfrom airflow.providers.amazon.aws.hooks.s3 import S3Hook\n\nhook = S3Hook(aws_conn_id=\"aws_default\")\nhook.load_string(\"data\", bucket_name=\"mybucket\", key=\"file.txt\")\n</code></pre> <pre><code># Google Cloud\nfrom airflow.providers.google.cloud.hooks.gcs import GCSHook\n\nhook = GCSHook(gcp_conn_id=\"google_cloud_default\")\nhook.upload(bucket_name=\"mybucket\", object_name=\"file.txt\", data=\"content\")\n</code></pre>"},{"location":"modules/13-connections-secrets/#http-connections","title":"HTTP Connections","text":"<pre><code>from airflow.providers.http.hooks.http import HttpHook\n\nhook = HttpHook(http_conn_id=\"my_api\", method=\"GET\")\nresponse = hook.run(endpoint=\"/users\")\n</code></pre>"},{"location":"modules/13-connections-secrets/#programmatic-connection-management","title":"Programmatic Connection Management","text":""},{"location":"modules/13-connections-secrets/#creating-connections","title":"Creating Connections","text":"<pre><code>from airflow.models import Connection\nfrom airflow.utils.session import create_session\nimport json\n\n# Create a new connection\nnew_conn = Connection(\n    conn_id=\"my_new_postgres\",\n    conn_type=\"postgres\",\n    host=\"db.example.com\",\n    schema=\"production\",\n    login=\"app_user\",\n    password=\"secure_password\",\n    port=5432,\n    extra=json.dumps({\n        \"sslmode\": \"require\",\n        \"connect_timeout\": 10,\n    }),\n)\n\n# Add to database\nwith create_session() as session:\n    session.add(new_conn)\n    session.commit()\n</code></pre>"},{"location":"modules/13-connections-secrets/#updating-connections","title":"Updating Connections","text":"<pre><code>from airflow.models import Connection\nfrom airflow.utils.session import create_session\n\nwith create_session() as session:\n    conn = session.query(Connection).filter(\n        Connection.conn_id == \"my_postgres\"\n    ).first()\n\n    if conn:\n        conn.password = \"new_secure_password\"\n        session.commit()\n</code></pre>"},{"location":"modules/13-connections-secrets/#deleting-connections","title":"Deleting Connections","text":"<pre><code>from airflow.models import Connection\nfrom airflow.utils.session import create_session\n\nwith create_session() as session:\n    session.query(Connection).filter(\n        Connection.conn_id == \"old_connection\"\n    ).delete()\n    session.commit()\n</code></pre>"},{"location":"modules/13-connections-secrets/#environment-variable-connections","title":"Environment Variable Connections","text":""},{"location":"modules/13-connections-secrets/#airflow_conn_-prefix","title":"AIRFLOW_CONN_ Prefix","text":"<p>Define connections via environment variables:</p> <pre><code># Format: AIRFLOW_CONN_&lt;CONN_ID_UPPERCASE&gt;\nexport AIRFLOW_CONN_MY_POSTGRES=\"postgresql://user:pass@host:5432/db\"\n\n# With extra parameters\nexport AIRFLOW_CONN_MY_API=\"http://@api.example.com?headers=%7B%22Auth%22%3A%22Bearer+token%22%7D\"\n</code></pre>"},{"location":"modules/13-connections-secrets/#url-encoding","title":"URL Encoding","text":"<p>Special characters must be URL encoded:</p> <pre><code>from urllib.parse import quote_plus\n\npassword = \"p@ss#word!\"\nencoded = quote_plus(password)\n# Result: p%40ss%23word%21\n</code></pre>"},{"location":"modules/13-connections-secrets/#best-practice-connection-factory","title":"Best Practice: Connection Factory","text":"<pre><code>import os\nfrom urllib.parse import quote_plus\n\ndef create_conn_uri(\n    conn_type: str,\n    host: str,\n    login: str,\n    password: str,\n    port: int = None,\n    schema: str = None,\n    extra: dict = None,\n) -&gt; str:\n    \"\"\"Create a properly formatted connection URI.\"\"\"\n    uri = f\"{conn_type}://{quote_plus(login)}:{quote_plus(password)}@{host}\"\n\n    if port:\n        uri += f\":{port}\"\n    if schema:\n        uri += f\"/{schema}\"\n    if extra:\n        params = \"&amp;\".join(f\"{k}={quote_plus(str(v))}\" for k, v in extra.items())\n        uri += f\"?{params}\"\n\n    return uri\n</code></pre>"},{"location":"modules/13-connections-secrets/#secrets-backends","title":"Secrets Backends","text":""},{"location":"modules/13-connections-secrets/#what-is-a-secrets-backend","title":"What Is a Secrets Backend?","text":"<p>A secrets backend retrieves credentials from external secret managers instead of Airflow's database:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Airflow DAG   \u2502\n\u2502                 \u2502\n\u2502  get_connection \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502\n         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Secrets Backend \u2502\n\u2502                 \u2502\n\u2502 1. Check cache  \u2502\n\u2502 2. Query backend\u2502\n\u2502 3. Return conn  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502\n         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502           External Secrets Manager           \u2502\n\u2502                                             \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502\n\u2502  \u2502 Vault   \u2502  \u2502 AWS SM  \u2502  \u2502 GCP SM  \u2502     \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"modules/13-connections-secrets/#available-backends","title":"Available Backends","text":"Backend Provider Use Case HashiCorp Vault <code>apache-airflow-providers-hashicorp</code> Self-hosted secrets AWS Secrets Manager <code>apache-airflow-providers-amazon</code> AWS environments AWS SSM Parameter Store <code>apache-airflow-providers-amazon</code> AWS with SSM GCP Secret Manager <code>apache-airflow-providers-google</code> Google Cloud Azure Key Vault <code>apache-airflow-providers-microsoft-azure</code> Azure environments"},{"location":"modules/13-connections-secrets/#configuring-secrets-backend","title":"Configuring Secrets Backend","text":"<pre><code># airflow.cfg\n[secrets]\nbackend = airflow.providers.hashicorp.secrets.vault.VaultBackend\nbackend_kwargs = {\"connections_path\": \"airflow/connections\", \"variables_path\": \"airflow/variables\", \"url\": \"http://vault:8200\", \"token\": \"hvs.xxxxx\"}\n</code></pre> <p>Or via environment:</p> <pre><code>export AIRFLOW__SECRETS__BACKEND=\"airflow.providers.amazon.aws.secrets.secrets_manager.SecretsManagerBackend\"\nexport AIRFLOW__SECRETS__BACKEND_KWARGS='{\"connections_prefix\": \"airflow/connections\", \"variables_prefix\": \"airflow/variables\"}'\n</code></pre>"},{"location":"modules/13-connections-secrets/#aws-secrets-manager-backend","title":"AWS Secrets Manager Backend","text":"<pre><code># Configuration\nbackend_kwargs = {\n    \"connections_prefix\": \"airflow/connections\",\n    \"variables_prefix\": \"airflow/variables\",\n    \"profile_name\": \"default\",  # AWS profile\n    \"full_url_mode\": False,     # Store as JSON, not URI\n}\n\n# Secret format in AWS Secrets Manager\n# Secret name: airflow/connections/my_postgres\n# Secret value (JSON):\n{\n    \"conn_type\": \"postgres\",\n    \"host\": \"db.example.com\",\n    \"login\": \"user\",\n    \"password\": \"secret\",\n    \"port\": 5432,\n    \"schema\": \"mydb\"\n}\n</code></pre>"},{"location":"modules/13-connections-secrets/#hashicorp-vault-backend","title":"HashiCorp Vault Backend","text":"<pre><code># Configuration\nbackend_kwargs = {\n    \"connections_path\": \"secret/airflow/connections\",\n    \"variables_path\": \"secret/airflow/variables\",\n    \"url\": \"https://vault.example.com:8200\",\n    \"auth_type\": \"approle\",\n    \"role_id\": \"airflow-role\",\n    \"secret_id\": \"secret-id-here\",\n}\n\n# Store connection in Vault\n# vault kv put secret/airflow/connections/my_postgres \\\n#   conn_type=postgres host=db.example.com login=user password=secret\n</code></pre>"},{"location":"modules/13-connections-secrets/#gcp-secret-manager-backend","title":"GCP Secret Manager Backend","text":"<pre><code># Configuration\nbackend_kwargs = {\n    \"connections_prefix\": \"airflow-connections\",\n    \"variables_prefix\": \"airflow-variables\",\n    \"project_id\": \"my-gcp-project\",\n    \"gcp_keyfile_dict\": {\"type\": \"service_account\", ...}\n}\n\n# Secret name format: airflow-connections-my_postgres\n</code></pre>"},{"location":"modules/13-connections-secrets/#custom-secrets-backend","title":"Custom Secrets Backend","text":"<pre><code>from airflow.secrets import BaseSecretsBackend\nfrom typing import Optional\nimport json\n\nclass CustomSecretsBackend(BaseSecretsBackend):\n    \"\"\"Custom secrets backend example.\"\"\"\n\n    def __init__(self, api_url: str, api_key: str, **kwargs):\n        super().__init__(**kwargs)\n        self.api_url = api_url\n        self.api_key = api_key\n\n    def get_conn_value(self, conn_id: str) -&gt; Optional[str]:\n        \"\"\"\n        Get connection value (URI format).\n\n        Returns None if connection doesn't exist.\n        \"\"\"\n        try:\n            # Fetch from your secret store\n            response = self._fetch_secret(f\"connections/{conn_id}\")\n            if response:\n                return response.get(\"uri\")\n        except Exception:\n            return None\n        return None\n\n    def get_variable(self, key: str) -&gt; Optional[str]:\n        \"\"\"Get variable value.\"\"\"\n        try:\n            response = self._fetch_secret(f\"variables/{key}\")\n            return response.get(\"value\") if response else None\n        except Exception:\n            return None\n\n    def _fetch_secret(self, path: str) -&gt; Optional[dict]:\n        \"\"\"Fetch secret from external API.\"\"\"\n        import requests\n\n        response = requests.get(\n            f\"{self.api_url}/{path}\",\n            headers={\"Authorization\": f\"Bearer {self.api_key}\"},\n        )\n        if response.status_code == 200:\n            return response.json()\n        return None\n</code></pre>"},{"location":"modules/13-connections-secrets/#variables","title":"Variables","text":""},{"location":"modules/13-connections-secrets/#variable-basics","title":"Variable Basics","text":"<pre><code>from airflow.models import Variable\n\n# Get variable (returns None if not found)\nvalue = Variable.get(\"my_var\", default_var=None)\n\n# Get JSON variable\nconfig = Variable.get(\"my_config\", deserialize_json=True)\n\n# Set variable\nVariable.set(\"my_var\", \"new_value\")\n\n# Delete variable\nVariable.delete(\"my_var\")\n</code></pre>"},{"location":"modules/13-connections-secrets/#variables-in-templates","title":"Variables in Templates","text":"<pre><code># In Jinja templates\nbash_command = \"echo {{ var.value.my_var }}\"\n\n# JSON variable\nbash_command = \"echo {{ var.json.my_config.key }}\"\n</code></pre>"},{"location":"modules/13-connections-secrets/#variable-best-practices","title":"Variable Best Practices","text":"<pre><code># DON'T: Multiple Variable.get() calls\ndef bad_task():\n    var1 = Variable.get(\"var1\")  # Database query\n    var2 = Variable.get(\"var2\")  # Another query\n    var3 = Variable.get(\"var3\")  # Another query\n\n# DO: Single call with JSON\ndef good_task():\n    config = Variable.get(\"my_config\", deserialize_json=True)\n    # config = {\"var1\": \"...\", \"var2\": \"...\", \"var3\": \"...\"}\n</code></pre>"},{"location":"modules/13-connections-secrets/#sensitive-variables","title":"Sensitive Variables","text":"<pre><code># Mark variables as sensitive in the UI\n# They won't be shown in logs or rendered templates\n\n# Or use secrets backend for sensitive variables\n# Store in Vault: airflow/variables/api_key\napi_key = Variable.get(\"api_key\")  # Retrieved from secrets backend\n</code></pre>"},{"location":"modules/13-connections-secrets/#connection-encryption","title":"Connection Encryption","text":""},{"location":"modules/13-connections-secrets/#fernet-encryption","title":"Fernet Encryption","text":"<p>Airflow encrypts connection passwords using Fernet:</p> <pre><code># Generate Fernet key\npython -c \"from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())\"\n\n# Set in environment\nexport AIRFLOW__CORE__FERNET_KEY=\"your-generated-key\"\n</code></pre>"},{"location":"modules/13-connections-secrets/#key-rotation","title":"Key Rotation","text":"<pre><code>from airflow.models import Connection\nfrom airflow.utils.session import create_session\n\ndef rotate_fernet_key(old_key: str, new_key: str):\n    \"\"\"Rotate Fernet encryption key.\"\"\"\n    from cryptography.fernet import Fernet, MultiFernet\n\n    # Create MultiFernet with new key first\n    f = MultiFernet([Fernet(new_key), Fernet(old_key)])\n\n    with create_session() as session:\n        connections = session.query(Connection).all()\n        for conn in connections:\n            if conn.password:\n                # Decrypt with old key, encrypt with new\n                decrypted = f.decrypt(conn.password.encode())\n                new_fernet = Fernet(new_key)\n                conn.password = new_fernet.encrypt(decrypted).decode()\n        session.commit()\n</code></pre>"},{"location":"modules/13-connections-secrets/#best-practices","title":"Best Practices","text":""},{"location":"modules/13-connections-secrets/#connection-naming","title":"Connection Naming","text":"<pre><code># Use descriptive, hierarchical naming\ngood_names = [\n    \"prod_postgres_main\",\n    \"dev_mysql_analytics\",\n    \"aws_s3_data_lake\",\n    \"api_payment_gateway\",\n]\n\n# Avoid ambiguous names\nbad_names = [\n    \"db1\",\n    \"conn\",\n    \"my_connection\",\n]\n</code></pre>"},{"location":"modules/13-connections-secrets/#security-guidelines","title":"Security Guidelines","text":"<ol> <li>Never hardcode credentials in DAG files</li> <li>Use secrets backends in production</li> <li>Rotate credentials regularly</li> <li>Audit connection access via logs</li> <li>Limit connection permissions per role</li> <li>Encrypt at rest with Fernet keys</li> </ol>"},{"location":"modules/13-connections-secrets/#testing-connections","title":"Testing Connections","text":"<pre><code>from airflow.hooks.base import BaseHook\n\ndef test_connection(conn_id: str) -&gt; bool:\n    \"\"\"Test if a connection works.\"\"\"\n    try:\n        conn = BaseHook.get_connection(conn_id)\n        # Try to use the connection\n        hook = conn.get_hook()\n        hook.test_connection()\n        return True\n    except Exception as e:\n        print(f\"Connection failed: {e}\")\n        return False\n</code></pre>"},{"location":"modules/13-connections-secrets/#exercises","title":"Exercises","text":""},{"location":"modules/13-connections-secrets/#exercise-131-connection-types","title":"Exercise 13.1: Connection Types","text":"<p>Configure different connection types including database, cloud, and API connections.</p> <p>Start Exercise 13.1 \u2192</p>"},{"location":"modules/13-connections-secrets/#exercise-132-secrets-backends","title":"Exercise 13.2: Secrets Backends","text":"<p>Implement a secrets backend for secure credential management.</p> <p>Start Exercise 13.2 \u2192</p>"},{"location":"modules/13-connections-secrets/#exercise-133-variable-patterns","title":"Exercise 13.3: Variable Patterns","text":"<p>Implement secure variable patterns for configuration management.</p> <p>Start Exercise 13.3 \u2192</p>"},{"location":"modules/13-connections-secrets/#key-takeaways","title":"Key Takeaways","text":"<ol> <li>Connections store credentials for external systems</li> <li>URI format provides portable connection definitions</li> <li>Secrets backends integrate with enterprise secret managers</li> <li>Variables store non-sensitive configuration</li> <li>Fernet encryption protects stored passwords</li> <li>Environment variables enable secure CI/CD deployment</li> </ol>"},{"location":"modules/13-connections-secrets/#next-steps","title":"Next Steps","text":"<p>Continue to Module 14: Resource Management to learn about pools, priorities, and concurrency management.</p>"},{"location":"modules/13-connections-secrets/#additional-resources","title":"Additional Resources","text":"<ul> <li>Airflow Connections Documentation</li> <li>Secrets Backend Configuration</li> <li>Managing Variables</li> </ul> <p>_[DAG]: Directed Acyclic Graph - Airflow's core workflow abstraction _[XCom]: Cross-Communication - Airflow's mechanism for task-to-task data passing _[Executor]: The component that runs tasks (Local, Celery, Kubernetes, etc.) _[Scheduler]: The component that triggers DAGs and determines task execution order _[Webserver]: The component that serves the Airflow UI _[Provider]: An Airflow package that integrates with external systems _[TaskFlow]: The modern Python-native API for defining Airflow tasks using decorators _[Asset]: A logical representation of data that DAGs can produce or consume _[SDK]: Software Development Kit - the airflow.sdk module in Airflow 3.x _[K8s]: Kubernetes - container orchestration platform _[uv]: A fast Python package manager by Astral _[ruff]: A fast Python linter and formatter by Astral _[CI]: Continuous Integration _[CD]: Continuous Deployment *[PR]: Pull Request</p>"},{"location":"modules/13-connections-secrets/exercises/exercise_13_1_connection_types/","title":"Exercise 13.1: Connection Types","text":""},{"location":"modules/13-connections-secrets/exercises/exercise_13_1_connection_types/#objective","title":"Objective","text":"<p>Learn to configure and manage different connection types in Airflow including database, cloud, and API connections.</p>"},{"location":"modules/13-connections-secrets/exercises/exercise_13_1_connection_types/#background","title":"Background","text":"<p>Airflow connections provide a standardized way to store and retrieve credentials. Understanding connection types and their configuration is essential for integrating with external systems.</p>"},{"location":"modules/13-connections-secrets/exercises/exercise_13_1_connection_types/#requirements","title":"Requirements","text":""},{"location":"modules/13-connections-secrets/exercises/exercise_13_1_connection_types/#part-1-connection-manager-class","title":"Part 1: Connection Manager Class","text":"<p>Create a utility class for managing connections:</p> <pre><code>class ConnectionManager:\n    def create_connection(self, conn_id: str, conn_type: str, **kwargs) -&gt; bool:\n        \"\"\"Create a new connection.\"\"\"\n        pass\n\n    def update_connection(self, conn_id: str, **kwargs) -&gt; bool:\n        \"\"\"Update existing connection properties.\"\"\"\n        pass\n\n    def delete_connection(self, conn_id: str) -&gt; bool:\n        \"\"\"Delete a connection.\"\"\"\n        pass\n\n    def test_connection(self, conn_id: str) -&gt; dict:\n        \"\"\"Test if a connection works.\"\"\"\n        pass\n</code></pre>"},{"location":"modules/13-connections-secrets/exercises/exercise_13_1_connection_types/#part-2-connection-factories","title":"Part 2: Connection Factories","text":"<p>Create factory functions for common connection types:</p> <pre><code>def create_postgres_connection(\n    conn_id: str,\n    host: str,\n    database: str,\n    user: str,\n    password: str,\n    port: int = 5432,\n    ssl_mode: str = None,\n) -&gt; Connection:\n    \"\"\"Create a PostgreSQL connection.\"\"\"\n    pass\n\ndef create_aws_connection(\n    conn_id: str,\n    aws_access_key_id: str,\n    aws_secret_access_key: str,\n    region_name: str = \"us-east-1\",\n    role_arn: str = None,\n) -&gt; Connection:\n    \"\"\"Create an AWS connection.\"\"\"\n    pass\n\ndef create_http_connection(\n    conn_id: str,\n    host: str,\n    headers: dict = None,\n    auth_type: str = None,\n    token: str = None,\n) -&gt; Connection:\n    \"\"\"Create an HTTP/API connection.\"\"\"\n    pass\n</code></pre>"},{"location":"modules/13-connections-secrets/exercises/exercise_13_1_connection_types/#part-3-connection-uri-builder","title":"Part 3: Connection URI Builder","text":"<p>Build connections from URI strings:</p> <pre><code>class ConnectionURIBuilder:\n    def from_uri(self, uri: str) -&gt; Connection:\n        \"\"\"Parse URI and create Connection object.\"\"\"\n        pass\n\n    def to_uri(self, connection: Connection) -&gt; str:\n        \"\"\"Convert Connection object to URI string.\"\"\"\n        pass\n\n    def validate_uri(self, uri: str) -&gt; tuple[bool, list]:\n        \"\"\"Validate URI format and return (valid, errors).\"\"\"\n        pass\n</code></pre>"},{"location":"modules/13-connections-secrets/exercises/exercise_13_1_connection_types/#starter-code","title":"Starter Code","text":"<p>See <code>exercise_13_1_connection_types_starter.py</code></p>"},{"location":"modules/13-connections-secrets/exercises/exercise_13_1_connection_types/#hints","title":"Hints","text":"Hint 1: Connection creation <pre><code>from airflow.models import Connection\nfrom airflow.utils.session import create_session\n\ndef create_connection(conn_id, conn_type, **kwargs):\n    conn = Connection(\n        conn_id=conn_id,\n        conn_type=conn_type,\n        host=kwargs.get(\"host\"),\n        login=kwargs.get(\"login\"),\n        password=kwargs.get(\"password\"),\n        port=kwargs.get(\"port\"),\n        schema=kwargs.get(\"schema\"),\n        extra=json.dumps(kwargs.get(\"extra\", {})),\n    )\n    with create_session() as session:\n        session.add(conn)\n        session.commit()\n    return True\n</code></pre> Hint 2: URI parsing <pre><code>from urllib.parse import urlparse, parse_qs, unquote\n\ndef parse_connection_uri(uri):\n    parsed = urlparse(uri)\n    return {\n        \"conn_type\": parsed.scheme,\n        \"host\": parsed.hostname,\n        \"port\": parsed.port,\n        \"login\": unquote(parsed.username) if parsed.username else None,\n        \"password\": unquote(parsed.password) if parsed.password else None,\n        \"schema\": parsed.path.lstrip(\"/\") if parsed.path else None,\n        \"extra\": parse_qs(parsed.query),\n    }\n</code></pre>"},{"location":"modules/13-connections-secrets/exercises/exercise_13_1_connection_types/#success-criteria","title":"Success Criteria","text":"<ul> <li> ConnectionManager creates connections</li> <li> ConnectionManager updates connections</li> <li> ConnectionManager tests connections</li> <li> Factory functions work for all types</li> <li> URI builder parses and generates URIs</li> <li> Error handling is comprehensive</li> </ul> <p>Next: Exercise 13.2: Secrets Backends \u2192</p> <p>_[DAG]: Directed Acyclic Graph - Airflow's core workflow abstraction _[XCom]: Cross-Communication - Airflow's mechanism for task-to-task data passing _[Executor]: The component that runs tasks (Local, Celery, Kubernetes, etc.) _[Scheduler]: The component that triggers DAGs and determines task execution order _[Webserver]: The component that serves the Airflow UI _[Provider]: An Airflow package that integrates with external systems _[TaskFlow]: The modern Python-native API for defining Airflow tasks using decorators _[Asset]: A logical representation of data that DAGs can produce or consume _[SDK]: Software Development Kit - the airflow.sdk module in Airflow 3.x _[K8s]: Kubernetes - container orchestration platform _[uv]: A fast Python package manager by Astral _[ruff]: A fast Python linter and formatter by Astral _[CI]: Continuous Integration _[CD]: Continuous Deployment *[PR]: Pull Request</p>"},{"location":"modules/13-connections-secrets/exercises/exercise_13_2_secrets_backends/","title":"Exercise 13.2: Secrets Backends","text":""},{"location":"modules/13-connections-secrets/exercises/exercise_13_2_secrets_backends/#objective","title":"Objective","text":"<p>Implement and configure secrets backends for secure credential management in production Airflow environments.</p>"},{"location":"modules/13-connections-secrets/exercises/exercise_13_2_secrets_backends/#background","title":"Background","text":"<p>Secrets backends allow Airflow to retrieve credentials from external secret managers like AWS Secrets Manager, HashiCorp Vault, or Google Cloud Secret Manager instead of storing them in Airflow's database.</p>"},{"location":"modules/13-connections-secrets/exercises/exercise_13_2_secrets_backends/#requirements","title":"Requirements","text":""},{"location":"modules/13-connections-secrets/exercises/exercise_13_2_secrets_backends/#part-1-local-secrets-backend","title":"Part 1: Local Secrets Backend","text":"<p>Create a file-based secrets backend for local development:</p> <pre><code>class LocalFileSecretsBackend(BaseSecretsBackend):\n    \"\"\"\n    Secrets backend that reads from local JSON files.\n\n    File structure:\n    secrets/\n    \u251c\u2500\u2500 connections/\n    \u2502   \u2514\u2500\u2500 my_postgres.json\n    \u2514\u2500\u2500 variables/\n        \u2514\u2500\u2500 api_key.json\n    \"\"\"\n\n    def get_conn_value(self, conn_id: str) -&gt; Optional[str]:\n        \"\"\"Get connection URI from file.\"\"\"\n        pass\n\n    def get_variable(self, key: str) -&gt; Optional[str]:\n        \"\"\"Get variable value from file.\"\"\"\n        pass\n</code></pre>"},{"location":"modules/13-connections-secrets/exercises/exercise_13_2_secrets_backends/#part-2-environment-variable-backend","title":"Part 2: Environment Variable Backend","text":"<p>Create a backend that reads from prefixed environment variables:</p> <pre><code>class EnvVarSecretsBackend(BaseSecretsBackend):\n    \"\"\"\n    Backend using environment variables with custom prefixes.\n\n    Examples:\n    AIRFLOW_CONN_MY_POSTGRES=postgresql://...\n    AIRFLOW_VAR_API_KEY=secret123\n    \"\"\"\n\n    def get_conn_value(self, conn_id: str) -&gt; Optional[str]:\n        pass\n\n    def get_variable(self, key: str) -&gt; Optional[str]:\n        pass\n</code></pre>"},{"location":"modules/13-connections-secrets/exercises/exercise_13_2_secrets_backends/#part-3-caching-secrets-backend","title":"Part 3: Caching Secrets Backend","text":"<p>Create a wrapper that adds caching to any secrets backend:</p> <pre><code>class CachingSecretsBackend(BaseSecretsBackend):\n    \"\"\"\n    Wrapper that adds TTL-based caching to reduce\n    external API calls.\n    \"\"\"\n\n    def __init__(self, backend: BaseSecretsBackend, ttl: int = 300):\n        pass\n\n    def get_conn_value(self, conn_id: str) -&gt; Optional[str]:\n        \"\"\"Get with caching.\"\"\"\n        pass\n</code></pre>"},{"location":"modules/13-connections-secrets/exercises/exercise_13_2_secrets_backends/#part-4-multi-backend-support","title":"Part 4: Multi-Backend Support","text":"<p>Create a fallback chain of backends:</p> <pre><code>class ChainedSecretsBackend(BaseSecretsBackend):\n    \"\"\"\n    Try multiple backends in order until one succeeds.\n\n    Example:\n    1. Try Vault first\n    2. Fall back to AWS Secrets Manager\n    3. Fall back to environment variables\n    \"\"\"\n    pass\n</code></pre>"},{"location":"modules/13-connections-secrets/exercises/exercise_13_2_secrets_backends/#starter-code","title":"Starter Code","text":"<p>See <code>exercise_13_2_secrets_backends_starter.py</code></p>"},{"location":"modules/13-connections-secrets/exercises/exercise_13_2_secrets_backends/#hints","title":"Hints","text":"Hint 1: BaseSecretsBackend interface <pre><code>from airflow.secrets import BaseSecretsBackend\n\nclass MyBackend(BaseSecretsBackend):\n    def get_conn_value(self, conn_id: str) -&gt; Optional[str]:\n        \"\"\"Return connection as URI string or None.\"\"\"\n        pass\n\n    def get_variable(self, key: str) -&gt; Optional[str]:\n        \"\"\"Return variable value or None.\"\"\"\n        pass\n</code></pre> Hint 2: TTL caching <pre><code>from time import time\n\nclass Cache:\n    def __init__(self, ttl: int):\n        self.ttl = ttl\n        self._cache = {}\n\n    def get(self, key: str):\n        if key in self._cache:\n            value, timestamp = self._cache[key]\n            if time() - timestamp &lt; self.ttl:\n                return value\n            del self._cache[key]\n        return None\n\n    def set(self, key: str, value: str):\n        self._cache[key] = (value, time())\n</code></pre>"},{"location":"modules/13-connections-secrets/exercises/exercise_13_2_secrets_backends/#success-criteria","title":"Success Criteria","text":"<ul> <li> Local file backend reads JSON secrets</li> <li> Environment backend uses custom prefixes</li> <li> Caching reduces external calls</li> <li> Chain backend implements fallback</li> <li> All backends handle missing secrets gracefully</li> </ul> <p>Next: Exercise 13.3: Variable Patterns \u2192</p> <p>_[DAG]: Directed Acyclic Graph - Airflow's core workflow abstraction _[XCom]: Cross-Communication - Airflow's mechanism for task-to-task data passing _[Executor]: The component that runs tasks (Local, Celery, Kubernetes, etc.) _[Scheduler]: The component that triggers DAGs and determines task execution order _[Webserver]: The component that serves the Airflow UI _[Provider]: An Airflow package that integrates with external systems _[TaskFlow]: The modern Python-native API for defining Airflow tasks using decorators _[Asset]: A logical representation of data that DAGs can produce or consume _[SDK]: Software Development Kit - the airflow.sdk module in Airflow 3.x _[K8s]: Kubernetes - container orchestration platform _[uv]: A fast Python package manager by Astral _[ruff]: A fast Python linter and formatter by Astral _[CI]: Continuous Integration _[CD]: Continuous Deployment *[PR]: Pull Request</p>"},{"location":"modules/13-connections-secrets/exercises/exercise_13_3_variable_patterns/","title":"Exercise 13.3: Variable Patterns","text":""},{"location":"modules/13-connections-secrets/exercises/exercise_13_3_variable_patterns/#objective","title":"Objective","text":"<p>Implement secure and efficient variable management patterns for configuration management in Airflow.</p>"},{"location":"modules/13-connections-secrets/exercises/exercise_13_3_variable_patterns/#background","title":"Background","text":"<p>Variables in Airflow store configuration that can change between environments or deployments. Proper variable patterns prevent: - Excessive database queries - Security vulnerabilities - Configuration drift - Hard-to-maintain code</p>"},{"location":"modules/13-connections-secrets/exercises/exercise_13_3_variable_patterns/#requirements","title":"Requirements","text":""},{"location":"modules/13-connections-secrets/exercises/exercise_13_3_variable_patterns/#part-1-configuration-manager","title":"Part 1: Configuration Manager","text":"<p>Create a centralized configuration manager:</p> <pre><code>class ConfigManager:\n    \"\"\"\n    Centralized configuration management.\n\n    Features:\n    - Single point of access for all config\n    - Lazy loading with caching\n    - Environment-aware defaults\n    - Type conversion\n    \"\"\"\n\n    def get(self, key: str, default=None, as_type=None) -&gt; Any:\n        \"\"\"Get config value with type conversion.\"\"\"\n        pass\n\n    def get_json(self, key: str, default=None) -&gt; dict:\n        \"\"\"Get JSON configuration.\"\"\"\n        pass\n\n    def refresh(self):\n        \"\"\"Refresh cached values.\"\"\"\n        pass\n</code></pre>"},{"location":"modules/13-connections-secrets/exercises/exercise_13_3_variable_patterns/#part-2-environment-aware-config","title":"Part 2: Environment-Aware Config","text":"<p>Create environment-specific configuration handling:</p> <pre><code>class EnvironmentConfig:\n    \"\"\"\n    Environment-aware configuration.\n\n    Supports:\n    - Environment prefixes (DEV_, PROD_, etc.)\n    - Default fallbacks\n    - Environment inheritance\n    \"\"\"\n\n    def __init__(self, environment: str = None):\n        pass\n\n    def get(self, key: str, default=None) -&gt; Any:\n        \"\"\"Get value for current environment.\"\"\"\n        pass\n</code></pre>"},{"location":"modules/13-connections-secrets/exercises/exercise_13_3_variable_patterns/#part-3-secure-variable-access","title":"Part 3: Secure Variable Access","text":"<p>Implement secure variable patterns:</p> <pre><code>class SecureVariables:\n    \"\"\"\n    Secure variable access with audit logging.\n\n    Features:\n    - Access logging for sensitive variables\n    - Masking in logs\n    - Rate limiting\n    \"\"\"\n\n    def get_sensitive(self, key: str) -&gt; str:\n        \"\"\"Get sensitive variable with audit log.\"\"\"\n        pass\n\n    def mask_value(self, value: str) -&gt; str:\n        \"\"\"Mask sensitive value for logging.\"\"\"\n        pass\n</code></pre>"},{"location":"modules/13-connections-secrets/exercises/exercise_13_3_variable_patterns/#part-4-variable-validation","title":"Part 4: Variable Validation","text":"<p>Create a validator for configuration:</p> <pre><code>class ConfigValidator:\n    \"\"\"\n    Validate configuration on startup.\n\n    Checks:\n    - Required variables exist\n    - Values match expected patterns\n    - No deprecated variables in use\n    \"\"\"\n\n    def validate_required(self, keys: list) -&gt; list:\n        \"\"\"Check all required variables exist.\"\"\"\n        pass\n\n    def validate_format(self, key: str, pattern: str) -&gt; bool:\n        \"\"\"Validate value matches regex pattern.\"\"\"\n        pass\n</code></pre>"},{"location":"modules/13-connections-secrets/exercises/exercise_13_3_variable_patterns/#starter-code","title":"Starter Code","text":"<p>See <code>exercise_13_3_variable_patterns_starter.py</code></p>"},{"location":"modules/13-connections-secrets/exercises/exercise_13_3_variable_patterns/#hints","title":"Hints","text":"Hint 1: Batch variable loading <pre><code># DON'T do this (N database queries)\ndef bad_get_config():\n    return {\n        \"key1\": Variable.get(\"key1\"),\n        \"key2\": Variable.get(\"key2\"),\n        \"key3\": Variable.get(\"key3\"),\n    }\n\n# DO this (1 database query)\ndef good_get_config():\n    config = Variable.get(\"app_config\", deserialize_json=True)\n    return config  # {\"key1\": \"...\", \"key2\": \"...\", \"key3\": \"...\"}\n</code></pre> Hint 2: Lazy loading <pre><code>class LazyConfig:\n    def __init__(self):\n        self._config = None\n\n    @property\n    def config(self):\n        if self._config is None:\n            self._config = Variable.get(\"config\", deserialize_json=True)\n        return self._config\n\n    def get(self, key):\n        return self.config.get(key)\n</code></pre>"},{"location":"modules/13-connections-secrets/exercises/exercise_13_3_variable_patterns/#success-criteria","title":"Success Criteria","text":"<ul> <li> ConfigManager provides centralized access</li> <li> Environment-aware config works correctly</li> <li> Sensitive access is logged</li> <li> Values are properly masked</li> <li> Validation catches missing variables</li> </ul> <p>Next: Module 14: Resource Management \u2192</p> <p>_[DAG]: Directed Acyclic Graph - Airflow's core workflow abstraction _[XCom]: Cross-Communication - Airflow's mechanism for task-to-task data passing _[Executor]: The component that runs tasks (Local, Celery, Kubernetes, etc.) _[Scheduler]: The component that triggers DAGs and determines task execution order _[Webserver]: The component that serves the Airflow UI _[Provider]: An Airflow package that integrates with external systems _[TaskFlow]: The modern Python-native API for defining Airflow tasks using decorators _[Asset]: A logical representation of data that DAGs can produce or consume _[SDK]: Software Development Kit - the airflow.sdk module in Airflow 3.x _[K8s]: Kubernetes - container orchestration platform _[uv]: A fast Python package manager by Astral _[ruff]: A fast Python linter and formatter by Astral _[CI]: Continuous Integration _[CD]: Continuous Deployment *[PR]: Pull Request</p>"},{"location":"modules/14-resource-management/","title":"Module 14: Resource Management","text":"<p>Master resource management in Airflow 3.x including pools, priorities, queues, and concurrency controls.</p>"},{"location":"modules/14-resource-management/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this module, you will: - Understand and configure Airflow pools - Implement priority weights for task scheduling - Manage concurrency at DAG and task levels - Use queues for worker distribution - Apply best practices for resource optimization</p>"},{"location":"modules/14-resource-management/#prerequisites","title":"Prerequisites","text":"<ul> <li>Module 01: Foundations</li> <li>Module 04: Scheduling</li> <li>Understanding of task execution</li> </ul>"},{"location":"modules/14-resource-management/#resource-management-overview","title":"Resource Management Overview","text":""},{"location":"modules/14-resource-management/#why-resource-management","title":"Why Resource Management?","text":"<pre><code>Without Resource Management:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 100 tasks all start simultaneously          \u2502\n\u2502 Database overwhelmed, API rate limited      \u2502\n\u2502 Memory exhausted, workers crash             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nWith Resource Management:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 10 tasks at a time (pool limit)             \u2502\n\u2502 Critical tasks run first (priority)         \u2502\n\u2502 Heavy tasks on dedicated workers (queues)   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"modules/14-resource-management/#resource-control-points","title":"Resource Control Points","text":"Control Scope Purpose Pools Task Limit concurrent access to shared resources Priority Weight Task Order task execution queue Concurrency DAG/Task Limit parallel execution Queues Worker Route tasks to specific workers"},{"location":"modules/14-resource-management/#pools","title":"Pools","text":""},{"location":"modules/14-resource-management/#what-are-pools","title":"What Are Pools?","text":"<p>Pools limit concurrent task execution for shared resources:</p> <pre><code>from airflow.sdk import dag, task\nfrom datetime import datetime\n\n@dag(\n    dag_id=\"pool_example\",\n    start_date=datetime(2024, 1, 1),\n    schedule=None,\n)\ndef pool_example():\n\n    @task(pool=\"database_connections\", pool_slots=1)\n    def query_database():\n        \"\"\"Uses 1 slot from the database_connections pool.\"\"\"\n        pass\n\n    @task(pool=\"database_connections\", pool_slots=2)\n    def heavy_etl():\n        \"\"\"Uses 2 slots from the database_connections pool.\"\"\"\n        pass\n</code></pre>"},{"location":"modules/14-resource-management/#pool-configuration","title":"Pool Configuration","text":"<pre><code># Create pools programmatically\nfrom airflow.models import Pool\nfrom airflow.utils.session import create_session\n\ndef create_pools():\n    pools = [\n        Pool(pool=\"database_connections\", slots=10, description=\"Database connection limit\"),\n        Pool(pool=\"api_calls\", slots=5, description=\"External API rate limit\"),\n        Pool(pool=\"memory_intensive\", slots=3, description=\"High memory tasks\"),\n    ]\n\n    with create_session() as session:\n        for pool in pools:\n            existing = session.query(Pool).filter_by(pool=pool.pool).first()\n            if not existing:\n                session.add(pool)\n        session.commit()\n</code></pre>"},{"location":"modules/14-resource-management/#pool-slots","title":"Pool Slots","text":"<pre><code>@task(pool=\"api_calls\", pool_slots=1)\ndef light_api_call():\n    \"\"\"Single API call - uses 1 slot.\"\"\"\n    pass\n\n@task(pool=\"api_calls\", pool_slots=3)\ndef batch_api_calls():\n    \"\"\"Batch of API calls - reserves 3 slots.\"\"\"\n    pass\n</code></pre>"},{"location":"modules/14-resource-management/#pool-monitoring","title":"Pool Monitoring","text":"<pre><code>from airflow.models import Pool\n\ndef get_pool_status(pool_name: str) -&gt; dict:\n    \"\"\"Get current pool utilization.\"\"\"\n    pool = Pool.get_pool(pool_name)\n    return {\n        \"name\": pool.pool,\n        \"total_slots\": pool.slots,\n        \"running_slots\": pool.running_slots(),\n        \"queued_slots\": pool.queued_slots(),\n        \"open_slots\": pool.open_slots(),\n    }\n</code></pre>"},{"location":"modules/14-resource-management/#common-pool-patterns","title":"Common Pool Patterns","text":""},{"location":"modules/14-resource-management/#database-connection-pool","title":"Database Connection Pool","text":"<pre><code># Limit database connections\n@task(pool=\"postgres_connections\", pool_slots=1)\ndef run_query(query: str):\n    \"\"\"Each query uses one connection.\"\"\"\n    from airflow.providers.postgres.hooks.postgres import PostgresHook\n    hook = PostgresHook()\n    return hook.get_records(query)\n</code></pre>"},{"location":"modules/14-resource-management/#api-rate-limiting-pool","title":"API Rate Limiting Pool","text":"<pre><code># Respect external API rate limits\n@task(pool=\"external_api\", pool_slots=1)\ndef call_api(endpoint: str):\n    \"\"\"Rate limited API calls.\"\"\"\n    import requests\n    return requests.get(f\"https://api.example.com/{endpoint}\").json()\n</code></pre>"},{"location":"modules/14-resource-management/#resource-intensive-tasks","title":"Resource-Intensive Tasks","text":"<pre><code># Limit memory-heavy tasks\n@task(pool=\"high_memory\", pool_slots=2)\ndef process_large_file(file_path: str):\n    \"\"\"Memory intensive - uses 2 slots.\"\"\"\n    import pandas as pd\n    df = pd.read_csv(file_path)\n    return df.describe().to_dict()\n</code></pre>"},{"location":"modules/14-resource-management/#priority-weight","title":"Priority Weight","text":""},{"location":"modules/14-resource-management/#task-priority","title":"Task Priority","text":"<p>Higher priority weight = runs sooner:</p> <pre><code>@task(priority_weight=10)\ndef critical_task():\n    \"\"\"Runs before lower priority tasks.\"\"\"\n    pass\n\n@task(priority_weight=1)\ndef normal_task():\n    \"\"\"Standard priority.\"\"\"\n    pass\n\n@task(priority_weight=0)\ndef background_task():\n    \"\"\"Runs last.\"\"\"\n    pass\n</code></pre>"},{"location":"modules/14-resource-management/#priority-weight-rules","title":"Priority Weight Rules","text":"<pre><code>from airflow.utils.weight_rule import WeightRule\n\n@dag(\n    dag_id=\"priority_example\",\n    start_date=datetime(2024, 1, 1),\n    schedule=None,\n)\ndef priority_example():\n\n    # Downstream: Priority = sum of downstream task weights\n    @task(priority_weight=5, weight_rule=WeightRule.DOWNSTREAM)\n    def upstream_task():\n        pass\n\n    # Upstream: Priority = sum of upstream task weights\n    @task(priority_weight=3, weight_rule=WeightRule.UPSTREAM)\n    def downstream_task():\n        pass\n\n    # Absolute: Use exact weight specified\n    @task(priority_weight=10, weight_rule=WeightRule.ABSOLUTE)\n    def fixed_priority_task():\n        pass\n</code></pre>"},{"location":"modules/14-resource-management/#priority-calculation-example","title":"Priority Calculation Example","text":"<pre><code>DAG Structure:\n    A (weight=1)\n    \u251c\u2500\u2500 B (weight=2)\n    \u2502   \u2514\u2500\u2500 D (weight=1)\n    \u2514\u2500\u2500 C (weight=3)\n        \u2514\u2500\u2500 D (weight=1)\n\nWith DOWNSTREAM rule:\n- A: 1 + (2+1) + (3+1) = 8\n- B: 2 + 1 = 3\n- C: 3 + 1 = 4\n- D: 1\n\nExecution order: A \u2192 C \u2192 B \u2192 D\n</code></pre>"},{"location":"modules/14-resource-management/#concurrency-controls","title":"Concurrency Controls","text":""},{"location":"modules/14-resource-management/#dag-concurrency","title":"DAG Concurrency","text":"<pre><code>@dag(\n    dag_id=\"concurrency_example\",\n    start_date=datetime(2024, 1, 1),\n    schedule=\"@hourly\",\n    max_active_runs=3,  # Max 3 DAG runs at once\n)\ndef concurrency_example():\n\n    @task(max_active_tis_per_dag=5)  # Max 5 instances across all runs\n    def limited_task():\n        pass\n</code></pre>"},{"location":"modules/14-resource-management/#task-concurrency","title":"Task Concurrency","text":"<pre><code>@task(\n    max_active_tis_per_dag=10,     # Max 10 across all DAG runs\n    max_active_tis_per_dagrun=2,  # Max 2 per single DAG run\n)\ndef controlled_concurrency_task():\n    \"\"\"Carefully controlled concurrency.\"\"\"\n    pass\n</code></pre>"},{"location":"modules/14-resource-management/#global-concurrency-settings","title":"Global Concurrency Settings","text":"<pre><code># airflow.cfg\n[core]\n# Maximum active tasks across all DAGs\nparallelism = 32\n\n# Maximum active tasks per DAG\nmax_active_tasks_per_dag = 16\n\n# Maximum active DAG runs per DAG\nmax_active_runs_per_dag = 16\n</code></pre>"},{"location":"modules/14-resource-management/#dynamic-concurrency","title":"Dynamic Concurrency","text":"<pre><code>from airflow.models import Variable\n\ndef get_dynamic_concurrency():\n    \"\"\"Get concurrency based on current load.\"\"\"\n    base = int(Variable.get(\"base_concurrency\", 5))\n    load = float(Variable.get(\"current_load\", 0.5))\n\n    # Reduce concurrency under high load\n    if load &gt; 0.8:\n        return max(1, base // 2)\n    return base\n</code></pre>"},{"location":"modules/14-resource-management/#queues","title":"Queues","text":""},{"location":"modules/14-resource-management/#worker-queues","title":"Worker Queues","text":"<p>Route tasks to specific Celery workers:</p> <pre><code>@task(queue=\"default\")\ndef standard_task():\n    \"\"\"Runs on default workers.\"\"\"\n    pass\n\n@task(queue=\"high_memory\")\ndef memory_task():\n    \"\"\"Runs on high-memory workers.\"\"\"\n    pass\n\n@task(queue=\"gpu\")\ndef ml_training():\n    \"\"\"Runs on GPU workers.\"\"\"\n    pass\n</code></pre>"},{"location":"modules/14-resource-management/#worker-configuration","title":"Worker Configuration","text":"<pre><code># Start worker for specific queue\nairflow celery worker -q default,high_priority\n\n# Start specialized worker\nairflow celery worker -q gpu --concurrency 2\n\n# Start high-memory worker\nairflow celery worker -q high_memory --concurrency 4\n</code></pre>"},{"location":"modules/14-resource-management/#queue-based-scaling","title":"Queue-Based Scaling","text":"<pre><code># docker-compose.yml\nservices:\n  worker-default:\n    command: airflow celery worker -q default\n    deploy:\n      replicas: 4\n\n  worker-heavy:\n    command: airflow celery worker -q heavy\n    deploy:\n      resources:\n        limits:\n          memory: 16G\n      replicas: 2\n\n  worker-gpu:\n    command: airflow celery worker -q gpu\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - capabilities: [gpu]\n      replicas: 1\n</code></pre>"},{"location":"modules/14-resource-management/#resource-management-patterns","title":"Resource Management Patterns","text":""},{"location":"modules/14-resource-management/#pattern-1-tiered-priority","title":"Pattern 1: Tiered Priority","text":"<pre><code>class Priority:\n    CRITICAL = 100\n    HIGH = 75\n    NORMAL = 50\n    LOW = 25\n    BACKGROUND = 1\n\n@task(priority_weight=Priority.CRITICAL)\ndef alert_check():\n    \"\"\"Must run immediately.\"\"\"\n    pass\n\n@task(priority_weight=Priority.LOW)\ndef report_generation():\n    \"\"\"Can wait.\"\"\"\n    pass\n</code></pre>"},{"location":"modules/14-resource-management/#pattern-2-resource-pools-by-tier","title":"Pattern 2: Resource Pools by Tier","text":"<pre><code>POOLS = {\n    \"tier1_critical\": {\"slots\": 20, \"description\": \"Production critical\"},\n    \"tier2_standard\": {\"slots\": 10, \"description\": \"Standard workloads\"},\n    \"tier3_batch\": {\"slots\": 5, \"description\": \"Batch processing\"},\n}\n\n@task(pool=\"tier1_critical\", priority_weight=100)\ndef production_etl():\n    pass\n\n@task(pool=\"tier3_batch\", priority_weight=10)\ndef historical_backfill():\n    pass\n</code></pre>"},{"location":"modules/14-resource-management/#pattern-3-hybrid-pool-queue","title":"Pattern 3: Hybrid Pool + Queue","text":"<pre><code>@task(\n    pool=\"database_connections\",\n    pool_slots=2,\n    queue=\"high_memory\",\n    priority_weight=50,\n)\ndef complex_etl():\n    \"\"\"\n    - Uses 2 database connection slots\n    - Runs on high-memory workers\n    - Medium priority\n    \"\"\"\n    pass\n</code></pre>"},{"location":"modules/14-resource-management/#pattern-4-sla-based-prioritization","title":"Pattern 4: SLA-Based Prioritization","text":"<pre><code>from datetime import timedelta\n\n@task(\n    priority_weight=100,\n    sla=timedelta(minutes=30),\n)\ndef sla_critical_task():\n    \"\"\"Must complete within 30 minutes.\"\"\"\n    pass\n\ndef sla_miss_callback(context):\n    \"\"\"Alert when SLA is missed.\"\"\"\n    task = context['task_instance']\n    # Send alert\n    send_alert(f\"SLA missed for {task.task_id}\")\n</code></pre>"},{"location":"modules/14-resource-management/#best-practices","title":"Best Practices","text":""},{"location":"modules/14-resource-management/#pool-design","title":"Pool Design","text":"<pre><code># Good: Specific, purpose-driven pools\nPOOLS = {\n    \"postgres_analytics\": 5,   # Analytics DB connections\n    \"postgres_production\": 10, # Production DB connections\n    \"s3_downloads\": 20,        # S3 download bandwidth\n    \"api_partner_x\": 3,        # Partner X rate limit\n}\n\n# Bad: Generic, overlapping pools\nBAD_POOLS = {\n    \"database\": 15,   # Which database?\n    \"external\": 10,   # Too vague\n    \"limited\": 5,     # Limited by what?\n}\n</code></pre>"},{"location":"modules/14-resource-management/#priority-strategy","title":"Priority Strategy","text":"<pre><code># Establish clear priority tiers\nPRIORITY_TIERS = {\n    # Tier 1: Business Critical (90-100)\n    \"revenue_impacting\": 100,\n    \"customer_facing\": 95,\n    \"sla_bound\": 90,\n\n    # Tier 2: Operational (60-80)\n    \"monitoring\": 80,\n    \"reporting\": 70,\n    \"analytics\": 60,\n\n    # Tier 3: Background (1-40)\n    \"archival\": 40,\n    \"cleanup\": 20,\n    \"experimental\": 1,\n}\n</code></pre>"},{"location":"modules/14-resource-management/#monitoring-resources","title":"Monitoring Resources","text":"<pre><code>from airflow.models import Pool, TaskInstance\nfrom airflow.utils.state import TaskInstanceState\n\ndef get_resource_utilization() -&gt; dict:\n    \"\"\"Get current resource utilization.\"\"\"\n    pools = Pool.get_pools()\n\n    utilization = {}\n    for pool in pools:\n        if pool.slots &gt; 0:\n            running = pool.running_slots()\n            utilization[pool.pool] = {\n                \"slots\": pool.slots,\n                \"running\": running,\n                \"utilization\": running / pool.slots * 100,\n            }\n\n    return utilization\n</code></pre>"},{"location":"modules/14-resource-management/#exercises","title":"Exercises","text":""},{"location":"modules/14-resource-management/#exercise-141-pool-configuration","title":"Exercise 14.1: Pool Configuration","text":"<p>Configure and manage pools for different resource types.</p> <p>Start Exercise 14.1 \u2192</p>"},{"location":"modules/14-resource-management/#exercise-142-priority-weights","title":"Exercise 14.2: Priority Weights","text":"<p>Implement priority-based task scheduling.</p> <p>Start Exercise 14.2 \u2192</p>"},{"location":"modules/14-resource-management/#exercise-143-concurrency-limits","title":"Exercise 14.3: Concurrency Limits","text":"<p>Manage concurrency at multiple levels.</p> <p>Start Exercise 14.3 \u2192</p>"},{"location":"modules/14-resource-management/#key-takeaways","title":"Key Takeaways","text":"<ol> <li>Pools prevent resource exhaustion by limiting concurrent access</li> <li>Priority weights ensure critical tasks run first</li> <li>Concurrency limits prevent system overload</li> <li>Queues route tasks to appropriate workers</li> <li>Combine controls for fine-grained resource management</li> </ol>"},{"location":"modules/14-resource-management/#next-steps","title":"Next Steps","text":"<p>You've completed all core modules! Consider: - Example DAGs for real-world patterns - Production Patterns for deployment - Advanced Topics for deeper exploration</p>"},{"location":"modules/14-resource-management/#additional-resources","title":"Additional Resources","text":"<ul> <li>Airflow Pools Documentation</li> <li>Priority Weights</li> <li>Executor Configuration</li> </ul> <p>_[DAG]: Directed Acyclic Graph - Airflow's core workflow abstraction _[XCom]: Cross-Communication - Airflow's mechanism for task-to-task data passing _[Executor]: The component that runs tasks (Local, Celery, Kubernetes, etc.) _[Scheduler]: The component that triggers DAGs and determines task execution order _[Webserver]: The component that serves the Airflow UI _[Provider]: An Airflow package that integrates with external systems _[TaskFlow]: The modern Python-native API for defining Airflow tasks using decorators _[Asset]: A logical representation of data that DAGs can produce or consume _[SDK]: Software Development Kit - the airflow.sdk module in Airflow 3.x _[K8s]: Kubernetes - container orchestration platform _[uv]: A fast Python package manager by Astral _[ruff]: A fast Python linter and formatter by Astral _[CI]: Continuous Integration _[CD]: Continuous Deployment *[PR]: Pull Request</p>"},{"location":"modules/14-resource-management/exercises/exercise_14_1_pool_configuration/","title":"Exercise 14.1: Pool Configuration","text":""},{"location":"modules/14-resource-management/exercises/exercise_14_1_pool_configuration/#objective","title":"Objective","text":"<p>Learn to configure and manage Airflow pools for controlling concurrent access to shared resources.</p>"},{"location":"modules/14-resource-management/exercises/exercise_14_1_pool_configuration/#background","title":"Background","text":"<p>Pools are essential for: - Limiting database connections - Respecting API rate limits - Managing memory-intensive tasks - Preventing resource exhaustion</p>"},{"location":"modules/14-resource-management/exercises/exercise_14_1_pool_configuration/#requirements","title":"Requirements","text":""},{"location":"modules/14-resource-management/exercises/exercise_14_1_pool_configuration/#part-1-pool-manager","title":"Part 1: Pool Manager","text":"<p>Create a pool management utility:</p> <pre><code>class PoolManager:\n    def create_pool(self, name: str, slots: int, description: str) -&gt; bool:\n        \"\"\"Create or update a pool.\"\"\"\n        pass\n\n    def delete_pool(self, name: str) -&gt; bool:\n        \"\"\"Delete a pool.\"\"\"\n        pass\n\n    def get_pool_status(self, name: str) -&gt; dict:\n        \"\"\"Get pool utilization status.\"\"\"\n        pass\n\n    def list_pools(self) -&gt; list:\n        \"\"\"List all pools with status.\"\"\"\n        pass\n</code></pre>"},{"location":"modules/14-resource-management/exercises/exercise_14_1_pool_configuration/#part-2-pool-aware-dag","title":"Part 2: Pool-Aware DAG","text":"<p>Create a DAG that uses pools effectively:</p> <pre><code>@dag(...)\ndef pool_aware_etl():\n    # Tasks using different pools\n    # - database_pool: for DB queries\n    # - api_pool: for external API calls\n    # - compute_pool: for heavy computation\n</code></pre>"},{"location":"modules/14-resource-management/exercises/exercise_14_1_pool_configuration/#part-3-dynamic-pool-sizing","title":"Part 3: Dynamic Pool Sizing","text":"<p>Implement dynamic pool slot allocation:</p> <pre><code>class DynamicPoolManager:\n    def adjust_pool_size(self, name: str, current_load: float) -&gt; int:\n        \"\"\"Adjust pool size based on system load.\"\"\"\n        pass\n\n    def scale_pool(self, name: str, factor: float) -&gt; int:\n        \"\"\"Scale pool by factor.\"\"\"\n        pass\n</code></pre>"},{"location":"modules/14-resource-management/exercises/exercise_14_1_pool_configuration/#starter-code","title":"Starter Code","text":"<p>See <code>exercise_14_1_pool_configuration_starter.py</code></p>"},{"location":"modules/14-resource-management/exercises/exercise_14_1_pool_configuration/#hints","title":"Hints","text":"Hint 1: Pool creation <pre><code>from airflow.models import Pool\nfrom airflow.utils.session import create_session\n\ndef create_pool(name, slots, description):\n    with create_session() as session:\n        pool = Pool(pool=name, slots=slots, description=description)\n        session.merge(pool)  # merge handles insert/update\n        session.commit()\n</code></pre> Hint 2: Pool slots in tasks <pre><code>@task(pool=\"my_pool\", pool_slots=2)\ndef heavy_task():\n    \"\"\"Uses 2 slots from the pool.\"\"\"\n    pass\n</code></pre>"},{"location":"modules/14-resource-management/exercises/exercise_14_1_pool_configuration/#success-criteria","title":"Success Criteria","text":"<ul> <li> Pool manager creates pools correctly</li> <li> Pool status reflects actual usage</li> <li> DAG tasks respect pool limits</li> <li> Dynamic sizing adjusts appropriately</li> </ul> <p>Next: Exercise 14.2: Priority Weights \u2192</p> <p>_[DAG]: Directed Acyclic Graph - Airflow's core workflow abstraction _[XCom]: Cross-Communication - Airflow's mechanism for task-to-task data passing _[Executor]: The component that runs tasks (Local, Celery, Kubernetes, etc.) _[Scheduler]: The component that triggers DAGs and determines task execution order _[Webserver]: The component that serves the Airflow UI _[Provider]: An Airflow package that integrates with external systems _[TaskFlow]: The modern Python-native API for defining Airflow tasks using decorators _[Asset]: A logical representation of data that DAGs can produce or consume _[SDK]: Software Development Kit - the airflow.sdk module in Airflow 3.x _[K8s]: Kubernetes - container orchestration platform _[uv]: A fast Python package manager by Astral _[ruff]: A fast Python linter and formatter by Astral _[CI]: Continuous Integration _[CD]: Continuous Deployment *[PR]: Pull Request</p>"},{"location":"modules/14-resource-management/exercises/exercise_14_2_priority_weights/","title":"Exercise 14.2: Priority Weights","text":""},{"location":"modules/14-resource-management/exercises/exercise_14_2_priority_weights/#objective","title":"Objective","text":"<p>Implement priority-based task scheduling to ensure critical tasks execute first.</p>"},{"location":"modules/14-resource-management/exercises/exercise_14_2_priority_weights/#background","title":"Background","text":"<p>Priority weights determine the order in which tasks are scheduled when resources are constrained: - Higher weight = higher priority - Critical tasks should run before background tasks - Weight rules determine how weights are calculated</p>"},{"location":"modules/14-resource-management/exercises/exercise_14_2_priority_weights/#requirements","title":"Requirements","text":""},{"location":"modules/14-resource-management/exercises/exercise_14_2_priority_weights/#part-1-priority-constants","title":"Part 1: Priority Constants","text":"<p>Create a priority tier system:</p> <pre><code>class Priority:\n    \"\"\"Priority weight constants for consistent task scheduling.\"\"\"\n\n    CRITICAL = 100\n    HIGH = 75\n    NORMAL = 50\n    LOW = 25\n    BACKGROUND = 1\n\n    @staticmethod\n    def get_tier(weight: int) -&gt; str:\n        \"\"\"Get tier name from weight.\"\"\"\n        pass\n</code></pre>"},{"location":"modules/14-resource-management/exercises/exercise_14_2_priority_weights/#part-2-priority-aware-dag","title":"Part 2: Priority-Aware DAG","text":"<p>Create a DAG demonstrating priority usage:</p> <pre><code>@dag(...)\ndef priority_demo():\n    # Critical: Alert checking\n    # High: Production ETL\n    # Normal: Reporting\n    # Low: Data cleanup\n    # Background: Archival\n</code></pre>"},{"location":"modules/14-resource-management/exercises/exercise_14_2_priority_weights/#part-3-priority-calculator","title":"Part 3: Priority Calculator","text":"<p>Implement weight rule calculations:</p> <pre><code>class PriorityCalculator:\n    def calculate_downstream(self, dag, task_id: str) -&gt; int:\n        \"\"\"Calculate weight based on downstream tasks.\"\"\"\n        pass\n\n    def calculate_upstream(self, dag, task_id: str) -&gt; int:\n        \"\"\"Calculate weight based on upstream tasks.\"\"\"\n        pass\n\n    def get_execution_order(self, dag) -&gt; list:\n        \"\"\"Get expected task execution order based on priority.\"\"\"\n        pass\n</code></pre>"},{"location":"modules/14-resource-management/exercises/exercise_14_2_priority_weights/#starter-code","title":"Starter Code","text":"<p>See <code>exercise_14_2_priority_weights_starter.py</code></p>"},{"location":"modules/14-resource-management/exercises/exercise_14_2_priority_weights/#hints","title":"Hints","text":"Hint 1: Weight rules <pre><code>from airflow.utils.weight_rule import WeightRule\n\n@task(\n    priority_weight=10,\n    weight_rule=WeightRule.DOWNSTREAM  # Sum of downstream weights\n)\ndef my_task():\n    pass\n</code></pre> Hint 2: Calculating effective priority <pre><code>def calculate_downstream_weight(task, visited=None):\n    visited = visited or set()\n    if task.task_id in visited:\n        return 0\n    visited.add(task.task_id)\n\n    weight = task.priority_weight\n    for downstream in task.downstream_list:\n        weight += calculate_downstream_weight(downstream, visited)\n    return weight\n</code></pre>"},{"location":"modules/14-resource-management/exercises/exercise_14_2_priority_weights/#success-criteria","title":"Success Criteria","text":"<ul> <li> Priority tiers are clearly defined</li> <li> DAG tasks use appropriate priorities</li> <li> Weight calculations are correct</li> <li> Execution order reflects priorities</li> </ul> <p>Next: Exercise 14.3: Concurrency Limits \u2192</p> <p>_[DAG]: Directed Acyclic Graph - Airflow's core workflow abstraction _[XCom]: Cross-Communication - Airflow's mechanism for task-to-task data passing _[Executor]: The component that runs tasks (Local, Celery, Kubernetes, etc.) _[Scheduler]: The component that triggers DAGs and determines task execution order _[Webserver]: The component that serves the Airflow UI _[Provider]: An Airflow package that integrates with external systems _[TaskFlow]: The modern Python-native API for defining Airflow tasks using decorators _[Asset]: A logical representation of data that DAGs can produce or consume _[SDK]: Software Development Kit - the airflow.sdk module in Airflow 3.x _[K8s]: Kubernetes - container orchestration platform _[uv]: A fast Python package manager by Astral _[ruff]: A fast Python linter and formatter by Astral _[CI]: Continuous Integration _[CD]: Continuous Deployment *[PR]: Pull Request</p>"},{"location":"modules/14-resource-management/exercises/exercise_14_3_concurrency_limits/","title":"Exercise 14.3: Concurrency Limits","text":""},{"location":"modules/14-resource-management/exercises/exercise_14_3_concurrency_limits/#objective","title":"Objective","text":"<p>Implement and manage concurrency controls at DAG and task levels to prevent system overload.</p>"},{"location":"modules/14-resource-management/exercises/exercise_14_3_concurrency_limits/#background","title":"Background","text":"<p>Concurrency limits prevent resource exhaustion: - DAG-level: Limit active DAG runs - Task-level: Limit active task instances - Global: System-wide parallelism</p>"},{"location":"modules/14-resource-management/exercises/exercise_14_3_concurrency_limits/#requirements","title":"Requirements","text":""},{"location":"modules/14-resource-management/exercises/exercise_14_3_concurrency_limits/#part-1-concurrency-configuration","title":"Part 1: Concurrency Configuration","text":"<p>Create a concurrency management system:</p> <pre><code>class ConcurrencyConfig:\n    def get_dag_concurrency(self, dag_id: str) -&gt; dict:\n        \"\"\"Get DAG concurrency settings.\"\"\"\n        pass\n\n    def set_dag_concurrency(self, dag_id: str, max_active_runs: int) -&gt; bool:\n        \"\"\"Update DAG concurrency.\"\"\"\n        pass\n\n    def get_system_concurrency(self) -&gt; dict:\n        \"\"\"Get global concurrency settings.\"\"\"\n        pass\n</code></pre>"},{"location":"modules/14-resource-management/exercises/exercise_14_3_concurrency_limits/#part-2-concurrency-controlled-dag","title":"Part 2: Concurrency-Controlled DAG","text":"<p>Create a DAG with various concurrency settings:</p> <pre><code>@dag(\n    max_active_runs=3,\n    max_active_tasks=10,\n    ...\n)\ndef controlled_pipeline():\n    # Tasks with max_active_tis_per_dag\n    # Tasks with max_active_tis_per_dagrun\n</code></pre>"},{"location":"modules/14-resource-management/exercises/exercise_14_3_concurrency_limits/#part-3-concurrency-monitor","title":"Part 3: Concurrency Monitor","text":"<p>Create a monitoring utility:</p> <pre><code>class ConcurrencyMonitor:\n    def get_running_dags(self) -&gt; dict:\n        \"\"\"Get currently running DAG runs.\"\"\"\n        pass\n\n    def get_running_tasks(self) -&gt; dict:\n        \"\"\"Get currently running task instances.\"\"\"\n        pass\n\n    def check_capacity(self) -&gt; dict:\n        \"\"\"Check available capacity.\"\"\"\n        pass\n</code></pre>"},{"location":"modules/14-resource-management/exercises/exercise_14_3_concurrency_limits/#starter-code","title":"Starter Code","text":"<p>See <code>exercise_14_3_concurrency_limits_starter.py</code></p>"},{"location":"modules/14-resource-management/exercises/exercise_14_3_concurrency_limits/#hints","title":"Hints","text":"Hint 1: DAG concurrency parameters <pre><code>@dag(\n    max_active_runs=3,        # Max 3 concurrent DAG runs\n    max_active_tasks=10,      # Max 10 tasks running at once\n)\ndef my_dag():\n    @task(\n        max_active_tis_per_dag=5,     # Max 5 across all runs\n        max_active_tis_per_dagrun=2,  # Max 2 per single run\n    )\n    def my_task():\n        pass\n</code></pre> Hint 2: Checking running instances <pre><code>from airflow.models import DagRun, TaskInstance\nfrom airflow.utils.state import DagRunState, TaskInstanceState\n\n# Count running DAG runs\nrunning_runs = DagRun.find(state=DagRunState.RUNNING)\n\n# Count running tasks\nrunning_tasks = TaskInstance.get_task_instances(\n    state=TaskInstanceState.RUNNING\n)\n</code></pre>"},{"location":"modules/14-resource-management/exercises/exercise_14_3_concurrency_limits/#success-criteria","title":"Success Criteria","text":"<ul> <li> Concurrency config is manageable</li> <li> DAG respects concurrency limits</li> <li> Monitor tracks running instances</li> <li> Capacity checks work correctly</li> </ul> <p>Congratulations! You've completed Module 14: Resource Management.</p> <p>_[DAG]: Directed Acyclic Graph - Airflow's core workflow abstraction _[XCom]: Cross-Communication - Airflow's mechanism for task-to-task data passing _[Executor]: The component that runs tasks (Local, Celery, Kubernetes, etc.) _[Scheduler]: The component that triggers DAGs and determines task execution order _[Webserver]: The component that serves the Airflow UI _[Provider]: An Airflow package that integrates with external systems _[TaskFlow]: The modern Python-native API for defining Airflow tasks using decorators _[Asset]: A logical representation of data that DAGs can produce or consume _[SDK]: Software Development Kit - the airflow.sdk module in Airflow 3.x _[K8s]: Kubernetes - container orchestration platform _[uv]: A fast Python package manager by Astral _[ruff]: A fast Python linter and formatter by Astral _[CI]: Continuous Integration _[CD]: Continuous Deployment *[PR]: Pull Request</p>"},{"location":"modules/15-ai-ml-orchestration/","title":"Module 15: AI/ML Orchestration with Airflow","text":"<p>Orchestrate modern AI and machine learning workflows using Apache Airflow 3's powerful scheduling, data-aware capabilities, and production-grade patterns.</p>"},{"location":"modules/15-ai-ml-orchestration/#learning-objectives","title":"Learning Objectives","text":"<p>By completing this module, you will be able to:</p> <ul> <li>Design and implement RAG (Retrieval-Augmented Generation) pipelines with Airflow</li> <li>Orchestrate multi-step LLM workflows with proper error handling and cost tracking</li> <li>Build data preparation pipelines with quality gates and human-in-the-loop patterns</li> <li>Apply rate limiting, caching, and retry strategies for LLM API calls</li> <li>Monitor and optimize AI/ML pipeline costs and performance</li> </ul>"},{"location":"modules/15-ai-ml-orchestration/#prerequisites","title":"Prerequisites","text":"<ul> <li>Completion of Modules 01-14 (especially Dynamic Tasks, Assets, and Production Patterns)</li> <li>Basic understanding of embeddings and vector databases</li> <li>Familiarity with LLM APIs (OpenAI, Anthropic, or similar)</li> <li>Python libraries: <code>openai</code>, <code>langchain</code>, or similar</li> </ul>"},{"location":"modules/15-ai-ml-orchestration/#why-airflow-for-aiml","title":"Why Airflow for AI/ML?","text":"<p>While specialized ML orchestration tools exist, Airflow excels for AI/ML workflows because:</p> Feature Airflow Strength Use Case Data-Aware Scheduling Assets trigger pipelines when data changes Retrain models on new data Dynamic Task Mapping Process variable-size batches efficiently Parallel embedding generation Production Maturity Battle-tested scheduling, monitoring, alerting Mission-critical ML pipelines Hybrid Workflows Mix ML tasks with data engineering End-to-end data + ML pipelines Kubernetes Integration Per-task resource allocation GPU pods for inference"},{"location":"modules/15-ai-ml-orchestration/#comparison-with-alternatives","title":"Comparison with Alternatives","text":"<pre><code>Airflow         \u2192 Best for: Hybrid data + ML, production scheduling, existing data infrastructure\nKubeflow        \u2192 Best for: Pure ML training, Kubernetes-native teams\nPrefect/Dagster \u2192 Best for: Python-first teams, simpler deployments\nMLflow          \u2192 Best for: Experiment tracking, model registry (complementary to Airflow)\n</code></pre>"},{"location":"modules/15-ai-ml-orchestration/#rag-pipeline-patterns","title":"RAG Pipeline Patterns","text":"<p>RAG (Retrieval-Augmented Generation) pipelines ingest documents, generate embeddings, and update vector stores for LLM retrieval.</p>"},{"location":"modules/15-ai-ml-orchestration/#architecture-overview","title":"Architecture Overview","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Document  \u2502\u2500\u2500\u2500\u2500\u25b6\u2502   Chunking  \u2502\u2500\u2500\u2500\u2500\u25b6\u2502  Embedding  \u2502\u2500\u2500\u2500\u2500\u25b6\u2502   Vector    \u2502\n\u2502   Source    \u2502     \u2502   Strategy  \u2502     \u2502  Generation \u2502     \u2502   Store     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n     S3/GCS              Semantic           OpenAI           Pinecone\n     Local               Recursive          Cohere           Weaviate\n     API                 Fixed-size         Local            Chroma\n</code></pre>"},{"location":"modules/15-ai-ml-orchestration/#document-ingestion-strategies","title":"Document Ingestion Strategies","text":"<pre><code>from airflow.sdk import task\n\n\n@task\ndef list_new_documents(source_path: str, last_processed: str) -&gt; list[dict]:\n    \"\"\"Identify documents that need processing (incremental updates).\"\"\"\n    # Compare modification times, checksums, or version markers\n    return [{\"path\": \"doc1.pdf\", \"checksum\": \"abc123\", \"modified\": \"2024-01-15\"}]\n\n\n@task\ndef load_document(doc_metadata: dict) -&gt; dict:\n    \"\"\"Load and parse a single document.\"\"\"\n    # Use appropriate loader: PDF, HTML, Markdown, etc.\n    return {\"content\": \"...\", \"metadata\": doc_metadata}\n</code></pre>"},{"location":"modules/15-ai-ml-orchestration/#chunking-approaches","title":"Chunking Approaches","text":"Strategy Description Best For Fixed-size Split by character/token count Simple documents, consistent structure Semantic Split by meaning boundaries Technical docs, articles Recursive Hierarchical splitting Code, nested structures Document-aware Preserve sections, headers Manuals, legal documents <pre><code>@task\ndef chunk_document(document: dict, strategy: str = \"semantic\") -&gt; list[dict]:\n    \"\"\"Split document into chunks for embedding.\"\"\"\n    # Typical chunk sizes: 500-2000 tokens with 10-20% overlap\n    chunks = semantic_chunker(document[\"content\"], max_tokens=1000, overlap=100)\n    return [{\"text\": chunk, \"metadata\": document[\"metadata\"]} for chunk in chunks]\n</code></pre>"},{"location":"modules/15-ai-ml-orchestration/#embedding-generation-with-batching","title":"Embedding Generation with Batching","text":"<pre><code>@task\ndef generate_embeddings(chunks: list[dict], batch_size: int = 100) -&gt; list[dict]:\n    \"\"\"Generate embeddings with batching and rate limiting.\"\"\"\n    embeddings = []\n    for batch in batched(chunks, batch_size):\n        # Respect rate limits, implement retry logic\n        batch_embeddings = embedding_model.embed_batch([c[\"text\"] for c in batch])\n        embeddings.extend(batch_embeddings)\n        time.sleep(0.1)  # Rate limiting\n    return embeddings\n</code></pre>"},{"location":"modules/15-ai-ml-orchestration/#vector-store-integration","title":"Vector Store Integration","text":"<p>Popular vector stores and their Airflow integration patterns:</p> <ul> <li>Pinecone: Managed service, excellent for production scale</li> <li>Weaviate: Self-hosted or cloud, GraphQL interface</li> <li>Chroma: Lightweight, perfect for development and small scale</li> <li>Qdrant: High-performance, good filtering capabilities</li> </ul>"},{"location":"modules/15-ai-ml-orchestration/#llm-api-orchestration","title":"LLM API Orchestration","text":"<p>Orchestrating LLM calls requires careful attention to rate limiting, cost tracking, and error handling.</p>"},{"location":"modules/15-ai-ml-orchestration/#rate-limiting-strategies","title":"Rate Limiting Strategies","text":"<pre><code>from airflow.sdk import task\n\n\n@task(retries=3, retry_delay=timedelta(seconds=30), retry_exponential_backoff=True)\ndef call_llm_with_retry(prompt: str, model: str = \"gpt-4\") -&gt; str:\n    \"\"\"LLM call with Airflow-native retry configuration.\"\"\"\n    response = openai.chat.completions.create(model=model, messages=[{\"role\": \"user\", \"content\": prompt}])\n    return response.choices[0].message.content\n</code></pre>"},{"location":"modules/15-ai-ml-orchestration/#cost-tracking-callbacks","title":"Cost Tracking Callbacks","text":"<pre><code>def track_llm_cost(context):\n    \"\"\"Callback to track LLM API costs per task.\"\"\"\n    task_instance = context[\"task_instance\"]\n\n    # Extract token usage from XCom or task output\n    usage = task_instance.xcom_pull(key=\"token_usage\")\n    if usage:\n        cost = calculate_cost(usage[\"prompt_tokens\"], usage[\"completion_tokens\"])\n\n        # Log to monitoring system\n        log_metric(\"llm_cost\", cost, tags={\"dag\": context[\"dag\"].dag_id})\n\n        # Alert if budget exceeded\n        if get_daily_spend() &gt; DAILY_BUDGET:\n            send_alert(\"LLM budget exceeded!\")\n\n\n@dag(on_success_callback=track_llm_cost)\ndef llm_pipeline(): ...\n</code></pre>"},{"location":"modules/15-ai-ml-orchestration/#multi-model-routing","title":"Multi-Model Routing","text":"<pre><code>@task\ndef route_to_model(query: str, complexity_score: float) -&gt; str:\n    \"\"\"Route queries to appropriate models based on complexity.\"\"\"\n    if complexity_score &lt; 0.3:\n        return \"gpt-3.5-turbo\"  # Fast, cheap for simple queries\n    elif complexity_score &lt; 0.7:\n        return \"gpt-4\"  # Balanced\n    else:\n        return \"gpt-4-turbo\"  # Complex reasoning\n</code></pre>"},{"location":"modules/15-ai-ml-orchestration/#prompt-versioning","title":"Prompt Versioning","text":"<pre><code>from airflow import Variable\n\n\n@task\ndef get_prompt_template(prompt_name: str, version: str = \"latest\") -&gt; str:\n    \"\"\"Retrieve versioned prompt templates.\"\"\"\n    prompts = Variable.get(\"prompt_templates\", deserialize_json=True)\n\n    if version == \"latest\":\n        return prompts[prompt_name][\"versions\"][-1][\"template\"]\n\n    return next(v[\"template\"] for v in prompts[prompt_name][\"versions\"] if v[\"version\"] == version)\n</code></pre>"},{"location":"modules/15-ai-ml-orchestration/#data-preparation-workflows","title":"Data Preparation Workflows","text":"<p>AI/ML models require high-quality, well-prepared data. Airflow excels at building robust data preparation pipelines.</p>"},{"location":"modules/15-ai-ml-orchestration/#quality-gates-pattern","title":"Quality Gates Pattern","text":"<pre><code>@task.branch\ndef quality_gate(data_stats: dict) -&gt; str:\n    \"\"\"Route based on data quality checks.\"\"\"\n    if data_stats[\"null_rate\"] &gt; 0.1:\n        return \"handle_missing_data\"\n    if data_stats[\"outlier_rate\"] &gt; 0.05:\n        return \"handle_outliers\"\n    return \"proceed_to_training\"\n</code></pre>"},{"location":"modules/15-ai-ml-orchestration/#human-in-the-loop","title":"Human-in-the-Loop","text":"<pre><code>from airflow.sdk import task\nfrom airflow.sensors.external_task import ExternalTaskSensor\n\n\n@task\ndef flag_for_review(low_confidence_items: list[dict]) -&gt; None:\n    \"\"\"Send items to human review queue.\"\"\"\n    for item in low_confidence_items:\n        review_queue.add(\n            item=item, reason=\"Low confidence classification\", deadline=datetime.now() + timedelta(hours=24)\n        )\n\n\n# Sensor waits for human review completion\nwait_for_review = ExternalTaskSensor(\n    task_id=\"wait_for_review\",\n    external_dag_id=\"human_review_dag\",\n    external_task_id=\"review_complete\",\n    timeout=86400,  # 24 hours\n)\n</code></pre>"},{"location":"modules/15-ai-ml-orchestration/#metadata-extraction-pipeline","title":"Metadata Extraction Pipeline","text":"<pre><code>@task\ndef extract_metadata(document: dict) -&gt; dict:\n    \"\"\"Extract structured metadata from documents.\"\"\"\n    return {\n        \"title\": extract_title(document),\n        \"author\": extract_author(document),\n        \"date\": extract_date(document),\n        \"entities\": extract_entities(document),\n        \"topics\": classify_topics(document),\n        \"language\": detect_language(document),\n        \"quality_score\": calculate_quality_score(document),\n    }\n</code></pre>"},{"location":"modules/15-ai-ml-orchestration/#best-practices","title":"Best Practices","text":""},{"location":"modules/15-ai-ml-orchestration/#idempotency-for-embeddings","title":"Idempotency for Embeddings","text":"<pre><code>@task\ndef upsert_embeddings(embeddings: list[dict], namespace: str) -&gt; dict:\n    \"\"\"Idempotent embedding upsert using content hashes.\"\"\"\n    results = {\"inserted\": 0, \"updated\": 0, \"skipped\": 0}\n\n    for emb in embeddings:\n        content_hash = hash_content(emb[\"text\"])\n        existing = vector_store.get(content_hash)\n\n        if existing and existing[\"embedding\"] == emb[\"embedding\"]:\n            results[\"skipped\"] += 1\n        else:\n            vector_store.upsert(id=content_hash, **emb)\n            results[\"updated\" if existing else \"inserted\"] += 1\n\n    return results\n</code></pre>"},{"location":"modules/15-ai-ml-orchestration/#caching-expensive-llm-calls","title":"Caching Expensive LLM Calls","text":"<pre><code>import hashlib\n\n\ndef get_cache_key(prompt: str, model: str) -&gt; str:\n    \"\"\"Generate deterministic cache key for LLM calls.\"\"\"\n    return hashlib.sha256(f\"{model}:{prompt}\".encode()).hexdigest()\n\n\n@task\ndef cached_llm_call(prompt: str, model: str) -&gt; str:\n    \"\"\"LLM call with persistent caching.\"\"\"\n    cache_key = get_cache_key(prompt, model)\n\n    # Check cache first\n    cached = cache.get(cache_key)\n    if cached:\n        return cached\n\n    # Make API call and cache result\n    result = call_llm(prompt, model)\n    cache.set(cache_key, result, ttl=86400)  # 24-hour TTL\n\n    return result\n</code></pre>"},{"location":"modules/15-ai-ml-orchestration/#observability-and-cost-monitoring","title":"Observability and Cost Monitoring","text":"<pre><code>@dag(\n    default_args={\n        \"on_failure_callback\": alert_on_failure,\n        \"on_success_callback\": track_metrics,\n    },\n    tags=[\"ai-ml\", \"rag\", \"production\"],\n)\ndef rag_pipeline():\n    \"\"\"RAG pipeline with comprehensive observability.\"\"\"\n    # Track document processing metrics\n    docs = process_documents()\n\n    # Track embedding costs and latency\n    embeddings = generate_embeddings(docs)\n\n    # Track vector store operations\n    upsert_results = upsert_to_vector_store(embeddings)\n\n    # Final cost summary\n    summarize_run_costs(docs, embeddings, upsert_results)\n</code></pre>"},{"location":"modules/15-ai-ml-orchestration/#exercises","title":"Exercises","text":"<p>Complete these exercises to master AI/ML orchestration with Airflow:</p>"},{"location":"modules/15-ai-ml-orchestration/#exercise-151-rag-ingestion-pipeline","title":"Exercise 15.1: RAG Ingestion Pipeline","text":"<p>Build a complete RAG pipeline that ingests documents, generates embeddings, and updates a vector store with incremental processing.</p> <p>Files:</p> <ul> <li><code>exercises/exercise_15_1_rag_pipeline.md</code></li> <li><code>exercises/exercise_15_1_rag_pipeline_starter.py</code></li> <li><code>solutions/solution_15_1_rag_pipeline.py</code></li> </ul>"},{"location":"modules/15-ai-ml-orchestration/#exercise-152-llm-chain-orchestration","title":"Exercise 15.2: LLM Chain Orchestration","text":"<p>Create a multi-step LLM workflow with rate limiting, cost tracking, and fallback model routing.</p> <p>Files:</p> <ul> <li><code>exercises/exercise_15_2_llm_chain.md</code></li> <li><code>exercises/exercise_15_2_llm_chain_starter.py</code></li> <li><code>solutions/solution_15_2_llm_chain.py</code></li> </ul>"},{"location":"modules/15-ai-ml-orchestration/#exercise-153-data-preparation-pipeline","title":"Exercise 15.3: Data Preparation Pipeline","text":"<p>Build a data preparation pipeline with quality gates, human-in-the-loop review, and metadata extraction.</p> <p>Files:</p> <ul> <li><code>exercises/exercise_15_3_data_prep.md</code></li> <li><code>exercises/exercise_15_3_data_prep_starter.py</code></li> <li><code>solutions/solution_15_3_data_prep.py</code></li> </ul>"},{"location":"modules/15-ai-ml-orchestration/#checkpoint","title":"Checkpoint","text":"<p>Before moving on, verify you can:</p> <ul> <li> Explain why Airflow is suitable for AI/ML orchestration</li> <li> Design a RAG pipeline with incremental document processing</li> <li> Implement rate limiting and retry strategies for LLM APIs</li> <li> Build cost tracking and budget alerting for LLM workflows</li> <li> Create data quality gates and human-in-the-loop patterns</li> <li> Apply caching strategies for expensive LLM operations</li> <li> Monitor AI/ML pipeline costs and performance</li> </ul>"},{"location":"modules/15-ai-ml-orchestration/#further-reading","title":"Further Reading","text":""},{"location":"modules/15-ai-ml-orchestration/#official-documentation","title":"Official Documentation","text":"<ul> <li>Apache Airflow TaskFlow API</li> <li>Dynamic Task Mapping</li> <li>Assets (Data-Aware Scheduling)</li> </ul>"},{"location":"modules/15-ai-ml-orchestration/#aiml-integration-resources","title":"AI/ML Integration Resources","text":"<ul> <li>LangChain Documentation</li> <li>OpenAI API Best Practices</li> <li>Pinecone Documentation</li> <li>Weaviate Documentation</li> </ul>"},{"location":"modules/15-ai-ml-orchestration/#community-resources","title":"Community Resources","text":"<ul> <li>Airflow for ML Pipelines</li> <li>Building RAG Applications</li> </ul> <p>_[DAG]: Directed Acyclic Graph - Airflow's core workflow abstraction _[XCom]: Cross-Communication - Airflow's mechanism for task-to-task data passing _[Executor]: The component that runs tasks (Local, Celery, Kubernetes, etc.) _[Scheduler]: The component that triggers DAGs and determines task execution order _[Webserver]: The component that serves the Airflow UI _[Provider]: An Airflow package that integrates with external systems _[TaskFlow]: The modern Python-native API for defining Airflow tasks using decorators _[Asset]: A logical representation of data that DAGs can produce or consume _[SDK]: Software Development Kit - the airflow.sdk module in Airflow 3.x _[K8s]: Kubernetes - container orchestration platform _[uv]: A fast Python package manager by Astral _[ruff]: A fast Python linter and formatter by Astral _[CI]: Continuous Integration _[CD]: Continuous Deployment *[PR]: Pull Request</p>"},{"location":"modules/15-ai-ml-orchestration/exercises/exercise_15_1_rag_pipeline/","title":"Exercise 15.1: RAG Ingestion Pipeline","text":"<p>Build a complete RAG (Retrieval-Augmented Generation) pipeline that ingests documents, generates embeddings, and updates a vector store with incremental processing.</p>"},{"location":"modules/15-ai-ml-orchestration/exercises/exercise_15_1_rag_pipeline/#learning-goals","title":"Learning Goals","text":"<ul> <li>Design an end-to-end document ingestion pipeline</li> <li>Implement incremental processing (only new/changed documents)</li> <li>Generate embeddings with proper batching and rate limiting</li> <li>Integrate with a vector store (Chroma for local development)</li> <li>Apply idempotent upsert patterns</li> </ul>"},{"location":"modules/15-ai-ml-orchestration/exercises/exercise_15_1_rag_pipeline/#scenario","title":"Scenario","text":"<p>You're building a knowledge base for a company's internal documentation. The pipeline should:</p> <ol> <li>Scan a source directory for new/modified documents</li> <li>Parse and chunk documents into embedding-ready segments</li> <li>Generate embeddings using OpenAI's API (or mock for testing)</li> <li>Store embeddings in a vector store with metadata</li> <li>Track processed documents to avoid reprocessing</li> </ol>"},{"location":"modules/15-ai-ml-orchestration/exercises/exercise_15_1_rag_pipeline/#requirements","title":"Requirements","text":""},{"location":"modules/15-ai-ml-orchestration/exercises/exercise_15_1_rag_pipeline/#task-1-document-discovery","title":"Task 1: Document Discovery","text":"<p>Create a task that identifies documents needing processing:</p> <ul> <li>Scan a source directory for PDF and Markdown files</li> <li>Track previously processed documents using Airflow Variables</li> <li>Return only new or modified documents (based on modification time or checksum)</li> </ul>"},{"location":"modules/15-ai-ml-orchestration/exercises/exercise_15_1_rag_pipeline/#task-2-document-processing","title":"Task 2: Document Processing","text":"<p>Create a dynamic task mapping that processes each document:</p> <ul> <li>Parse PDF files using <code>pypdf</code> or similar</li> <li>Parse Markdown files directly</li> <li>Split documents into chunks (500-1000 tokens, 100 token overlap)</li> <li>Preserve document metadata (filename, path, type)</li> </ul>"},{"location":"modules/15-ai-ml-orchestration/exercises/exercise_15_1_rag_pipeline/#task-3-embedding-generation","title":"Task 3: Embedding Generation","text":"<p>Create a task that generates embeddings with batching:</p> <ul> <li>Batch chunks to respect API rate limits (100 chunks per batch)</li> <li>Mock the embedding API for local testing</li> <li>Add rate limiting (0.1s delay between batches)</li> <li>Track token usage for cost monitoring</li> </ul>"},{"location":"modules/15-ai-ml-orchestration/exercises/exercise_15_1_rag_pipeline/#task-4-vector-store-upsert","title":"Task 4: Vector Store Upsert","text":"<p>Create an idempotent upsert task:</p> <ul> <li>Use content hash as document ID for deduplication</li> <li>Include metadata: source file, chunk index, timestamp</li> <li>Return statistics: inserted, updated, skipped counts</li> </ul>"},{"location":"modules/15-ai-ml-orchestration/exercises/exercise_15_1_rag_pipeline/#task-5-pipeline-summary","title":"Task 5: Pipeline Summary","text":"<p>Create a final task that summarizes the run:</p> <ul> <li>Total documents processed</li> <li>Total chunks generated</li> <li>Embedding costs (estimated)</li> <li>Any errors or warnings</li> </ul>"},{"location":"modules/15-ai-ml-orchestration/exercises/exercise_15_1_rag_pipeline/#success-criteria","title":"Success Criteria","text":"<ul> <li> Pipeline handles empty source directory gracefully</li> <li> Incremental processing works (re-running skips already processed docs)</li> <li> Embedding generation respects rate limits</li> <li> Vector store upserts are idempotent (same doc = same ID)</li> <li> Pipeline logs include cost/token tracking</li> <li> All tasks have proper error handling and retries</li> </ul>"},{"location":"modules/15-ai-ml-orchestration/exercises/exercise_15_1_rag_pipeline/#hints","title":"Hints","text":"Hint 1: Document Tracking  Use Airflow Variables to persist processing state:  <pre><code>from airflow import Variable\n\nprocessed_docs = Variable.get(\"rag_processed_docs\", default_var={}, deserialize_json=True)\n# After processing:\nVariable.set(\"rag_processed_docs\", processed_docs, serialize_json=True)\n</code></pre> Hint 2: Content Hashing  Create deterministic IDs for chunks:  <pre><code>import hashlib\n\n\ndef chunk_id(source_path: str, chunk_index: int, content: str) -&gt; str:\n    \"\"\"Generate stable ID for chunk.\"\"\"\n    data = f\"{source_path}:{chunk_index}:{content}\"\n    return hashlib.sha256(data.encode()).hexdigest()[:16]\n</code></pre> Hint 3: Dynamic Task Mapping  Use `expand()` for parallel document processing:  <pre><code>documents = discover_documents()\nchunks = process_document.expand(doc=documents)\nembeddings = generate_embeddings(chunks)\n</code></pre> Hint 4: Mock Embedding API  For local testing without API costs:  <pre><code>def mock_embedding(text: str) -&gt; list[float]:\n    \"\"\"Generate deterministic mock embedding.\"\"\"\n    import hashlib\n\n    hash_bytes = hashlib.sha256(text.encode()).digest()\n    return [b / 255.0 for b in hash_bytes[:384]]  # 384-dim like OpenAI small\n</code></pre>"},{"location":"modules/15-ai-ml-orchestration/exercises/exercise_15_1_rag_pipeline/#files","title":"Files","text":"<ul> <li>Starter: <code>exercise_15_1_rag_pipeline_starter.py</code></li> <li>Solution: <code>../solutions/solution_15_1_rag_pipeline.py</code></li> </ul>"},{"location":"modules/15-ai-ml-orchestration/exercises/exercise_15_1_rag_pipeline/#estimated-time","title":"Estimated Time","text":"<p>60-90 minutes</p>"},{"location":"modules/15-ai-ml-orchestration/exercises/exercise_15_1_rag_pipeline/#next-steps","title":"Next Steps","text":"<p>After completing this exercise:</p> <ol> <li>Test with real PDF documents in a <code>test_docs/</code> directory</li> <li>Try with actual OpenAI API (set <code>OPENAI_API_KEY</code>)</li> <li>Explore Pinecone or Weaviate for production vector stores</li> </ol> <p>_[DAG]: Directed Acyclic Graph - Airflow's core workflow abstraction _[XCom]: Cross-Communication - Airflow's mechanism for task-to-task data passing _[Executor]: The component that runs tasks (Local, Celery, Kubernetes, etc.) _[Scheduler]: The component that triggers DAGs and determines task execution order _[Webserver]: The component that serves the Airflow UI _[Provider]: An Airflow package that integrates with external systems _[TaskFlow]: The modern Python-native API for defining Airflow tasks using decorators _[Asset]: A logical representation of data that DAGs can produce or consume _[SDK]: Software Development Kit - the airflow.sdk module in Airflow 3.x _[K8s]: Kubernetes - container orchestration platform _[uv]: A fast Python package manager by Astral _[ruff]: A fast Python linter and formatter by Astral _[CI]: Continuous Integration _[CD]: Continuous Deployment *[PR]: Pull Request</p>"},{"location":"modules/15-ai-ml-orchestration/exercises/exercise_15_2_llm_chain/","title":"Exercise 15.2: LLM Chain Orchestration","text":"<p>Build a multi-step LLM workflow with rate limiting, cost tracking, fallback model routing, and comprehensive observability.</p>"},{"location":"modules/15-ai-ml-orchestration/exercises/exercise_15_2_llm_chain/#learning-goals","title":"Learning Goals","text":"<ul> <li>Design multi-step LLM processing chains</li> <li>Implement rate limiting and retry strategies for LLM APIs</li> <li>Track and budget LLM costs at the task level</li> <li>Build fallback model routing for reliability</li> <li>Apply observability patterns for LLM workflows</li> </ul>"},{"location":"modules/15-ai-ml-orchestration/exercises/exercise_15_2_llm_chain/#scenario","title":"Scenario","text":"<p>You're building a content analysis pipeline that processes articles through multiple LLM steps:</p> <ol> <li>Extract: Pull key entities and facts from the text</li> <li>Summarize: Create a concise summary</li> <li>Classify: Categorize the content into predefined topics</li> <li>Score: Rate the content quality and relevance</li> </ol> <p>Each step should handle rate limits, track costs, and fall back to cheaper models when appropriate.</p>"},{"location":"modules/15-ai-ml-orchestration/exercises/exercise_15_2_llm_chain/#requirements","title":"Requirements","text":""},{"location":"modules/15-ai-ml-orchestration/exercises/exercise_15_2_llm_chain/#task-1-llm-client-with-rate-limiting","title":"Task 1: LLM Client with Rate Limiting","text":"<p>Create a reusable LLM client task that:</p> <ul> <li>Implements token bucket rate limiting</li> <li>Tracks input/output tokens for cost calculation</li> <li>Supports configurable retry with exponential backoff</li> <li>Logs all API interactions for debugging</li> </ul>"},{"location":"modules/15-ai-ml-orchestration/exercises/exercise_15_2_llm_chain/#task-2-multi-step-chain","title":"Task 2: Multi-Step Chain","text":"<p>Create a chain of LLM tasks:</p> <ul> <li>extract_entities: Identify people, places, organizations, dates</li> <li>generate_summary: Create a 2-3 sentence summary</li> <li>classify_content: Assign category labels (tech, business, science, etc.)</li> <li>score_quality: Rate relevance and quality (1-10)</li> </ul> <p>Each task should pass its output to the next via XCom.</p>"},{"location":"modules/15-ai-ml-orchestration/exercises/exercise_15_2_llm_chain/#task-3-cost-tracking-callbacks","title":"Task 3: Cost Tracking Callbacks","text":"<p>Implement callbacks that:</p> <ul> <li>Track token usage per task</li> <li>Calculate cost using model-specific pricing</li> <li>Store cumulative costs in Airflow Variables</li> <li>Alert when daily budget threshold is reached</li> </ul>"},{"location":"modules/15-ai-ml-orchestration/exercises/exercise_15_2_llm_chain/#task-4-fallback-model-routing","title":"Task 4: Fallback Model Routing","text":"<p>Implement intelligent model selection:</p> <ul> <li>Route simple tasks to cheaper models (gpt-3.5-turbo)</li> <li>Use expensive models (gpt-4) only for complex analysis</li> <li>Fallback to backup model on primary model failure</li> <li>Track model usage distribution</li> </ul>"},{"location":"modules/15-ai-ml-orchestration/exercises/exercise_15_2_llm_chain/#task-5-pipeline-summary","title":"Task 5: Pipeline Summary","text":"<p>Create a final task that aggregates:</p> <ul> <li>Total processing time per step</li> <li>Token usage breakdown by step</li> <li>Total cost with per-step attribution</li> <li>Success/failure rates</li> <li>Model usage distribution</li> </ul>"},{"location":"modules/15-ai-ml-orchestration/exercises/exercise_15_2_llm_chain/#success-criteria","title":"Success Criteria","text":"<ul> <li> Rate limiting prevents API quota exhaustion</li> <li> Retry logic handles transient failures gracefully</li> <li> Cost tracking accurately reflects token usage</li> <li> Fallback routing maintains pipeline reliability</li> <li> All LLM interactions are logged for debugging</li> <li> Pipeline completes within configured time budget</li> </ul>"},{"location":"modules/15-ai-ml-orchestration/exercises/exercise_15_2_llm_chain/#hints","title":"Hints","text":"Hint 1: Token Bucket Rate Limiter <pre><code>import time\nfrom threading import Lock\n\n\nclass TokenBucketRateLimiter:\n    \"\"\"Simple token bucket rate limiter.\"\"\"\n\n    def __init__(self, tokens_per_second: float, max_tokens: int = 10):\n        self.tokens_per_second = tokens_per_second\n        self.max_tokens = max_tokens\n        self.tokens = max_tokens\n        self.last_update = time.time()\n        self.lock = Lock()\n\n    def acquire(self, tokens: int = 1) -&gt; float:\n        \"\"\"Acquire tokens, blocking if necessary. Returns wait time.\"\"\"\n        with self.lock:\n            self._refill()\n            if self.tokens &gt;= tokens:\n                self.tokens -= tokens\n                return 0.0\n            wait_time = (tokens - self.tokens) / self.tokens_per_second\n            time.sleep(wait_time)\n            self._refill()\n            self.tokens -= tokens\n            return wait_time\n\n    def _refill(self) -&gt; None:\n        now = time.time()\n        elapsed = now - self.last_update\n        self.tokens = min(self.max_tokens, self.tokens + elapsed * self.tokens_per_second)\n        self.last_update = now\n</code></pre> Hint 2: Cost Calculation <pre><code># OpenAI pricing (as of 2024)\nMODEL_PRICING = {\n    \"gpt-4-turbo\": {\"input\": 0.01, \"output\": 0.03},  # per 1K tokens\n    \"gpt-4\": {\"input\": 0.03, \"output\": 0.06},\n    \"gpt-3.5-turbo\": {\"input\": 0.0005, \"output\": 0.0015},\n}\n\n\ndef calculate_cost(model: str, input_tokens: int, output_tokens: int) -&gt; float:\n    \"\"\"Calculate cost in USD for an LLM call.\"\"\"\n    pricing = MODEL_PRICING.get(model, MODEL_PRICING[\"gpt-3.5-turbo\"])\n    input_cost = (input_tokens / 1000) * pricing[\"input\"]\n    output_cost = (output_tokens / 1000) * pricing[\"output\"]\n    return input_cost + output_cost\n</code></pre> Hint 3: Fallback Model Logic <pre><code>def select_model(task_complexity: str, primary_failed: bool = False) -&gt; str:\n    \"\"\"Select model based on task complexity and failure state.\"\"\"\n    if primary_failed:\n        # Fallback chain: gpt-4 -&gt; gpt-3.5-turbo -&gt; local\n        return \"gpt-3.5-turbo\"\n\n    complexity_map = {\n        \"simple\": \"gpt-3.5-turbo\",\n        \"moderate\": \"gpt-3.5-turbo\",\n        \"complex\": \"gpt-4\",\n        \"critical\": \"gpt-4-turbo\",\n    }\n    return complexity_map.get(task_complexity, \"gpt-3.5-turbo\")\n</code></pre> Hint 4: Task-Level Cost Tracking Callback <pre><code>def track_llm_cost(context):\n    \"\"\"On-success callback to track LLM costs.\"\"\"\n    ti = context[\"task_instance\"]\n    usage = ti.xcom_pull(key=\"token_usage\")\n\n    if usage:\n        # Update daily spend variable\n        daily_key = f\"llm_spend_{datetime.now().strftime('%Y-%m-%d')}\"\n        current_spend = Variable.get(daily_key, default_var=0.0, deserialize_json=True)\n        new_spend = current_spend + usage.get(\"cost\", 0)\n        Variable.set(daily_key, new_spend, serialize_json=True)\n\n        # Check budget\n        if new_spend &gt; DAILY_BUDGET:\n            # Trigger alert (webhook, email, etc.)\n            send_budget_alert(new_spend, DAILY_BUDGET)\n</code></pre>"},{"location":"modules/15-ai-ml-orchestration/exercises/exercise_15_2_llm_chain/#files","title":"Files","text":"<ul> <li>Starter: <code>exercise_15_2_llm_chain_starter.py</code></li> <li>Solution: <code>../solutions/solution_15_2_llm_chain.py</code></li> </ul>"},{"location":"modules/15-ai-ml-orchestration/exercises/exercise_15_2_llm_chain/#estimated-time","title":"Estimated Time","text":"<p>60-90 minutes</p>"},{"location":"modules/15-ai-ml-orchestration/exercises/exercise_15_2_llm_chain/#next-steps","title":"Next Steps","text":"<p>After completing this exercise:</p> <ol> <li>Integrate with a real LLM API (OpenAI, Anthropic)</li> <li>Add prompt versioning and A/B testing</li> <li>Implement caching for repeated queries</li> <li>Build a dashboard for cost monitoring</li> </ol> <p>_[DAG]: Directed Acyclic Graph - Airflow's core workflow abstraction _[XCom]: Cross-Communication - Airflow's mechanism for task-to-task data passing _[Executor]: The component that runs tasks (Local, Celery, Kubernetes, etc.) _[Scheduler]: The component that triggers DAGs and determines task execution order _[Webserver]: The component that serves the Airflow UI _[Provider]: An Airflow package that integrates with external systems _[TaskFlow]: The modern Python-native API for defining Airflow tasks using decorators _[Asset]: A logical representation of data that DAGs can produce or consume _[SDK]: Software Development Kit - the airflow.sdk module in Airflow 3.x _[K8s]: Kubernetes - container orchestration platform _[uv]: A fast Python package manager by Astral _[ruff]: A fast Python linter and formatter by Astral _[CI]: Continuous Integration _[CD]: Continuous Deployment *[PR]: Pull Request</p>"},{"location":"modules/15-ai-ml-orchestration/exercises/exercise_15_3_data_prep/","title":"Exercise 15.3: Data Preparation Pipeline","text":"<p>Build a production-ready data preparation pipeline with quality gates, human-in-the-loop review, metadata enrichment, and training dataset versioning.</p>"},{"location":"modules/15-ai-ml-orchestration/exercises/exercise_15_3_data_prep/#learning-goals","title":"Learning Goals","text":"<ul> <li>Design quality gates with configurable thresholds</li> <li>Implement human-in-the-loop patterns for low-confidence items</li> <li>Build metadata extraction and enrichment workflows</li> <li>Create versioned training datasets for ML models</li> <li>Apply data drift detection patterns</li> </ul>"},{"location":"modules/15-ai-ml-orchestration/exercises/exercise_15_3_data_prep/#scenario","title":"Scenario","text":"<p>You're preparing data for an ML model that classifies customer support tickets. The pipeline should:</p> <ol> <li>Ingest: Load raw ticket data from a source</li> <li>Validate: Apply quality checks with configurable thresholds</li> <li>Route: Send low-confidence items for human review</li> <li>Enrich: Extract and add metadata (sentiment, topics, urgency)</li> <li>Version: Create versioned training datasets with lineage</li> </ol> <p>Each step should support idempotent processing and comprehensive observability.</p>"},{"location":"modules/15-ai-ml-orchestration/exercises/exercise_15_3_data_prep/#requirements","title":"Requirements","text":""},{"location":"modules/15-ai-ml-orchestration/exercises/exercise_15_3_data_prep/#task-1-quality-gate-framework","title":"Task 1: Quality Gate Framework","text":"<p>Create a reusable quality validation system that:</p> <ul> <li>Validates data completeness (required fields present)</li> <li>Checks data format compliance (date formats, email patterns)</li> <li>Scores data quality with configurable thresholds</li> <li>Routes items based on quality score (pass/review/reject)</li> </ul>"},{"location":"modules/15-ai-ml-orchestration/exercises/exercise_15_3_data_prep/#task-2-human-in-the-loop-routing","title":"Task 2: Human-in-the-Loop Routing","text":"<p>Implement routing logic for human review:</p> <ul> <li>Define confidence thresholds for automatic processing</li> <li>Create a review queue for borderline items</li> <li>Store review decisions for model improvement</li> <li>Support reviewer assignment and workload balancing</li> </ul>"},{"location":"modules/15-ai-ml-orchestration/exercises/exercise_15_3_data_prep/#task-3-metadata-enrichment","title":"Task 3: Metadata Enrichment","text":"<p>Create enrichment tasks that:</p> <ul> <li>Extract sentiment using LLM or rule-based analysis</li> <li>Identify topics and categories</li> <li>Estimate urgency based on content patterns</li> <li>Add timestamp and source metadata</li> </ul>"},{"location":"modules/15-ai-ml-orchestration/exercises/exercise_15_3_data_prep/#task-4-training-dataset-versioning","title":"Task 4: Training Dataset Versioning","text":"<p>Implement dataset versioning that:</p> <ul> <li>Creates immutable dataset snapshots</li> <li>Tracks data lineage (source \u2192 processed \u2192 training)</li> <li>Supports dataset diffing between versions</li> <li>Stores dataset statistics and distribution metrics</li> </ul>"},{"location":"modules/15-ai-ml-orchestration/exercises/exercise_15_3_data_prep/#task-5-data-drift-detection","title":"Task 5: Data Drift Detection","text":"<p>Add monitoring for data drift:</p> <ul> <li>Compare current batch to historical distribution</li> <li>Alert on significant statistical changes</li> <li>Track feature distribution over time</li> <li>Log drift metrics for model monitoring</li> </ul>"},{"location":"modules/15-ai-ml-orchestration/exercises/exercise_15_3_data_prep/#success-criteria","title":"Success Criteria","text":"<ul> <li> Quality gates prevent invalid data from processing</li> <li> Human review queue functions correctly</li> <li> Metadata enrichment adds value to raw data</li> <li> Dataset versions are immutable and traceable</li> <li> Drift detection identifies distribution changes</li> <li> All operations are idempotent and resumable</li> </ul>"},{"location":"modules/15-ai-ml-orchestration/exercises/exercise_15_3_data_prep/#hints","title":"Hints","text":"Hint 1: Quality Scoring Framework <pre><code>from dataclasses import dataclass\n\n\n@dataclass\nclass QualityScore:\n    \"\"\"Quality assessment for a data item.\"\"\"\n\n    completeness: float  # 0-1: required fields present\n    validity: float  # 0-1: format compliance\n    consistency: float  # 0-1: cross-field consistency\n\n    @property\n    def overall(self) -&gt; float:\n        \"\"\"Weighted overall score.\"\"\"\n        return self.completeness * 0.4 + self.validity * 0.3 + self.consistency * 0.3\n\n    @property\n    def routing_decision(self) -&gt; str:\n        \"\"\"Determine routing based on score.\"\"\"\n        if self.overall &gt;= 0.9:\n            return \"auto_approve\"\n        elif self.overall &gt;= 0.6:\n            return \"human_review\"\n        else:\n            return \"reject\"\n</code></pre> Hint 2: Human Review Queue Pattern <pre><code>def route_for_review(\n    item: dict,\n    quality_score: QualityScore,\n    review_queue_key: str = \"data_prep_review_queue\",\n) -&gt; dict:\n    \"\"\"Route item for human review if needed.\"\"\"\n    from airflow import Variable\n\n    decision = quality_score.routing_decision\n\n    if decision == \"human_review\":\n        # Add to review queue\n        queue = Variable.get(review_queue_key, default_var=[], deserialize_json=True)\n        queue.append(\n            {\n                \"item_id\": item[\"id\"],\n                \"quality_score\": quality_score.overall,\n                \"timestamp\": datetime.now().isoformat(),\n                \"reasons\": get_review_reasons(quality_score),\n            }\n        )\n        Variable.set(review_queue_key, queue, serialize_json=True)\n\n    return {\n        \"item\": item,\n        \"decision\": decision,\n        \"score\": quality_score.overall,\n    }\n</code></pre> Hint 3: Dataset Versioning <pre><code>import hashlib\nimport json\n\n\ndef create_dataset_version(\n    data: list[dict],\n    version_prefix: str = \"dataset\",\n) -&gt; dict:\n    \"\"\"Create an immutable dataset version.\"\"\"\n    # Create content hash for version ID\n    content = json.dumps(data, sort_keys=True)\n    content_hash = hashlib.sha256(content.encode()).hexdigest()[:12]\n\n    version_id = f\"{version_prefix}_{datetime.now().strftime('%Y%m%d')}_{content_hash}\"\n\n    # Calculate statistics\n    stats = {\n        \"record_count\": len(data),\n        \"created_at\": datetime.now().isoformat(),\n        \"content_hash\": content_hash,\n        \"field_stats\": calculate_field_stats(data),\n    }\n\n    return {\n        \"version_id\": version_id,\n        \"data\": data,\n        \"statistics\": stats,\n        \"lineage\": {\"parent_version\": None, \"transformations\": []},\n    }\n</code></pre> Hint 4: Drift Detection <pre><code>def detect_drift(\n    current_stats: dict,\n    baseline_stats: dict,\n    threshold: float = 0.1,\n) -&gt; dict:\n    \"\"\"Detect statistical drift between datasets.\"\"\"\n    drift_report = {\"drifted_features\": [], \"overall_drift\": False}\n\n    for feature in current_stats.get(\"distributions\", {}):\n        current_dist = current_stats[\"distributions\"][feature]\n        baseline_dist = baseline_stats[\"distributions\"].get(feature, {})\n\n        # Calculate distribution difference (simplified)\n        diff = abs(current_dist.get(\"mean\", 0) - baseline_dist.get(\"mean\", 0))\n        normalized_diff = diff / (baseline_dist.get(\"std\", 1) + 0.001)\n\n        if normalized_diff &gt; threshold:\n            drift_report[\"drifted_features\"].append(\n                {\n                    \"feature\": feature,\n                    \"drift_score\": normalized_diff,\n                    \"current_mean\": current_dist.get(\"mean\"),\n                    \"baseline_mean\": baseline_dist.get(\"mean\"),\n                }\n            )\n\n    drift_report[\"overall_drift\"] = len(drift_report[\"drifted_features\"]) &gt; 0\n    return drift_report\n</code></pre>"},{"location":"modules/15-ai-ml-orchestration/exercises/exercise_15_3_data_prep/#files","title":"Files","text":"<ul> <li>Starter: <code>exercise_15_3_data_prep_starter.py</code></li> <li>Solution: <code>../solutions/solution_15_3_data_prep.py</code></li> </ul>"},{"location":"modules/15-ai-ml-orchestration/exercises/exercise_15_3_data_prep/#estimated-time","title":"Estimated Time","text":"<p>75-90 minutes</p>"},{"location":"modules/15-ai-ml-orchestration/exercises/exercise_15_3_data_prep/#next-steps","title":"Next Steps","text":"<p>After completing this exercise:</p> <ol> <li>Integrate with a real data labeling platform (Label Studio, Scale AI)</li> <li>Add A/B testing for different enrichment strategies</li> <li>Implement continuous training triggers based on data quality</li> <li>Build dashboards for data quality monitoring</li> </ol> <p>_[DAG]: Directed Acyclic Graph - Airflow's core workflow abstraction _[XCom]: Cross-Communication - Airflow's mechanism for task-to-task data passing _[Executor]: The component that runs tasks (Local, Celery, Kubernetes, etc.) _[Scheduler]: The component that triggers DAGs and determines task execution order _[Webserver]: The component that serves the Airflow UI _[Provider]: An Airflow package that integrates with external systems _[TaskFlow]: The modern Python-native API for defining Airflow tasks using decorators _[Asset]: A logical representation of data that DAGs can produce or consume _[SDK]: Software Development Kit - the airflow.sdk module in Airflow 3.x _[K8s]: Kubernetes - container orchestration platform _[uv]: A fast Python package manager by Astral _[ruff]: A fast Python linter and formatter by Astral _[CI]: Continuous Integration _[CD]: Continuous Deployment *[PR]: Pull Request</p>"},{"location":"plans/2026-01-04-curriculum-platform-design/","title":"Curriculum Platform Design","text":"<p>Design Date: 2026-01-04 Status: Implemented Version: 1.2.0</p>"},{"location":"plans/2026-01-04-curriculum-platform-design/#overview","title":"Overview","text":"<p>This document describes the design and implementation of the Airflow Mastery curriculum platform, transforming the learning content into a complete, professional educational platform.</p>"},{"location":"plans/2026-01-04-curriculum-platform-design/#goals","title":"Goals","text":"<ol> <li>One-command quickstart \u2014 Students can start learning with a single command</li> <li>Professional documentation \u2014 MkDocs Material site hosted on GitHub Pages</li> <li>Unified tooling \u2014 Root pyproject.toml and justfile for all project operations</li> <li>Automated CI/CD \u2014 Testing and docs deployment via GitHub Actions</li> </ol>"},{"location":"plans/2026-01-04-curriculum-platform-design/#architecture","title":"Architecture","text":"<pre><code>airflow-mastery/\n\u251c\u2500\u2500 docs/                          # MkDocs source\n\u2502   \u251c\u2500\u2500 index.md                   # Landing page\n\u2502   \u251c\u2500\u2500 getting-started/           # Quickstart guides\n\u2502   \u2502   \u251c\u2500\u2500 quickstart.md\n\u2502   \u2502   \u251c\u2500\u2500 manual-setup.md\n\u2502   \u2502   \u2514\u2500\u2500 troubleshooting.md\n\u2502   \u251c\u2500\u2500 case-studies/              # Real-world examples\n\u2502   \u251c\u2500\u2500 modules -&gt; ../modules      # Symlink to curriculum\n\u2502   \u251c\u2500\u2500 CONTRIBUTING.md -&gt; ../...  # Symlink to contributing\n\u2502   \u251c\u2500\u2500 includes/                  # MkDocs includes\n\u2502   \u2502   \u2514\u2500\u2500 abbreviations.md\n\u2502   \u2514\u2500\u2500 stylesheets/               # Custom CSS\n\u2502       \u2514\u2500\u2500 extra.css\n\u251c\u2500\u2500 modules/                       # Curriculum content\n\u2502   \u251c\u2500\u2500 README.md                  # Curriculum index\n\u2502   \u251c\u2500\u2500 00-environment-setup/\n\u2502   \u2514\u2500\u2500 01-15...                   # Learning modules\n\u251c\u2500\u2500 scripts/\n\u2502   \u251c\u2500\u2500 quickstart.sh              # macOS/Linux installer\n\u2502   \u2514\u2500\u2500 quickstart.ps1             # Windows installer\n\u251c\u2500\u2500 .github/workflows/\n\u2502   \u251c\u2500\u2500 docs.yml                   # Deploy docs on push\n\u2502   \u2514\u2500\u2500 ci.yml                     # Test on PRs\n\u251c\u2500\u2500 mkdocs.yml                     # MkDocs configuration\n\u251c\u2500\u2500 pyproject.toml                 # Project dependencies\n\u2514\u2500\u2500 justfile                       # Developer commands\n</code></pre>"},{"location":"plans/2026-01-04-curriculum-platform-design/#components","title":"Components","text":""},{"location":"plans/2026-01-04-curriculum-platform-design/#1-documentation-site-mkdocs-material","title":"1. Documentation Site (MkDocs Material)","text":"<p>Configuration (<code>mkdocs.yml</code>):</p> <ul> <li>Material theme with dark/light mode toggle</li> <li>Navigation tabs with sections and expansion</li> <li>Code highlighting with line numbers and copy button</li> <li>Search with suggestions</li> <li>Mermaid diagram support</li> <li>pymdownx extensions (superfences, tabbed, details, snippets)</li> </ul> <p>Content Structure:</p> <ul> <li>Home: Hero section with quick links</li> <li>Getting Started: Quickstart, manual setup, troubleshooting</li> <li>Curriculum: All 16 learning modules</li> <li>Case Studies: Real-world implementations</li> <li>Reference: External resources, guides</li> </ul> <p>Symlink Strategy:</p> <p>Modules live outside <code>docs/</code> for standalone use. Symlinks provide MkDocs access:</p> <pre><code>docs/modules -&gt; ../modules\ndocs/CONTRIBUTING.md -&gt; ../CONTRIBUTING.md\n</code></pre>"},{"location":"plans/2026-01-04-curriculum-platform-design/#2-quickstart-scripts","title":"2. Quickstart Scripts","text":"<p>Bash Script (<code>scripts/quickstart.sh</code>):</p> <pre><code>1. Check prerequisites (git, docker, docker compose)\n2. Clone repository to ~/airflow-mastery\n3. Install uv if missing\n4. Run uv sync\n5. Start Airflow with docker compose\n6. Wait for health check (poll /health endpoint)\n7. Open browser tabs (Airflow UI, docs)\n8. Print success message with credentials\n</code></pre> <p>PowerShell Script (<code>scripts/quickstart.ps1</code>):</p> <p>Same flow adapted for Windows, using:</p> <ul> <li><code>Invoke-RestMethod</code> for health checks</li> <li><code>Start-Process</code> for browser opening</li> <li><code>irm | iex</code> installation pattern</li> </ul>"},{"location":"plans/2026-01-04-curriculum-platform-design/#3-developer-tooling","title":"3. Developer Tooling","text":"<p>pyproject.toml:</p> <pre><code>[dependency-groups]\ndev = [\"ruff\", \"pytest\", \"pytest-cov\", \"pre-commit\"]\ndocs = [\"mkdocs-material\", \"mkdocs-awesome-pages-plugin\", \"mkdocs-minify-plugin\"]\ntest = [\"pytest\", \"pytest-cov\"]\n</code></pre> <p>justfile (26 recipes):</p> Category Recipes Student quickstart Docs docs, docs-build, docs-deploy Development install, lint, format, test, check Airflow up, down, logs, restart, ps Maintenance clean, update, pre-commit"},{"location":"plans/2026-01-04-curriculum-platform-design/#4-cicd-workflows","title":"4. CI/CD Workflows","text":"<p>ci.yml \u2014 Runs on PRs and pushes to main:</p> <ol> <li>lint \u2014 ruff check and format verification</li> <li>test \u2014 pytest with verbose output</li> <li>validate-dags \u2014 Import check for all DAG files</li> <li>docs \u2014 MkDocs build verification</li> </ol> <p>docs.yml \u2014 Runs on push to main when docs change:</p> <ol> <li>Checkout with full history (for git-revision-date)</li> <li>Install docs dependencies</li> <li>Build and deploy with <code>mkdocs gh-deploy --force</code></li> </ol>"},{"location":"plans/2026-01-04-curriculum-platform-design/#design-decisions","title":"Design Decisions","text":""},{"location":"plans/2026-01-04-curriculum-platform-design/#why-symlinks-for-modules","title":"Why Symlinks for Modules?","text":"<p>Modules are standalone learning units that can be used without the docs site. Symlinks allow:</p> <ul> <li>MkDocs to include them in navigation</li> <li>Modules to remain self-contained</li> <li>Easy updates without moving files</li> </ul>"},{"location":"plans/2026-01-04-curriculum-platform-design/#why-justfile-over-makefile","title":"Why justfile over Makefile?","text":"<ul> <li>Cleaner syntax without <code>.PHONY</code> declarations</li> <li>Better cross-platform support (Windows)</li> <li>Built-in help with <code>@just --list</code></li> <li>Simpler recipe definitions</li> </ul>"},{"location":"plans/2026-01-04-curriculum-platform-design/#why-separate-dependency-groups","title":"Why Separate Dependency Groups?","text":"<ul> <li><code>dev</code>: Minimal for code work</li> <li><code>docs</code>: Only for documentation</li> <li><code>test</code>: Only for testing</li> <li>Reduces install time for focused tasks</li> </ul>"},{"location":"plans/2026-01-04-curriculum-platform-design/#why-not-strict-mode-in-ci","title":"Why Not Strict Mode in CI?","text":"<p>The curriculum content has pre-existing broken links (case studies linking to exercises not yet created). Rather than blocking CI, we:</p> <ul> <li>Build without <code>--strict</code> in CI</li> <li>Document the known issues</li> <li>Fix incrementally as modules are completed</li> </ul>"},{"location":"plans/2026-01-04-curriculum-platform-design/#usage","title":"Usage","text":""},{"location":"plans/2026-01-04-curriculum-platform-design/#for-students","title":"For Students","text":"<pre><code># One-command start\ncurl -fsSL https://raw.githubusercontent.com/YOUR_ORG/airflow-mastery/main/scripts/quickstart.sh | bash\n</code></pre>"},{"location":"plans/2026-01-04-curriculum-platform-design/#for-developers","title":"For Developers","text":"<pre><code>just install      # Install all dependencies\njust docs         # Serve docs locally\njust up           # Start Airflow\njust check        # Run all checks\n</code></pre>"},{"location":"plans/2026-01-04-curriculum-platform-design/#for-maintainers","title":"For Maintainers","text":"<pre><code>just docs-deploy  # Manual docs deployment\njust clean        # Clean build artifacts\njust update       # Update dependencies\n</code></pre>"},{"location":"plans/2026-01-04-curriculum-platform-design/#post-implementation","title":"Post-Implementation","text":"<ul> <li> Enable GitHub Pages (Settings \u2192 Pages \u2192 gh-pages branch)</li> <li> Update README.md with quickstart one-liner</li> <li> Update repository URL placeholders (YOUR_ORG)</li> <li> Create first release tag (v1.2.0)</li> </ul>"},{"location":"plans/2026-01-04-curriculum-platform-design/#files-created","title":"Files Created","text":"File Purpose <code>pyproject.toml</code> Root project configuration <code>justfile</code> Developer command runner <code>mkdocs.yml</code> Documentation site config <code>docs/index.md</code> Landing page <code>docs/getting-started/*.md</code> Setup guides <code>docs/includes/abbreviations.md</code> Tooltip definitions <code>docs/stylesheets/extra.css</code> Custom styling <code>modules/README.md</code> Curriculum index <code>scripts/quickstart.sh</code> macOS/Linux installer <code>scripts/quickstart.ps1</code> Windows installer <code>.github/workflows/docs.yml</code> Docs deployment <code>.github/workflows/ci.yml</code> CI pipeline <p>_[DAG]: Directed Acyclic Graph - Airflow's core workflow abstraction _[XCom]: Cross-Communication - Airflow's mechanism for task-to-task data passing _[Executor]: The component that runs tasks (Local, Celery, Kubernetes, etc.) _[Scheduler]: The component that triggers DAGs and determines task execution order _[Webserver]: The component that serves the Airflow UI _[Provider]: An Airflow package that integrates with external systems _[TaskFlow]: The modern Python-native API for defining Airflow tasks using decorators _[Asset]: A logical representation of data that DAGs can produce or consume _[SDK]: Software Development Kit - the airflow.sdk module in Airflow 3.x _[K8s]: Kubernetes - container orchestration platform _[uv]: A fast Python package manager by Astral _[ruff]: A fast Python linter and formatter by Astral _[CI]: Continuous Integration _[CD]: Continuous Deployment *[PR]: Pull Request</p>"},{"location":"plans/2026-01-04-module00-environment-setup-design/","title":"Design: Module 00 - Development Environment &amp; Modern Python Tooling","text":"<p>Date: 2026-01-04 Status: Approved Author: Brainstorming Session</p>"},{"location":"plans/2026-01-04-module00-environment-setup-design/#overview","title":"Overview","text":"<p>Create a new prerequisite module (Module 00) that teaches modern Python development tooling for Airflow projects. This module sets up learners with a professional development environment before they begin learning Airflow concepts.</p>"},{"location":"plans/2026-01-04-module00-environment-setup-design/#target-audience","title":"Target Audience","text":"<ul> <li>Experience Level: Intermediate Python developers</li> <li>Background: Familiar with pip/virtualenv basics, unfamiliar with modern tooling (uv, pyproject.toml, ruff)</li> <li>Goal: Learn modern Python project setup specifically for Airflow development</li> </ul>"},{"location":"plans/2026-01-04-module00-environment-setup-design/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this module, learners will:</p> <ol> <li>Install and configure uv for fast Python package management</li> <li>Create a professional pyproject.toml with Airflow dependency groups</li> <li>Set up ruff for linting and formatting</li> <li>Configure pre-commit hooks for automated code quality</li> <li>Build optimized Docker images using uv</li> <li>Establish a testing workflow with pytest</li> </ol>"},{"location":"plans/2026-01-04-module00-environment-setup-design/#module-structure","title":"Module Structure","text":"<pre><code>modules/00-environment-setup/\n\u251c\u2500\u2500 README.md                    # Main module content (~2500 words)\n\u251c\u2500\u2500 exercises/\n\u2502   \u251c\u2500\u2500 exercise_0_1.md          # Install uv and create project\n\u2502   \u251c\u2500\u2500 exercise_0_2.md          # Configure pyproject.toml for Airflow\n\u2502   \u251c\u2500\u2500 exercise_0_3.md          # Set up ruff and pre-commit\n\u2502   \u251c\u2500\u2500 exercise_0_4.md          # Build Airflow Docker image with uv\n\u2502   \u2514\u2500\u2500 exercise_0_5.md          # Integrate testing workflow\n\u2514\u2500\u2500 solutions/\n    \u251c\u2500\u2500 solution_0_1/            # Directory with pyproject.toml template\n    \u251c\u2500\u2500 solution_0_2.toml        # Complete pyproject.toml\n    \u251c\u2500\u2500 solution_0_3/            # pre-commit config + ruff.toml\n    \u251c\u2500\u2500 solution_0_4/            # Dockerfile + docker-compose\n    \u2514\u2500\u2500 solution_0_5/            # pytest setup + example tests\n</code></pre>"},{"location":"plans/2026-01-04-module00-environment-setup-design/#content-outline","title":"Content Outline","text":""},{"location":"plans/2026-01-04-module00-environment-setup-design/#readmemd-sections","title":"README.md Sections","text":"<ol> <li> <p>Why Modern Python Tooling Matters (~200 words)</p> <ul> <li>Traditional pain points: pip slowness, tool fragmentation</li> <li>The modern stack: uv + ruff + pyproject.toml</li> <li>Speed comparison: uv vs pip (10-100x faster)</li> </ul> </li> <li> <p>Installing and Understanding uv (~400 words)</p> <ul> <li>Installation methods</li> <li>uv vs pip mental model</li> <li>Virtual environment management</li> <li>Lock files explained</li> </ul> </li> <li> <p>Project Configuration with pyproject.toml (~400 words)</p> <ul> <li>Anatomy for Airflow projects</li> <li>Dependency groups: core, dev, test, providers</li> <li>Version constraints</li> </ul> </li> <li> <p>Code Quality with Ruff (~300 words)</p> <ul> <li>One tool replacing flake8, black, isort</li> <li>Airflow-specific rules</li> <li>IDE integration</li> </ul> </li> <li> <p>Pre-commit Hooks (~250 words)</p> <ul> <li>Why pre-commit matters</li> <li>Configuration</li> <li>CI integration</li> </ul> </li> <li> <p>Docker Builds with uv (~400 words)</p> <ul> <li>Multi-stage Dockerfile pattern</li> <li>Layer caching optimization</li> <li>Comparison with traditional pip</li> </ul> </li> <li> <p>Testing Setup (~300 words)</p> <ul> <li>pytest configuration</li> <li>Conftest patterns</li> <li>Coverage</li> </ul> </li> <li> <p>Quick Reference (~250 words)</p> <ul> <li>Command cheat sheet</li> <li>CI/CD example</li> </ul> </li> </ol>"},{"location":"plans/2026-01-04-module00-environment-setup-design/#exercises","title":"Exercises","text":""},{"location":"plans/2026-01-04-module00-environment-setup-design/#exercise-01-project-initialization-with-uv","title":"Exercise 0.1: Project Initialization with uv","text":"<p>Objective: Install uv and create a new Airflow project Deliverable: Working directory with <code>.venv</code>, <code>pyproject.toml</code>, <code>uv.lock</code></p>"},{"location":"plans/2026-01-04-module00-environment-setup-design/#exercise-02-configure-pyprojecttoml-for-airflow","title":"Exercise 0.2: Configure pyproject.toml for Airflow","text":"<p>Objective: Set up production-ready dependency groups Deliverable: Complete pyproject.toml with all groups and providers</p>"},{"location":"plans/2026-01-04-module00-environment-setup-design/#exercise-03-code-quality-setup","title":"Exercise 0.3: Code Quality Setup","text":"<p>Objective: Configure ruff and pre-commit Deliverable: Working pre-commit setup with Airflow-aware rules</p>"},{"location":"plans/2026-01-04-module00-environment-setup-design/#exercise-04-docker-image-build","title":"Exercise 0.4: Docker Image Build","text":"<p>Objective: Build optimized Airflow Docker image with uv Deliverable: Dockerfile + docker-compose.yaml</p>"},{"location":"plans/2026-01-04-module00-environment-setup-design/#exercise-05-testing-integration","title":"Exercise 0.5: Testing Integration","text":"<p>Objective: Set up pytest for DAG validation Deliverable: Working test suite with DAG integrity tests</p>"},{"location":"plans/2026-01-04-module00-environment-setup-design/#integration-points","title":"Integration Points","text":""},{"location":"plans/2026-01-04-module00-environment-setup-design/#files-to-modify","title":"Files to Modify","text":"File Change <code>README.md</code> Add Module 00 to curriculum, update week structure <code>docs/references.md</code> Add uv/ruff/pre-commit resources <code>CHANGELOG.md</code> Document Module 00 addition"},{"location":"plans/2026-01-04-module00-environment-setup-design/#curriculum-placement","title":"Curriculum Placement","text":"<pre><code>Week 0 (NEW): Development Environment Setup\n\u251c\u2500\u2500 Module 00: Modern Python Tooling\n\nWeek 1-2: Foundations (existing)\n\u251c\u2500\u2500 Module 01: Foundations\n\u251c\u2500\u2500 Module 02: TaskFlow API\n... (unchanged)\n</code></pre>"},{"location":"plans/2026-01-04-module00-environment-setup-design/#cross-references","title":"Cross-References","text":"<p>Forward references to:</p> <ul> <li>Module 07: Testing &amp; Debugging (deeper pytest patterns)</li> <li>Module 08: Kubernetes Executor (Docker builds for K8s)</li> <li>Module 09: Production Patterns (CI/CD integration)</li> </ul>"},{"location":"plans/2026-01-04-module00-environment-setup-design/#technical-decisions","title":"Technical Decisions","text":"<ol> <li>uv over poetry/pipenv: Fastest, simplest, best Docker support</li> <li>ruff over black+flake8: Single tool, faster, better Airflow rules</li> <li>pyproject.toml only: No requirements.txt, modern standard</li> <li>Multi-stage Docker: Smaller images, better caching</li> </ol>"},{"location":"plans/2026-01-04-module00-environment-setup-design/#success-criteria","title":"Success Criteria","text":"<ul> <li> All 5 exercises complete with solutions</li> <li> README covers all learning objectives</li> <li> Solutions pass ruff checks</li> <li> Docker image builds successfully</li> <li> Tests run with <code>uv run pytest</code></li> </ul>"},{"location":"plans/2026-01-04-module00-environment-setup-design/#estimated-effort","title":"Estimated Effort","text":"<ul> <li>README.md: ~2500 words, 2-3 hours</li> <li>Exercises (5): ~500 words each, 3-4 hours</li> <li>Solutions: 4-5 hours</li> <li>Integration updates: 1 hour</li> <li>Total: ~10-12 hours</li> </ul>"},{"location":"plans/2026-01-04-module00-environment-setup-design/#approval","title":"Approval","text":"<p>Validated through brainstorming session on 2026-01-04</p> <p>Decisions confirmed:</p> <ul> <li>Module 00 placement (prerequisite)</li> <li>Scope: uv + pyproject.toml + ruff + pre-commit + Docker + testing</li> <li>Audience: Intermediate Python developers</li> <li>Context: Local development + Docker (no K8s in this module)</li> <li>Exercises: 5 comprehensive exercises</li> </ul> <p>_[DAG]: Directed Acyclic Graph - Airflow's core workflow abstraction _[XCom]: Cross-Communication - Airflow's mechanism for task-to-task data passing _[Executor]: The component that runs tasks (Local, Celery, Kubernetes, etc.) _[Scheduler]: The component that triggers DAGs and determines task execution order _[Webserver]: The component that serves the Airflow UI _[Provider]: An Airflow package that integrates with external systems _[TaskFlow]: The modern Python-native API for defining Airflow tasks using decorators _[Asset]: A logical representation of data that DAGs can produce or consume _[SDK]: Software Development Kit - the airflow.sdk module in Airflow 3.x _[K8s]: Kubernetes - container orchestration platform _[uv]: A fast Python package manager by Astral _[ruff]: A fast Python linter and formatter by Astral _[CI]: Continuous Integration _[CD]: Continuous Deployment *[PR]: Pull Request</p>"}]}